<!doctype html>
<html lang="en" data-theme="light">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>AppSec Dashboards â€” Program & Risk (EverythingUX Mockup)</title>

    <!-- Typography (EverythingUX uses Roboto with sentence case) -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;600;700&display=swap" rel="stylesheet">

    <style>
        :root {
            /* ===== Base tokens (subset aligned to EverythingUX) ===== */
            --color-primary: #0073e7;

            /* Neutrals */
            --gray-05: #fafafa;
            --gray-10: #f1f2f3;
            --gray-20: #e8eaeb;
            --gray-30: #cfd0d1;
            --gray-40: #bdbec0;
            --gray-50: #a3a5a8;
            --gray-60: #8a8d8f;
            --gray-70: #747577;
            --gray-80: #4c4e50;
            --gray-90: #323435;
            --gray-95: #1c1d1e;

            /* Status */
            --status-success: #1aa364;
            --status-warning: #ffc002;
            --status-error: #e5004c;
            --status-info: #0064ca;

            /* Elevations (shadows) */
            --elev-1: 0 2px 6px rgba(0, 0, 0, 0.06);
            --elev-2: 0 2px 4px rgba(0, 0, 0, 0.10);
            --elev-3: 0 6px 10px rgba(0, 0, 0, 0.10);
            --elev-4: 0 12px 24px rgba(0, 0, 0, 0.12);

            /* Radii */
            --radius-sm: 2px;
            --radius-md: 4px;
            --radius-lg: 8px;

            /* Spacing (8â€‘pt scale) */
            --space-4: 4px;
            --space-8: 8px;
            --space-12: 12px;
            --space-16: 16px;
            --space-24: 24px;
            --space-32: 32px;
            --space-40: 40px;

            /* Content grid */
            --content-width: 1376px;
            --gutter: 24px;

            /* Program: scan-type palette */
            --c-sast: #3B82F6;
            --c-dast: #F59E0B;
            --c-sca: #10B981;
            --c-other: #9CA3AF;

            /* Risk: severity palette */
            --sev-critical: #e5004c;
            --sev-high: #F59E0B;
            --sev-medium: #FFC002;
            --sev-low: #FDE68A;

            /* Compliance: policy status */
            --compliance-pass: #1aa364;
            --compliance-fail: #e5004c;
            --compliance-unassessed: #bdbec0;
        }

        /* ===== Theme roles ===== */
        html[data-theme="light"] {
            --bg-canvas: var(--gray-10);
            --bg-surface: #ffffff;
            --border: var(--gray-20);
            --text: var(--gray-90);
            --text-muted: var(--gray-70);
            --track: var(--gray-20);
            --gridline: var(--gray-20);
            --shadow: var(--elev-1);
        }

        html[data-theme="dark"] {
            --bg-canvas: var(--gray-95);
            --bg-surface: var(--gray-90);
            --border: var(--gray-80);
            --text: var(--gray-10);
            --text-muted: var(--gray-50);
            --track: var(--gray-80);
            --gridline: var(--gray-80);
            --shadow: var(--elev-2);
        }

        /* ===== Base ===== */
        * {
            box-sizing: border-box
        }

        body {
            background: var(--bg-canvas);
            color: var(--text);
            font-family: Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
        }

        .wrap {
            max-width: calc(var(--content-width) + 64px);
            margin: 0 auto;
            padding: 24px 32px 64px;
        }

        /* Masthead + Nav */
        .masthead {
            background: var(--bg-surface);
            border: 1px solid var(--border);
            border-radius: var(--radius-lg);
            box-shadow: var(--shadow);
            padding: 16px 24px;
            margin-bottom: 24px;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }

        .masthead h1 {
            font-size: 24px;
            line-height: 32px;
            margin: 0;
            font-weight: 600;
            color: var(--text);
        }

        .nav {
            display: flex;
            gap: 8px;
        }

        .nav button {
            appearance: none;
            background: transparent;
            border: 1px solid var(--border);
            color: var(--text);
            border-radius: 16px;
            padding: 6px 12px;
            cursor: pointer;
            font: 500 14px/20px Roboto, sans-serif;
        }

        .nav button.active {
            background: var(--color-primary);
            color: #fff;
            border-color: var(--color-primary);
        }

        .toggle {
            appearance: none;
            background: transparent;
            border: 1px solid var(--border);
            color: var(--text);
            border-radius: 20px;
            padding: 8px 12px;
            font: 500 14px/20px Roboto, sans-serif;
            cursor: pointer;
            box-shadow: var(--shadow);
        }

        .toggle:focus-visible {
            outline: 2px solid var(--color-primary);
            outline-offset: 2px;
        }

        /* Theme toggle switch */
        .theme-toggle {
            position: relative;
            display: inline-block;
            width: 52px;
            height: 28px;
        }

        .theme-toggle input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .theme-slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: var(--border);
            transition: 0.3s;
            border-radius: 28px;
            border: 1px solid var(--border);
        }

        .theme-slider:before {
            position: absolute;
            content: "â˜€ï¸";
            height: 20px;
            width: 20px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.3s;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 12px;
        }

        .theme-toggle input:checked+.theme-slider {
            background-color: var(--color-primary);
        }

        .theme-toggle input:checked+.theme-slider:before {
            transform: translateX(24px);
            content: "ðŸŒ™";
        }

        .theme-toggle input:focus-visible+.theme-slider {
            outline: 2px solid var(--color-primary);
            outline-offset: 2px;
        }

        .nav-group {
            display: flex;
            align-items: center;
            gap: 16px;
        }

        /* Pages */
        .page {
            display: none;
        }

        .page.active {
            display: block;
        }

        /* Grid */
        .grid {
            display: grid;
            grid-template-columns: repeat(12, 1fr);
            grid-column-gap: var(--gutter);
        }

        .row {
            margin-bottom: 24px;
        }

        .col-4 {
            grid-column: span 4;
        }

        .col-6 {
            grid-column: span 6;
        }

        .col-8 {
            grid-column: span 8;
        }

        .col-12 {
            grid-column: span 12;
        }

        /* Card */
        .card {
            background: var(--bg-surface);
            border: 1px solid var(--border);
            border-radius: var(--radius-lg);
            box-shadow: var(--shadow);
            padding: 16px;
        }

        .card h3 {
            margin: 0 0 8px 0;
            font-size: 16px;
            line-height: 24px;
            color: var(--text-muted);
            font-weight: 500;
        }

        .kpi {
            font-size: 32px;
            line-height: 40px;
            font-weight: 600;
            color: var(--text);
        }

        /* KPI with inline delta (YoY next to value) */
        .kpi-inline {
            display: flex;
            align-items: center;
            gap: 16px;
            flex-wrap: wrap;
        }

        .muted {
            color: var(--text-muted);
        }

        /* Donut (coverage/compliance/dep risk) */
        .donut-wrap {
            display: flex;
            align-items: center;
            gap: 16px;
        }

        .donut {
            --pct: 72;
            --ring-color: var(--color-primary);
            width: 120px;
            height: 120px;
            background: conic-gradient(var(--ring-color) calc(var(--pct)*1%), var(--track) 0);
            -webkit-mask: radial-gradient(closest-side, transparent 68%, black 70%);
            mask: radial-gradient(closest-side, transparent 68%, black 70%);
            border-radius: 50%;
        }


        /* KPI deltas (YoY) */
        .kpi-row {
            display: flex;
            align-items: baseline;
            gap: 12px;
            flex-wrap: wrap;
        }

        .delta {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            font-size: 12px;
            line-height: 16px;
            padding: 2px 8px;
            border-radius: 12px;
            border: 1px solid var(--border);
            color: var(--text-muted);
            white-space: nowrap;
        }

        .delta i {
            font-style: normal;
        }

        .delta.positive {
            color: var(--status-success);
            border-color: var(--status-success);
        }

        .delta.negative {
            color: var(--status-error);
            border-color: var(--status-error);
        }

        .delta.neutral {
            color: var(--text-muted);
            border-color: var(--gridline);
        }

        /* Trend (stacked 12 bars) */
        .trend {
            height: 200px;
            display: flex;
            align-items: flex-end;
            gap: 24px;
            padding: 8px 16px 0 16px;
            border-top: 1px solid var(--gridline);
        }

        /* Grouped bar chart container */
        .trend-grouped {
            display: flex;
            height: 280px;
            padding: 16px 0 40px 60px;
            position: relative;
        }

        .trend-grouped .chart-area {
            flex: 1;
            display: flex;
            align-items: flex-end;
            gap: 16px;
            border-left: 2px solid var(--border);
            border-bottom: 2px solid var(--border);
            padding: 0 16px 0 16px;
            position: relative;
        }

        .trend-grouped .y-axis {
            position: absolute;
            left: 0;
            top: 0;
            bottom: 40px;
            width: 50px;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            font-size: 11px;
            color: var(--text-muted);
            text-align: right;
            padding-right: 8px;
        }

        .trend-grouped .y-axis-label {
            position: absolute;
            left: 8px;
            top: 50%;
            transform: translateY(-50%) rotate(-90deg);
            font-size: 12px;
            font-weight: 600;
            color: var(--text);
            white-space: nowrap;
        }

        .trend-grouped .x-axis {
            position: absolute;
            bottom: 0;
            left: 60px;
            right: 0;
            height: 40px;
            display: flex;
            align-items: flex-start;
            padding-top: 8px;
        }

        .trend-grouped .x-axis .month-label {
            flex: 1;
            text-align: center;
            font-size: 11px;
            color: var(--text-muted);
        }

        .trend-grouped .month-group {
            flex: 1;
            display: flex;
            align-items: flex-end;
            justify-content: center;
            gap: 3px;
            padding: 0 4px;
        }

        .trend-grouped .grouped-bar {
            width: 12px;
            border-radius: 2px 2px 0 0;
            transition: opacity 0.2s ease;
        }

        .trend-grouped .grouped-bar:hover {
            opacity: 0.8;
        }

        .bar {
            width: 28px;
            display: flex;
            flex-direction: column-reverse;
            border-radius: 2px;
            overflow: hidden;
        }

        .seg {
            width: 100%;
        }

        /* Legend */
        .legend,
        .legend-coverage {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
            margin-top: 8px;
        }

        .sw {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            color: var(--text-muted);
            font-size: 12px;
        }

        .sw i {
            width: 10px;
            height: 10px;
            display: inline-block;
            border-radius: 2px;
        }

        /* Treemap (simple) */
        .treemap {
            display: flex;
            height: 160px;
            gap: 2px;
            border-radius: 4px;
            overflow: hidden;
        }

        .tree-seg {
            height: 100%;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: separate;
            border-spacing: 0;
        }

        thead th {
            font-size: 14px;
            color: var(--text-muted);
            text-align: left;
            padding: 8px 0;
            border-bottom: 1px solid var(--gridline);
        }

        tbody td {
            font-size: 14px;
            color: var(--text);
            padding: 10px 0;
            border-bottom: 1px dashed var(--gridline);
        }

        .warn {
            color: var(--status-error);
            font-weight: 600;
        }

        /* Horizontal bars (ratings/coverage rows) */
        .hbars {
            display: grid;
            grid-template-columns: 60px 1fr 80px;
            row-gap: 12px;
            column-gap: 16px;
            margin-top: 8px;
        }

        .hlabel {
            font-size: 14px;
            color: var(--text-muted);
            align-self: center;
        }

        .hbar {
            --pct: 0;
            --fill-color: var(--color-primary);
            height: 16px;
            width: 100%;
            background: var(--track);
            border: 1px solid var(--border);
            border-radius: 8px;
            overflow: hidden;
            position: relative;
        }

        .hbar>.fill {
            position: absolute;
            inset: 0 auto 0 0;
            width: calc(var(--pct)*1%);
            background: var(--fill-color);
        }

        .hmeta {
            font-size: 12px;
            color: var(--text-muted);
            text-align: right;
            align-self: center;
        }

        /* Matrix (age x severity) */
        .matrix {
            display: grid;
            grid-template-columns: 120px repeat(4, 1fr);
            gap: 8px;
            margin-top: 8px;
            max-height: 320px;
            min-height: 240px;
            align-items: stretch;
        }

        .mx-h {
            font-size: 12px;
            color: var(--text-muted);
            text-align: center;
        }

        .mx-rowlabel {
            font-size: 14px;
            color: var(--text-muted);
            display: flex;
            align-items: center;
        }

        .mx-cell {
            min-height: 40px;
            border: 1px solid var(--border);
            border-radius: 6px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 4px;
            font-size: 12px;
            color: var(--text);
            flex-grow: 1;
        }

        /* color-mix gives a nice tinted cell; harmless if unsupported */
        .mx-cell.crit {
            background: color-mix(in srgb, var(--sev-critical) 15%, transparent);
        }

        .mx-cell.high {
            background: color-mix(in srgb, var(--sev-high) 15%, transparent);
        }

        .mx-cell.med {
            background: color-mix(in srgb, var(--sev-medium) 18%, transparent);
        }

        .mx-cell.low {
            background: color-mix(in srgb, var(--sev-low) 22%, transparent);
        }

        .badge {
            font-size: 11px;
            padding: 2px 6px;
            border-radius: 10px;
            border: 1px solid var(--border);
            background: var(--bg-surface);
            color: var(--text-muted);
        }

        @media (max-width:1280px) {
            .grid {
                grid-template-columns: repeat(6, 1fr);
            }

            .col-4,
            .col-6,
            .col-8,
            .col-12 {
                grid-column: span 6;
            }
        }

        @media (max-width:768px) {
            .wrap {
                padding: 16px;
            }
        }

        /* OpenText Product Masthead */
        .ot-masthead {
            height: 48px;
            background: linear-gradient(90deg, #19224A 0%, #222E61 25%, #23346A 65%, #146693 90%, #088CB2 100%);
            display: flex;
            align-items: center;
            padding: 0 24px;
            gap: 8px;
        }

        [data-theme="dark"] .ot-masthead {
            background: linear-gradient(90deg, #303A60 0%, #2C3661 25%, #394780 65%, #146693 90%, #088CB2 100%);
        }

        .ot-logo {
            width: 95px;
            height: 18px;
        }

        .ot-divider {
            width: 1px;
            height: 18px;
            background: #FFFFFF;
            margin: 0 8px 0 0;
        }

        .ot-product-name {
            font-family: Roboto, sans-serif;
            font-size: 18px;
            font-weight: 400;
            color: #FFFFFF;
            line-height: 18px;
        }

        /* Filter Panel Styles */
        .filter-toggle-btn {
            position: relative;
            background: var(--bg-surface);
            border: 1px solid var(--border);
            border-radius: 4px;
            padding: 8px 16px;
            font-size: 14px;
            font-weight: 500;
            color: var(--text);
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 8px;
            transition: all 0.2s ease;
            margin-bottom: 16px;
        }

        .filter-toggle-btn:hover {
            background: var(--bg-canvas);
            border-color: var(--accent);
        }

        .filter-toggle-btn svg {
            width: 16px;
            height: 16px;
            fill: var(--text);
        }

        .filter-badge {
            background: var(--accent);
            color: white;
            font-size: 11px;
            font-weight: 600;
            padding: 2px 6px;
            border-radius: 10px;
            min-width: 18px;
            text-align: center;
        }

        .filter-panel {
            display: none;
            background: var(--bg-surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: var(--shadow);
        }

        .filter-panel.open {
            display: block;
        }

        .filter-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 24px;
            padding-bottom: 16px;
            border-bottom: 1px solid var(--border);
        }

        .filter-header h3 {
            margin: 0;
            font-size: 18px;
            font-weight: 600;
            color: var(--text);
        }

        .filter-actions {
            display: flex;
            gap: 12px;
        }

        .filter-btn {
            padding: 6px 16px;
            font-size: 13px;
            font-weight: 500;
            border-radius: 4px;
            border: 1px solid var(--border);
            background: transparent;
            color: var(--text);
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .filter-btn:hover {
            background: var(--bg-canvas);
        }

        .filter-btn.primary {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }

        .filter-btn.primary:hover {
            opacity: 0.9;
        }

        .filter-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 24px;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .filter-group-label {
            font-size: 13px;
            font-weight: 600;
            color: var(--text);
            margin-bottom: 4px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .filter-option {
            display: flex;
            align-items: center;
            gap: 8px;
            cursor: pointer;
            padding: 4px 0;
        }

        .filter-option input[type="checkbox"] {
            width: 16px;
            height: 16px;
            border: 2px solid var(--border);
            border-radius: 3px;
            cursor: pointer;
            position: relative;
            appearance: none;
            background: var(--bg-surface);
            transition: all 0.2s ease;
        }

        .filter-option input[type="checkbox"]:checked {
            background: var(--accent);
            border-color: var(--accent);
        }

        .filter-option input[type="checkbox"]:checked::after {
            content: 'âœ“';
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: white;
            font-size: 12px;
            font-weight: bold;
        }

        .filter-option label {
            font-size: 14px;
            color: var(--text);
            cursor: pointer;
            user-select: none;
        }

        .filter-option:hover label {
            color: var(--accent);
        }

        /* Remove dark theme override since we're using proper theme variables now */

        /* Active Filter Tags */
        .filter-tags-container {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            flex-wrap: wrap;
            margin-bottom: 16px;
        }

        .filter-tag {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            background: var(--accent);
            color: white;
            font-size: 13px;
            font-weight: 500;
            padding: 6px 10px;
            border-radius: 16px;
            white-space: nowrap;
            animation: slideIn 0.2s ease;
        }

        /* Ensure tags are readable in light mode */
        html[data-theme="light"] .filter-tag {
            background: var(--gray-80);
            color: white;
        }

        html[data-theme="dark"] .filter-tag {
            background: var(--accent);
            color: white;
        }

        .filter-tag-close {
            cursor: pointer;
            font-size: 16px;
            line-height: 1;
            opacity: 0.8;
            transition: opacity 0.2s ease;
            font-weight: bold;
        }

        .filter-tag-close:hover {
            opacity: 1;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(-10px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }
    </style>
</head>

<body>
    <!-- OpenText Product Masthead -->
    <div class="ot-masthead">
        <svg class="ot-logo" width="95" height="18" viewBox="0 0 95 18" fill="none" xmlns="http://www.w3.org/2000/svg">
            <g clip-path="url(#clip0_3195_418902)">
                <path fill-rule="evenodd" clip-rule="evenodd" d="M12.5776 8.5218C12.5776 11.4213 10.8536 14.5559 6.30839 14.5559C3.01705 14.5559 0 12.7535 0 8.5218C0 5.03455 2.27259 2.33096 6.7394 2.56605C11.5197 2.84033 12.5776 6.44512 12.5776 8.5218ZM4.07498 6.32757C3.64398 6.95449 3.48725 7.73814 3.48725 8.5218C3.48725 10.285 4.38844 11.8915 6.30839 11.8915C8.18915 11.8915 9.09035 10.4026 9.09035 8.67853C9.09035 7.42468 8.77689 6.4843 8.11078 5.89657C7.4055 5.26965 6.58267 5.23046 6.07329 5.26965C5.13291 5.30883 4.54517 5.62229 4.07498 6.32757Z" fill="white"/>
                <path d="M43.4158 3.69692C43.6831 3.41945 43.9055 3.18866 44.433 2.95788C44.9815 2.72278 45.726 2.56605 46.588 2.56605C47.2933 2.56605 48.077 2.6836 48.6647 2.99706C49.8794 3.62398 50.232 4.64273 50.232 6.44512V14.2816H46.7448V7.81651C46.7448 6.79776 46.7056 6.40594 46.588 6.09248C46.3138 5.46556 45.726 5.23046 45.0207 5.23046C43.2183 5.23046 43.2183 6.64103 43.2183 8.09079V14.2816H39.7703V2.84033H43.2575V3.85907C43.3134 3.80318 43.3654 3.74918 43.4158 3.69692Z" fill="white"/>
                <path fill-rule="evenodd" clip-rule="evenodd" d="M38.1246 11.3038C38.113 11.3283 38.1005 11.3555 38.0869 11.3851C37.7313 12.1571 36.6262 14.5559 32.2864 14.5559C28.7992 14.5559 26.3698 12.4792 26.3698 8.71771C26.3698 5.97493 27.7804 2.56605 32.404 2.56605C32.4338 2.56605 32.466 2.56591 32.5003 2.56576C33.2776 2.56237 35.1723 2.55409 36.6357 4.05499C38.1638 5.66147 38.2422 7.89488 38.2813 9.1879H29.8571C29.8571 10.5985 30.6799 12.0482 32.5607 12.0482C34.4395 12.0482 35.1062 10.8361 35.5372 10.0524L35.5386 10.0499L38.1246 11.3038ZM34.6613 6.97902C34.5863 6.517 34.4871 5.90552 34.0104 5.46556C33.5794 5.03455 32.9133 4.83864 32.3256 4.83864C31.5028 4.83864 30.915 5.23046 30.5624 5.58311C30.0922 6.09248 29.9746 6.60185 29.8571 7.11122L34.6765 7.07204C34.6715 7.04194 34.6664 7.01062 34.6613 6.97902Z" fill="white"/>
                <path d="M86.2016 5.23046H88.4742V2.87951H86.2016V0.450195H82.7535V2.84033H82.205L80.6377 5.19128H82.7535V10.6377C82.7535 11.7348 82.7927 12.5576 83.2629 13.2237C84.0074 14.2816 85.3004 14.36 86.515 14.36C87.142 14.36 87.6121 14.2816 88.3174 14.1641V11.5389L87.1028 11.578C86.1624 11.578 86.1624 10.9903 86.2016 10.285V5.23046Z" fill="white"/>
                <path d="M55.3257 0.450195V2.84033H58.8914L57.324 5.19128H55.2866V10.285C55.2474 10.9903 55.2474 11.578 56.1878 11.578L57.4024 11.5389V14.1641L57.2106 14.1945C56.5796 14.295 56.1711 14.36 55.6 14.36C54.4245 14.36 53.0923 14.2816 52.3479 13.2237C51.8777 12.5576 51.8385 11.7348 51.8385 10.6377V0.450195H55.3257Z" fill="white"/>
                <path d="M91.6088 5.23046H91.2169V3.19297H90.4333V2.84033H92.3924V3.19297H91.6088V5.23046Z" fill="white"/>
                <path d="M94.9393 5.23046H94.5475V3.78071L94.5866 3.31052L94.4691 3.70234L93.9597 5.26965H93.5679L93.0585 3.70234L92.941 3.31052L92.9802 3.78071V5.23046H92.5883V2.87951H93.1761L93.7638 4.76027L94.3515 2.87951H94.9393V5.23046Z" fill="white"/>
                <path fill-rule="evenodd" clip-rule="evenodd" d="M69.7037 11.3039L69.0376 12.2834C68.4107 13.1454 67.2744 14.5168 63.9439 14.5168C60.4566 14.5168 58.1449 12.4402 58.1449 8.67863C58.1449 5.93586 59.5554 2.52698 64.179 2.52698C64.2089 2.52698 64.2411 2.52684 64.2754 2.52669C65.0527 2.52329 66.9473 2.51502 68.4107 4.01591C69.9388 5.6224 70.0172 7.8558 70.0564 9.14882H61.6713C61.6321 10.5594 62.3766 12.0091 64.2182 12.0091C65.9954 12.0091 66.583 10.9246 67.006 10.144L67.0067 10.1426C67.0311 10.0976 67.055 10.0536 67.0785 10.0108L69.7037 11.3039ZM66.4755 6.97916C66.4005 6.51714 66.3013 5.90563 65.8246 5.46567C65.3936 5.03466 64.7275 4.83875 64.1398 4.83875C63.317 4.83875 62.7292 5.23057 62.3766 5.58321C61.9064 6.09257 61.7889 6.60192 61.6713 7.11128L66.4907 7.07215C66.4857 7.04208 66.4806 7.01072 66.4755 6.97916Z" fill="white"/>
                <path d="M77.1925 8.28793L81.5026 14.2829H77.5451L75.2726 11.1091L73.1959 14.2829H69.2776L73.2742 8.28793L69.3952 2.88074H73.3134L75.155 5.4276L76.8399 2.88074H80.7581L77.1925 8.28793Z" fill="white"/>
                <path fill-rule="evenodd" clip-rule="evenodd" d="M20.3778 2.56728C18.693 2.56728 17.6351 3.35093 17.3216 3.8603V2.84156H13.8343V18.0052H17.2824V13.2249C17.5567 13.6951 18.7322 14.518 20.2211 14.518C24.4136 14.518 25.2756 10.8348 25.2756 8.48384C25.2756 4.68314 23.2382 2.56728 20.3778 2.56728ZM17.9093 6.05452C18.3795 5.4276 19.0456 5.19251 19.6725 5.19251C19.8024 5.19251 19.9322 5.2194 20.0843 5.2509C20.1158 5.25743 20.1483 5.26415 20.1819 5.27087C21.5925 5.58433 22.0235 7.07327 22.0235 8.48384C22.0235 10.1687 21.279 11.8927 19.5942 11.8927C19.0064 11.8927 18.4187 11.6184 18.0269 11.1874C17.5959 10.7172 17.204 9.89441 17.204 8.56221C17.204 7.54346 17.4 6.64226 17.9093 6.05452Z" fill="white"/>
            </g>
            <defs>
                <clipPath id="clip0_3195_418902">
                    <rect width="95" height="18" fill="white"/>
                </clipPath>
            </defs>
        </svg>
        <div class="ot-divider"></div>
        <div class="ot-product-name">Application Security Posture Management</div>
    </div>
    <div class="wrap">
        <!-- MASTHEAD with page nav + theme toggle -->
        <div class="masthead">
            <div>
                <h1 id="pageTitle" style="margin:0;">Program Dashboard</h1>
                <p id="pageSubtitle" style="margin:4px 0 0 0; font-size:14px; color:var(--text-muted);">Track
                    application inventory, scan coverage, and overall security program status</p>
            </div>
            <div class="nav-group">
                <div class="nav">
                    <button id="navProgram" class="active" type="button">Program</button>
                    <button id="navRisk" type="button">Risk Exposure</button>
                    <button id="navRemediation" type="button">Remediation</button>
                </div>
                <label class="theme-toggle">
                    <input type="checkbox" id="themeToggle" aria-label="Toggle dark mode">
                    <span class="theme-slider"></span>
                </label>
            </div>
        </div>

        <!-- Filter Toggle Button and Active Tags -->
        <div class="filter-tags-container">
            <button class="filter-toggle-btn" id="filterToggle" onclick="toggleFilters()">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                    <path d="M10 18h4v-2h-4v2zM3 6v2h18V6H3zm3 7h12v-2H6v2z"/>
                </svg>
                Filters
                <span class="filter-badge" id="filterBadge" style="display: none;">0</span>
            </button>
            <div id="activeFilterTags"></div>
        </div>

        <!-- Filter Panel -->
        <div class="filter-panel" id="filterPanel">
            <div class="filter-header">
                <h3>Filter Applications</h3>
                <div class="filter-actions">
                    <button class="filter-btn" onclick="clearFilters()">Clear All</button>
                    <button class="filter-btn primary" onclick="applyFilters()">Apply Filters</button>
                </div>
            </div>
            
            <div class="filter-grid">
                <!-- Business Criticality/Risk -->
                <div class="filter-group">
                    <div class="filter-group-label">Business Criticality/Risk</div>
                    <div class="filter-option">
                        <input type="checkbox" id="risk-high" name="risk" value="high" onchange="updateFilterCount()">
                        <label for="risk-high">High</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="risk-medium" name="risk" value="medium" onchange="updateFilterCount()">
                        <label for="risk-medium">Medium</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="risk-low" name="risk" value="low" onchange="updateFilterCount()">
                        <label for="risk-low">Low</label>
                    </div>
                </div>

                <!-- Application Type -->
                <div class="filter-group">
                    <div class="filter-group-label">Application Type</div>
                    <div class="filter-option">
                        <input type="checkbox" id="type-mobile" name="type" value="mobile" onchange="updateFilterCount()">
                        <label for="type-mobile">Mobile</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="type-web" name="type" value="web" onchange="updateFilterCount()">
                        <label for="type-web">Web</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="type-api" name="type" value="api" onchange="updateFilterCount()">
                        <label for="type-api">API</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="type-thick" name="type" value="thick" onchange="updateFilterCount()">
                        <label for="type-thick">Thick client</label>
                    </div>
                </div>

                <!-- Business Unit -->
                <div class="filter-group">
                    <div class="filter-group-label">Business Unit</div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-content" name="unit" value="content" onchange="updateFilterCount()">
                        <label for="bu-content">Content</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-itom" name="unit" value="itom" onchange="updateFilterCount()">
                        <label for="bu-itom">ITOM</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-cyber" name="unit" value="cyber" onchange="updateFilterCount()">
                        <label for="bu-cyber">Cybersecurity</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-adm" name="unit" value="adm" onchange="updateFilterCount()">
                        <label for="bu-adm">ADM</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-networks" name="unit" value="networks" onchange="updateFilterCount()">
                        <label for="bu-networks">Business Networks</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="bu-portfolio" name="unit" value="portfolio" onchange="updateFilterCount()">
                        <label for="bu-portfolio">Portfolio</label>
                    </div>
                </div>

                <!-- SDLC Status -->
                <div class="filter-group">
                    <div class="filter-group-label">SDLC Status</div>
                    <div class="filter-option">
                        <input type="checkbox" id="sdlc-dev" name="sdlc" value="dev" onchange="updateFilterCount()">
                        <label for="sdlc-dev">Development</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="sdlc-qa" name="sdlc" value="qa" onchange="updateFilterCount()">
                        <label for="sdlc-qa">QA/Test</label>
                    </div>
                    <div class="filter-option">
                        <input type="checkbox" id="sdlc-prod" name="sdlc" value="prod" onchange="updateFilterCount()">
                        <label for="sdlc-prod">Production</label>
                    </div>
                </div>
            </div>
        </div>

        <!-- ===================== PROGRAM DASHBOARD ===================== -->
        <section id="page-program" class="page active" aria-label="Program Dashboard">
            <!-- ROW 1: inventory at-a-glance  -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>Applications</h3>
                    <div class="kpi-inline">
                        <div class="kpi">124</div>
                        <span class="delta positive" aria-label="Up 12 percent year over year" title="+12% vs. last year">
                            <i>â–²</i> 12% YoY
                        </span>
                    </div>
                </div>
                <div class="card col-4">
                    <h3>Versions/Releases</h3>
                    <div class="kpi-inline">
                        <div class="kpi">367</div>
                        <span class="delta positive" aria-label="Up 7 percent year over year" title="+7% vs. last year">
                            <i>â–²</i> 7% YoY
                        </span>
                    </div>
                </div>
                <div class="card col-4">
                    <h3>Users</h3>
                    <div class="kpi-inline">
                        <div class="kpi">342</div>
                        <span class="delta negative" aria-label="Down 3 percent year over year" title="-3% vs. last year">
                            <i>â–¼</i> 3% YoY
                        </span>
                    </div>
                </div>
            </div>

            <!-- ROW 2: OS components / LOC / Files -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>Lines of Code Scanned</h3>
                    <div class="kpi">74.2M</div>
                </div>
                <div class="card col-4">
                    <h3>Files Scanned</h3>
                    <div class="kpi">1.9M</div>
                </div>
                <div class="card col-4">
                    <h3>Open Source Components</h3>
                    <div class="kpi">58,432</div>
                </div>
            </div>

            <!-- ROW 3: Application & Version Coverage -->
            <div class="grid row">
                <!-- Applications -->
                <div class="card col-6">
                    <h3>Application Coverage</h3>

                    <!-- Legend: scanned vs remainder (not scanned) -->
                    <div class="legend-coverage">
                        <span class="sw"><i style="background:var(--c-sast)"></i>SAST</span>
                        <span class="sw"><i style="background:var(--c-dast)"></i>DAST</span>
                        <span class="sw"><i style="background:var(--c-sca)"></i>SCA</span>
                        <span class="sw"><i style="background:var(--c-other)"></i>Other</span>
                        <span class="sw"><i style="background:var(--track)"></i>Not scanned</span>
                    </div>

                    <div class="hbars">
                        <!-- SAST (Applications) -->
                        <div class="hlabel">SAST</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="SAST application coverage"
                                aria-valuemin="0" aria-valuemax="100" aria-valuenow="82"
                                style="--pct:82; --fill-color:var(--c-sast);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">82%</span>

                        <!-- DAST (Applications) -->
                        <div class="hlabel">DAST</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="DAST application coverage"
                                aria-valuemin="0" aria-valuemax="100" aria-valuenow="64"
                                style="--pct:64; --fill-color:var(--c-dast);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">64%</span>

                        <!-- SCA (Applications) -->
                        <div class="hlabel">SCA</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="SCA application coverage" aria-valuemin="0"
                                aria-valuemax="100" aria-valuenow="71" style="--pct:71; --fill-color:var(--c-sca);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">71%</span>

                        <!-- Other (Applications) -->
                        <div class="hlabel">Other</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="Other scan application coverage"
                                aria-valuemin="0" aria-valuemax="100" aria-valuenow="12"
                                style="--pct:12; --fill-color:var(--c-other);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">12%</span>
                    </div>
                </div>

                <!-- Versions -->
                <div class="card col-6">
                    <h3>Version Coverage</h3>

                    <div class="legend-coverage">
                        <span class="sw"><i style="background:var(--c-sast)"></i>SAST</span>
                        <span class="sw"><i style="background:var(--c-dast)"></i>DAST</span>
                        <span class="sw"><i style="background:var(--c-sca)"></i>SCA</span>
                        <span class="sw"><i style="background:var(--c-other)"></i>Other</span>
                        <span class="sw"><i style="background:var(--track)"></i>Not scanned</span>
                    </div>

                    <div class="hbars">
                        <!-- SAST (Versions) -->
                        <div class="hlabel">SAST</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="SAST version coverage" aria-valuemin="0"
                                aria-valuemax="100" aria-valuenow="68" style="--pct:68; --fill-color:var(--c-sast);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">68%</span>

                        <!-- DAST (Versions) -->
                        <div class="hlabel">DAST</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="DAST version coverage" aria-valuemin="0"
                                aria-valuemax="100" aria-valuenow="55" style="--pct:55; --fill-color:var(--c-dast);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">55%</span>

                        <!-- SCA (Versions) -->
                        <div class="hlabel">SCA</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="SCA version coverage" aria-valuemin="0"
                                aria-valuemax="100" aria-valuenow="73" style="--pct:73; --fill-color:var(--c-sca);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">73%</span>

                        <!-- Other (Versions) -->
                        <div class="hlabel">Other</div>
                        <div class="hstack">
                            <div class="hbar" role="progressbar" aria-label="Other scan version coverage"
                                aria-valuemin="0" aria-valuemax="100" aria-valuenow="12"
                                style="--pct:12; --fill-color:var(--c-other);">
                                <div class="fill"></div>
                            </div>
                        </div>
                        <span class="hmeta">12%</span>
                    </div>
                </div>
            </div>

            <!-- ROW 4: Scan Activity (Monthly, grouped bar chart with axes) -->
            <div class="grid row">
                <div class="card col-12">
                    <h3>Scan Activity Trend</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--c-sast)"></i>SAST</span>
                        <span class="sw"><i style="background:var(--c-dast)"></i>DAST</span>
                        <span class="sw"><i style="background:var(--c-sca)"></i>SCA</span>
                        <span class="sw"><i style="background:var(--c-other)"></i>Other</span>
                    </div>
                    <div class="trend-grouped">
                        <div class="y-axis-label">Completed Scans</div>
                        <div class="y-axis">
                            <div>500</div>
                            <div>400</div>
                            <div>300</div>
                            <div>200</div>
                            <div>100</div>
                            <div>0</div>
                        </div>
                        <div class="chart-area">
                            <!-- Jan -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:168px; background:var(--c-sast)" title="SAST: 420"></div>
                                <div class="grouped-bar" style="height:104px; background:var(--c-dast)" title="DAST: 260"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--c-sca)" title="SCA: 200"></div>
                                <div class="grouped-bar" style="height:40px; background:var(--c-other)" title="Other: 100"></div>
                            </div>
                            <!-- Feb -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:152px; background:var(--c-sast)" title="SAST: 380"></div>
                                <div class="grouped-bar" style="height:112px; background:var(--c-dast)" title="DAST: 280"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-sca)" title="SCA: 220"></div>
                                <div class="grouped-bar" style="height:48px; background:var(--c-other)" title="Other: 120"></div>
                            </div>
                            <!-- Mar -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:200px; background:var(--c-sast)" title="SAST: 500"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-dast)" title="DAST: 220"></div>
                                <div class="grouped-bar" style="height:72px; background:var(--c-sca)" title="SCA: 180"></div>
                                <div class="grouped-bar" style="height:32px; background:var(--c-other)" title="Other: 80"></div>
                            </div>
                            <!-- Apr -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:176px; background:var(--c-sast)" title="SAST: 440"></div>
                                <div class="grouped-bar" style="height:104px; background:var(--c-dast)" title="DAST: 260"></div>
                                <div class="grouped-bar" style="height:64px; background:var(--c-sca)" title="SCA: 160"></div>
                                <div class="grouped-bar" style="height:36px; background:var(--c-other)" title="Other: 90"></div>
                            </div>
                            <!-- May -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:184px; background:var(--c-sast)" title="SAST: 460"></div>
                                <div class="grouped-bar" style="height:96px; background:var(--c-dast)" title="DAST: 240"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--c-sca)" title="SCA: 200"></div>
                                <div class="grouped-bar" style="height:44px; background:var(--c-other)" title="Other: 110"></div>
                            </div>
                            <!-- Jun -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:160px; background:var(--c-sast)" title="SAST: 400"></div>
                                <div class="grouped-bar" style="height:120px; background:var(--c-dast)" title="DAST: 300"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-sca)" title="SCA: 220"></div>
                                <div class="grouped-bar" style="height:40px; background:var(--c-other)" title="Other: 100"></div>
                            </div>
                            <!-- Jul -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:144px; background:var(--c-sast)" title="SAST: 360"></div>
                                <div class="grouped-bar" style="height:112px; background:var(--c-dast)" title="DAST: 280"></div>
                                <div class="grouped-bar" style="height:96px; background:var(--c-sca)" title="SCA: 240"></div>
                                <div class="grouped-bar" style="height:48px; background:var(--c-other)" title="Other: 120"></div>
                            </div>
                            <!-- Aug -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:192px; background:var(--c-sast)" title="SAST: 480"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-dast)" title="DAST: 220"></div>
                                <div class="grouped-bar" style="height:72px; background:var(--c-sca)" title="SCA: 180"></div>
                                <div class="grouped-bar" style="height:36px; background:var(--c-other)" title="Other: 90"></div>
                            </div>
                            <!-- Sep -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:168px; background:var(--c-sast)" title="SAST: 420"></div>
                                <div class="grouped-bar" style="height:104px; background:var(--c-dast)" title="DAST: 260"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--c-sca)" title="SCA: 200"></div>
                                <div class="grouped-bar" style="height:40px; background:var(--c-other)" title="Other: 100"></div>
                            </div>
                            <!-- Oct -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:176px; background:var(--c-sast)" title="SAST: 440"></div>
                                <div class="grouped-bar" style="height:96px; background:var(--c-dast)" title="DAST: 240"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--c-sca)" title="SCA: 200"></div>
                                <div class="grouped-bar" style="height:44px; background:var(--c-other)" title="Other: 110"></div>
                            </div>
                            <!-- Nov -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:152px; background:var(--c-sast)" title="SAST: 380"></div>
                                <div class="grouped-bar" style="height:112px; background:var(--c-dast)" title="DAST: 280"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-sca)" title="SCA: 220"></div>
                                <div class="grouped-bar" style="height:48px; background:var(--c-other)" title="Other: 120"></div>
                            </div>
                            <!-- Dec -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:200px; background:var(--c-sast)" title="SAST: 500"></div>
                                <div class="grouped-bar" style="height:88px; background:var(--c-dast)" title="DAST: 220"></div>
                                <div class="grouped-bar" style="height:72px; background:var(--c-sca)" title="SCA: 180"></div>
                                <div class="grouped-bar" style="height:40px; background:var(--c-other)" title="Other: 100"></div>
                            </div>
                        </div>
                        <div class="x-axis">
                            <div class="month-label">Jan</div>
                            <div class="month-label">Feb</div>
                            <div class="month-label">Mar</div>
                            <div class="month-label">Apr</div>
                            <div class="month-label">May</div>
                            <div class="month-label">Jun</div>
                            <div class="month-label">Jul</div>
                            <div class="month-label">Aug</div>
                            <div class="month-label">Sep</div>
                            <div class="month-label">Oct</div>
                            <div class="month-label">Nov</div>
                            <div class="month-label">Dec</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ROW 5: Entitlements (left) + Tech Stack (right) -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Entitlements</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Entitlement ID</th>
                                <th>Units Purchased</th>
                                <th>Units Consumed</th>
                                <th>% Used</th>
                                <th>Start</th>
                                <th>End</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1524</td>
                                <td>500</td>
                                <td>412</td>
                                <td>82%</td>
                                <td>2025-01-01</td>
                                <td>2025-12-31</td>
                            </tr>
                            <tr>
                                <td>1880</td>
                                <td>200</td>
                                <td>190</td>
                                <td class="warn">95%</td>
                                <td>2025-01-01</td>
                                <td>2025-12-31</td>
                            </tr>
                            <tr>
                                <td>2003</td>
                                <td>400</td>
                                <td>240</td>
                                <td>60%</td>
                                <td>2025-01-01</td>
                                <td>2025-12-31</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="card col-6">
                    <h3>Applications by Language</h3>
                    <div class="treemap mt-8">
                        <div class="tree-seg" style="width:38%; background:#3B82F6" title="Java 38%"></div>
                        <div class="tree-seg" style="width:24%; background:#22C55E" title="JavaScript/TypeScript 24%">
                        </div>
                        <div class="tree-seg" style="width:18%; background:#6366F1" title=".NET 18%"></div>
                        <div class="tree-seg" style="width:10%; background:#F97316" title="Python 10%"></div>
                        <div class="tree-seg" style="width:6%; background:#0EA5E9" title="Go 6%"></div>
                        <div class="tree-seg" style="width:4%; background:#64748B" title="Other 4%"></div>
                    </div>
                </div>
            </div>

            <!-- ROW: ScanCentral SAST Utilization -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>ScanCentral SAST Utilization</h3>
                    <div class="hmeta">24 Total Sensors</div>
                    <div class="legend">
                        <span class="sw"><i style="background:#F59E0B"></i>Active Scanning</span>
                        <span class="sw"><i style="background:var(--track)"></i>Idle</span>
                    </div>
                    <div class="donut-wrap">
                        <div class="donut" style="background:conic-gradient(
                #F59E0B 0% 97%,
                var(--track) 97% 100%
              ) !important;"></div>
                        <div>
                            <div style="font-size: 20px; color:#F59E0B; font-weight:500;">97% Active Scanning</div>
                            <div class="muted">3% Idle</div>
                        </div>
                    </div>
                    <div class="muted" style="margin-top: 12px; font-size: 12px;">Last 7 days average utilization</div>
                </div>
                <div class="card col-6">
                    <h3>ScanCentral DAST Utilization</h3>
                    <div class="hmeta">12 Total Sensors</div>
                    <div class="legend">
                        <span class="sw"><i style="background:#22C55E"></i>Active Scanning</span>
                        <span class="sw"><i style="background:var(--track)"></i>Idle</span>
                    </div>
                    <div class="donut-wrap">
                        <div class="donut" style="background:conic-gradient(
                #22C55E 0% 71%,
                var(--track) 71% 100%
              ) !important;"></div>
                        <div>
                            <div style="font-size: 20px; color:#22C55E; font-weight:500;">71% Active Scanning</div>
                            <div class="muted">29% Idle</div>
                        </div>
                    </div>
                    <div class="muted" style="margin-top: 12px; font-size: 12px;">Last 7 days average utilization</div>
                </div>
            </div>

            <!-- KPI Analysis (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ“Š Program Dashboard KPI Analysis</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>Target Personas:</strong> Director of Application Security & Application Security Engineers</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Application Count - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total number of unique applications under security management, with year-over-year growth trend.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Demonstrates program scale for executive reporting and budget justification. Shows portfolio growth requiring resource planning.</li>
                                <li><strong>Engineers:</strong> Helps understand workload scope and prioritize capacity planning.</li>
                                <li><strong>Insights gained:</strong> Portfolio expansion rate, need for additional tooling/headcount.</li>
                                <li><strong>Actions driven:</strong> Budget requests, hiring justifications, tool scaling decisions.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Essential for program sizing and resource planning. High-level metric useful for executives but doesn't indicate security posture. The YoY trend is valuable for growth tracking. Loses points because application count alone doesn't reflect complexity or riskâ€”10 critical apps may matter more than 100 low-risk ones.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Ambiguity:</strong> What constitutes an "application"? Microservices vs. monoliths could skew numbers.</li>
                                <li><strong>False positives:</strong> Growth might look good but could indicate shadow IT discovery (actually a risk indicator).</li>
                                <li><strong>Missing context:</strong> Doesn't show which apps are critical or high-risk, making the raw number less actionable. Use filters to perform this analysis.</li>
                            </ul>
                            </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Versions/Release Count - Value: 6/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total application versions/releases being tracked, showing deployment velocity.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Indicates DevOps maturity and CI/CD adoption rates. Shows program keeping pace with development velocity.</li>
                                <li><strong>Engineers:</strong> Helps gauge scan automation effectiveness and pipeline integration health.</li>
                                <li><strong>Insights gained:</strong> Development velocity, need for automated scanning vs. manual processes.</li>
                                <li><strong>Actions driven:</strong> Pipeline integration priorities, automation investments, version retirement policies.</li>
                            </ul>
                            <p><strong>Value Rating: 6/10</strong><br><em>Reasoning:</em> Useful for understanding scanning workload and DevOps alignment. The 3:1 ratio (versions to apps) indicates healthy release cadence tracking. However, it's somewhat passiveâ€”more versions doesn't necessarily mean better security. Could be a vanity metric if not tied to coverage or risk reduction.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Version sprawl:</strong> High numbers might indicate poor version retirement policies (technical debt).</li>
                                <li><strong>Unclear definition:</strong> Does this include dev/test/prod separately? Internal builds vs. releases?</li>
                                <li><strong>No quality indicator:</strong> Scanning 367 versions means nothing if coverage is poor or findings aren't remediated.</li>
                                <li><strong>Confusion risk:</strong> Users might wonder if 367 is good or bad without industry benchmarks.</li>
                            </ul>
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. User Count - Value: 5/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Number of users accessing the security platform/tools.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Tracks tool adoption across dev teams. Declining users could indicate tool sprawl, team consolidation, or disengagement.</li>
                                <li><strong>Engineers:</strong> Identifies which teams are engaged vs. isolated from security processes.</li>
                                <li><strong>Insights gained:</strong> Platform adoption, training effectiveness, organizational security culture.</li>
                                <li><strong>Actions driven:</strong> User onboarding campaigns, training initiatives, license management, investigating attrition causes.</li>
                                <li><strong>Granularity:</strong> This one is potentially problematic for SSC. It could imply LIM integrations we don't have for licensing purposes (i.e LIM integration gaps) and also won't work for SSO/LDAP integrations. Replace with something else for SSC initially.</li>
                            </ul>
                            <p><strong>Value Rating: 5/10</strong><br><em>Reasoning:</em> The negative delta is a red flag that needs investigationâ€”could indicate cultural issues, tool usability problems, or team attrition. However, user count is a weak proxy for program effectiveness. 10 engaged security champions might be better than 500 casual users. The metric lacks depth without engagement quality measures.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Interpretation ambiguity:</strong> Is declining user count bad (disengagement) or good (consolidation, automation reducing manual work)?</li>
                                <li><strong>No engagement depth:</strong> Active vs. inactive users? Power users vs. occasional logins?</li>
                                <li><strong>Potential frustration:</strong> Engineers seeing this decline might feel demotivated or question program health without explanation.</li>
                                <li><strong>Missing context:</strong> Needs clarification on whether this includes developers, security engineers, or both.</li>
                            </ul>
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Lines of Code Scanned - Value: 4/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total volume of source code analyzed across all applications.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Demonstrates comprehensive program reach and scanning thoroughness. Useful for executive/board presentations showing scale.</li>
                                <li><strong>Engineers:</strong> Validates scanning tool effectiveness and helps estimate scan times/resource needs.</li>
                                <li><strong>Insights gained:</strong> Codebase scale, potential performance bottlenecks, scanning infrastructure adequacy.</li>
                                <li><strong>Actions driven:</strong> Infrastructure scaling, scan optimization, tool performance tuning.</li>
                            </ul>
                            <p><strong>Value Rating: 4/10</strong><br><em>Reasoning:</em> A vanity metric that looks impressive but has limited actionable value. 74M lines could be poorly written legacy code or clean modern codeâ€”quality matters more than quantity. Useful for tool vendor discussions (licensing, capacity) but doesn't indicate security outcomes. Better suited for infrastructure planning than security decision-making.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Classic vanity metric:</strong> More code isn't betterâ€”could indicate bloat or legacy debt.</li>
                                <li><strong>No quality indicator:</strong> Doesn't show code quality, test coverage, or security hygiene.</li>
                                <li><strong>Tool-specific:</strong> Different tools count lines differently (comments, whitespace, generated code).</li>
                                <li><strong>Potential confusion:</strong> Users might focus on growing this number rather than improving security outcomes.</li>
                                <li><strong>Frustration risk:</strong> Engineers might view this as management theater rather than meaningful security measurement.</li>
                            </ul>
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Files Scanned - Value: 3/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total number of files analyzed across the codebase.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Another scale indicator for reporting program comprehensiveness.</li>
                                <li><strong>Engineers:</strong> Helps troubleshoot scan completeness and identify scanning gaps (missing file types).</li>
                                <li><strong>Insights gained:</strong> Scanning thoroughness, potential blind spots in file type coverage.</li>
                                <li><strong>Actions driven:</strong> Scan configuration reviews, file type inclusion rules, build artifact validation.</li>
                            </ul>
                            <p><strong>Value Rating: 3/10</strong><br><em>Reasoning:</em> Even less valuable than LOCâ€”highly dependent on project structure (monorepo vs. microservices, file size conventions). Provides minimal actionable insights. Main use case is technical troubleshooting. Not a business or security outcome metric.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Highly variable:</strong> File organization is arbitraryâ€”same codebase could be 100 or 100,000 files.</li>
                                <li><strong>No security relevance:</strong> File count has no correlation to risk or vulnerability density.</li>
                                <li><strong>Noise factor:</strong> Generated files, dependencies, test files inflate counts without adding value.</li>
                                <li><strong>User frustration:</strong> Engineers will question why this matters for security decisions.</li>
                                <li><strong>Better as diagnostic:</strong> Should be hidden technical metadata rather than a dashboard KPI.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. Open Source Components - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total number of third-party libraries and dependencies tracked across all applications.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Illustrates supply chain risk exposure and SCA program scope. Important for board/compliance discussions about modern development practices.</li>
                                <li><strong>Engineers:</strong> Essential for understanding dependency management workload and license compliance scope.</li>
                                <li><strong>Insights gained:</strong> Dependency footprint, potential license risk, SCA tool effectiveness, supply chain attack surface.</li>
                                <li><strong>Actions driven:</strong> Dependency consolidation initiatives, vulnerability prioritization, license compliance reviews, SCA tool tuning.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Highly relevant in modern development where 80%+ of code is third-party. Directly correlates to supply chain risk (SolarWinds, Log4Shell scenarios). Large number (58K) indicates significant attack surface requiring management. Only loses points for lacking additional context (unique vs. duplicates across apps, vulnerable vs. clean components).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Overwhelming scale:</strong> 58,432 is hugeâ€”could cause analysis paralysis without prioritization strategy.</li>
                                <li><strong>Duplicates:</strong> Does this count the same library used in 100 apps as 1 or 100 components?</li>
                                <li><strong>Transitive dependencies:</strong> Includes indirect dependencies that teams may not be aware of, causing confusion.</li>
                                <li><strong>Actionability gap:</strong> High number needs paired with vulnerability/license risk data to drive decisions.</li>
                                <li><strong>False sense of control:</strong> Tracking â‰  managingâ€”users might assume "tracked" means "secure."</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Application Coverage - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of applications scanned by each security testing tool type.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Critical metric showing program maturity and gaps. Demonstrates CI/CD integration success. Key for audit/compliance evidence.</li>
                                <li><strong>Engineers:</strong> Identifies onboarding gapsâ€”which apps need pipeline integration, which teams need training.</li>
                                <li><strong>Insights gained:</strong> Tool adoption maturity, integration gaps, team engagement levels, potential blind spots.</li>
                                <li><strong>Actions driven:</strong> Onboarding campaigns, pipeline integration sprints, tool standardization, gap analysis for unscanned apps.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional KPIâ€”directly measures program reach and integration effectiveness. The different percentages tell a story (SAST highest = easiest integration, DAST lowest = harder to automate). Actionable gaps (36% without DAST = prioritization target). Audit-friendly metric. Only loses 1 point because coverage doesn't guarantee scan quality or finding remediation.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Coverage â‰  quality:</strong> Scanning with misconfigured tools or ignoring results still counts as "covered."</li>
                                <li><strong>Threshold confusion:</strong> Is 82% good? Users need benchmarks or targets.</li>
                                <li><strong>Binary simplification:</strong> One scan six months ago still counts as "covered"â€”needs recency dimension.</li>
                                <li><strong>Multiple counting:</strong> Apps with multiple scan types get counted multiple timesâ€”not additive.</li>
                                <li><strong>Potential frustration:</strong> Low DAST at 64% might demoralize teams without context (DAST is genuinely harder).</li>
                            </ul>
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Version Coverage - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of application versions/releases scanned by each tool type.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Shows pipeline integration depthâ€”scanning apps vs. scanning every release. Lower than application coverage indicates manual/periodic scanning vs. automated.</li>
                                <li><strong>Engineers:</strong> Reveals CI/CD automation gaps. Identifies which apps scan manually vs. in pipelines.</li>
                                <li><strong>Insights gained:</strong> Automation maturity, pipeline integration success, scanning frequency patterns.</li>
                                <li><strong>Actions driven:</strong> Pipeline automation priorities, shift-left initiatives, breaking build policies, release gate enforcement.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Extremely valuableâ€”the gap between application coverage (82% SAST) and version coverage (68% SAST) reveals that ~14% of apps scan periodically rather than per-release. This identifies shift-left gaps. Version-level scanning is the gold standard for modern AppSec. Directly measures DevSecOps maturity.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Complexity:</strong> Two similar metrics (Application vs. Version Coverage) might confuse usersâ€”requires clear labeling.</li>
                                <li><strong>Recency bias:</strong> Old versions with scans inflate the percentageâ€”needs "scanned in last 30 days" filter.</li>
                                <li><strong>Potential frustration:</strong> Lower than application coverage might look like regression without explanation that it's actually a harder/better standard.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Scan Activity (12-month trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Monthly volume of scans by tool type (SAST, DAST, SCA, Other) over 12 months, showing activity patterns.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Demonstrates program consistency and growth over time. Identifies seasonality (e.g., dips during holidays) or concerning drops requiring investigation.</li>
                                <li><strong>Engineers:</strong> Reveals pipeline health, build failures, tool outages, or team disengagement patterns.</li>
                                <li><strong>Insights gained:</strong> Scanning consistency, seasonal patterns, tool reliability, automation health.</li>
                                <li><strong>Actions driven:</strong> Investigating activity drops, capacity planning for peak periods, tool reliability improvements.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Solid operational health metric. Consistent activity indicates mature automation; erratic patterns signal problems. The stacked view shows tool balance. Useful for spotting infrastructure issues (sudden drops = outage). Loses points for being activity-focused rather than outcome-focusedâ€”high activity could mean scanning the same code repeatedly without fixing issues.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Activity â‰  effectiveness:</strong> Many scans don't mean better securityâ€”could indicate noisy tools or inefficient processes.</li>
                                <li><strong>Scale ambiguity:</strong> Y-axis height is illustrative onlyâ€”actual numbers matter. Is 50 scans/month good or bad?</li>
                                <li><strong>Noise vs. signal:</strong> Re-scans, duplicate scans, failed scans all count as activity.</li>
                                <li><strong>Interpretation complexity:</strong> Drops could be good (version consolidation) or bad (broken automation).</li>
                                <li><strong>Potential confusion:</strong> Users might try to read absolute values from relative bar heights without numeric labels.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Entitlements (FoD only) - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> License consumption tracking showing purchased units vs. used units for each entitlement ID, with utilization percentages and contract dates.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Critical for budget management, renewal planning, and avoiding surprise overages. 95% usage (ID 1880) flags urgent renewal/expansion need.</li>
                                <li><strong>Engineers:</strong> Helps manage scan quotas, prioritize high-value scans, avoid hitting license limits mid-sprint.</li>
                                <li><strong>Insights gained:</strong> License utilization efficiency, upcoming renewal timing, expansion needs, potential overspending on underused licenses.</li>
                                <li><strong>Actions driven:</strong> Renewal negotiations, license expansion requests, entitlement consolidation, usage optimization.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional business metricâ€”directly impacts budget and operational continuity. The 95% warning on entitlement 1880 is high-value alerting. Proactive visibility prevents service disruptions. Essential for financial planning. Loses 1 point for being purely operational rather than security-focused, but financial continuity enables security programs.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Jargon:</strong> "Entitlement ID" is vendor-speakâ€”unclear to non-procurement personas.</li>
                                <li><strong>Context missing:</strong> What does each entitlement cover? (e.g., 1524 = SAST, 1880 = DAST?).</li>
                                <li><strong>Action ambiguity:</strong> 95% utilizationâ€”should we reduce usage or buy more? Depends on business criticality.</li>
                                <li><strong>Renewal anxiety:</strong> Seeing 95% might create stress without clear escalation paths.</li>
                                <li><strong>Granularity:</strong> Needs drill-down to understand what's consuming units (which apps/teams).</li>
                                <li><strong>Granularity:</strong> This will not be applicable to SSC. Instead, we could consider showing a summary of SCSAST and SCDAST sensors deployed, which demonstrates consumption (if on scan model) and scope/maturity of program.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">11. Apps by Tech Stack (Treemap) - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Application portfolio distribution across programming languages/frameworks (Java 38%, JavaScript 24%, .NET 18%, Python 10%, Go 6%, Other 4%).</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Strategic workforce planningâ€”hiring specialists, tool selection prioritization, training budget allocation.</li>
                                <li><strong>Engineers:</strong> Understand coverage priority (Java tooling is most critical). Identify niche language support needs.</li>
                                <li><strong>Insights gained:</strong> Language diversity, technology debt, hiring needs, tool investment priorities.</li>
                                <li><strong>Actions driven:</strong> Tool evaluations (prioritize Java-focused tools), training plans, specialist hiring, standardization initiatives.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Excellent strategic planning metric. Directly informs tool selection (38% Java = must have strong Java analysis). Highlights diversity (6 languages = complex tooling needs). Useful for hiring (need .NET specialists?). Treemap is visually efficient for proportions. Loses points for being staticâ€”doesn't show trends (are we consolidating or fragmenting?).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Static snapshot:</strong> No trend dataâ€”is Java growing or shrinking? Affects long-term strategy.</li>
                                <li><strong>Oversimplification:</strong> "JavaScript/TypeScript" is actually React, Angular, Vue, Node.jsâ€”very different architectures.</li>
                                <li><strong>False equivalence:</strong> One Python app might be 5M LOC, while 20 Go apps are 100K LOC eachâ€”counts hide complexity.</li>
                                <li><strong>Actionability:</strong> Interesting but what should users do with this? Needs paired with "languages with most critical vulns" to drive decisions.</li>
                                <li><strong>Potential frustration:</strong> High fragmentation (6+ languages) might overwhelm small teams without clear rationalization strategy.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">12. ScanCentral SAST Utilization - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Real-time utilization of ScanCentral SAST sensor pool, showing percentage of sensors actively scanning vs. idle over the past 7 days.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Monitors infrastructure capacity to ensure scan requests don't queue. Justifies additional sensor licenses when utilization consistently exceeds 85-90%. Demonstrates ROI on ScanCentral investment to executives.</li>
                                <li><strong>Engineers:</strong> Identifies bottlenecks causing scan delays. Coordinates scan schedules during low-utilization periods. Troubleshoots sensor health issues when utilization is unexpectedly low.</li>
                                <li><strong>Insights gained:</strong> High utilization (>90%) indicates capacity constraints requiring additional sensors or workload distribution. Low utilization (<40%) may indicate over-provisioning or underutilization of SAST capabilities. Idle sensors may represent offline/unhealthy infrastructure requiring maintenance.</li>
                                <li><strong>Actions driven:</strong> License expansion requests, sensor provisioning adjustments, scan scheduling optimization, infrastructure maintenance alerts.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Critical operational metric for organizations using ScanCentral SAST. High utilization directly impacts developer experience (scan queue times) and security coverage (delayed feedback). Unlike cloud-based scanning, on-premise sensor pools are fixed capacity requiring proactive monitoring. Essential for capacity planning and budget justification. Only loses 1 point because it's specific to ScanCentral deployments (not applicable to pure cloud/FoD-only environments).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Threshold interpretation:</strong> "Healthy" utilization varies by organizationâ€”80% may be optimal for one team but indicate saturation for another with bursty scan patterns. Consider showing peak vs. average utilization.</li>
                                <li><strong>Misleading low utilization:</strong> Low percentage could indicate offline sensors (infrastructure problem) vs. genuine idle capacity (good). Include sensor health status to differentiate.</li>
                                <li><strong>Temporal patterns:</strong> 7-day average masks daily patterns (e.g., 100% utilization during business hours, 0% nights/weekends). Consider showing hourly heatmap or peak usage times.</li>
                                <li><strong>Queue depth missing:</strong> High utilization alone doesn't show if scans are queuing. Pair with "Average Queue Wait Time" metric for complete picture.</li>
                            </ul>
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>SSC Only:</strong> This metric is only applicable to Software Security Center (SSC) deployments using ScanCentral SAST. Fortify on Demand (FoD) is a cloud-based service with elastic scanning capacity managed by OpenTextâ€”customers do not manage sensor pools and therefore have no need for utilization monitoring.</li>
                                <li><strong>Data Source:</strong> ScanCentral SAST sensor data is available via SSC's ScanCentral Controller API. Primary endpoint: <code>GET /scancentral-ctrl/api/v2/sensors</code> returns list of registered sensors with status information including: <code>sensorId</code>, <code>state</code> (IDLE, SCANNING, OFFLINE), <code>lastSeenTime</code>, <code>currentJobId</code>, <code>sensorPoolId</code>.</li>
                                <li><strong>Utilization Calculation:</strong> Query sensor states at regular intervals (e.g., every 5-15 minutes) over the past 7 days. Calculate percentage of time each sensor spent in SCANNING state vs. IDLE state. Aggregate across all sensors for pool-level utilization. Formula: <code>(Total SCANNING minutes across all sensors) / (Total available minutes across all sensors) Ã— 100</code>. Exclude OFFLINE sensors from denominator to avoid inflating "idle" percentage.</li>
                                <li><strong>API Implementation:</strong> Use <code>GET /scancentral-ctrl/api/v2/sensors</code> for current sensor states. For historical utilization, query ScanCentral job history: <code>GET /scancentral-ctrl/api/v2/jobs?startDate={7_days_ago}&endDate={now}</code> returns completed jobs with <code>sensorId</code>, <code>startTime</code>, <code>endTime</code>. Calculate scan duration per sensor, sum across 7 days, divide by total available time (7 days Ã— 24 hours Ã— number of sensors).</li>
                                <li><strong>Sensor Pool Filtering:</strong> Organizations may have multiple sensor pools (e.g., dedicated pools for high-priority apps, different technology stacks). Use <code>sensorPoolId</code> filter to calculate per-pool utilization if needed. Dashboard shows aggregate across all pools by default.</li>
                                <li><strong>Health Check Integration:</strong> Cross-reference sensor states with <code>lastSeenTime</code> to identify offline/unhealthy sensors. Sensors not reporting within 10 minutes should be flagged as OFFLINE and excluded from utilization calculation. Consider adding "X sensors offline" alert to dashboard.</li>
                                <li><strong>Performance Optimization:</strong> Cache utilization calculations and refresh every 15-30 minutes rather than real-time queries. Historical job data for 7 days can be expensive to query repeatedly. Store aggregated hourly utilization in intermediate database table for faster dashboard rendering.</li>
                                <li><strong>Data Quality:</strong> Handle edge cases: (1) Sensors joining/leaving pool mid-periodâ€”use time-weighted average. (2) Controller downtime periodsâ€”exclude from calculation or show data gap warning. (3) Job failures/cancellationsâ€”include in utilization time if sensor was actively working before failure.</li>
                                <li><strong>Threshold Configuration:</strong> Warning threshold (orange) typically 85-95% utilization depending on scan volume variability. Critical threshold (red) at 95%+ indicates immediate capacity constraints. Make thresholds configurable per organization.</li>
                                <li><strong>Alternative Metrics:</strong> Consider supplementing with: (1) Average queue wait time, (2) Number of queued scans, (3) Peak utilization hour/day of week, (4) Per-sensor utilization distribution (identify underutilized sensors).</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">13. ScanCentral DAST Utilization - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Real-time utilization of ScanCentral DAST sensor pool, showing percentage of sensors actively scanning vs. idle over the past 7 days.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Monitors DAST scanning capacity to prevent bottlenecks in CI/CD pipelines. Justifies additional WebInspect sensor licenses when utilization exceeds 80-85%. Balances investment between SAST and DAST infrastructure based on relative utilization rates.</li>
                                <li><strong>Engineers:</strong> Schedules long-running DAST scans during off-peak hours to maximize sensor availability. Identifies when DAST scans are queuing due to capacity constraints. Troubleshoots sensor health issues affecting scan execution.</li>
                                <li><strong>Insights gained:</strong> DAST sensors typically have lower utilization than SAST (scans are longer but less frequent). Utilization spikes may correlate with release cycles or compliance deadlines. Idle capacity enables ad-hoc security testing without impacting scheduled scans.</li>
                                <li><strong>Actions driven:</strong> DAST license optimization, scan schedule adjustments, capacity planning for peak periods, infrastructure maintenance during low-utilization windows.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Critical for organizations with on-premise DAST infrastructure. DAST scans are resource-intensive and long-running (hours to days), making capacity management more complex than SAST. Lower rating than SAST utilization (8 vs. 9) because DAST is typically lower volumeâ€”fewer applications scanned, less frequent cadence. Still highly valuable for preventing pipeline delays and optimizing license costs. Not applicable to FoD-only or third-party DAST tool users.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Long scan duration variability:</strong> Single DAST scan can occupy sensor for 4-48 hours, making utilization percentage less meaningful than queue depth. A sensor at "50% utilization" could have 12-hour scans every day or 24-hour scans every other dayâ€”different capacity implications.</li>
                                <li><strong>Scheduled vs. on-demand scans:</strong> High utilization from scheduled compliance scans leaves no capacity for ad-hoc penetration testing. Consider showing "reserved" vs. "available" capacity separately.</li>
                                <li><strong>Misleading idle percentage:</strong> Low utilization may indicate insufficient DAST adoption (coverage gap) rather than excess capacity. Pair with "DAST Coverage %" metric to diagnose root cause.</li>
                                <li><strong>Sensor types:</strong> Some organizations run multiple DAST sensor types (standard vs. high-memory for complex apps). Aggregate metric may mask specific sensor pool saturation.</li>
                            </ul>
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>SSC Only:</strong> This metric is only applicable to Software Security Center (SSC) deployments using ScanCentral DAST (Fortify WebInspect Enterprise). Fortify on Demand (FoD) provides cloud-based DAST scanning with managed capacityâ€”customers do not operate sensor pools and therefore do not need utilization monitoring.</li>
                                <li><strong>Data Source:</strong> ScanCentral DAST sensor data is available via SSC's ScanCentral DAST Controller API. Primary endpoint: <code>GET /wi-controller/api/v1/sensors</code> returns list of registered WebInspect sensors with fields: <code>sensorId</code>, <code>sensorName</code>, <code>status</code> (Available, Scanning, Offline, Maintenance), <code>currentScanId</code>, <code>lastHeartbeat</code>, <code>sensorPoolId</code>.</li>
                                <li><strong>Utilization Calculation:</strong> Query sensor states at regular intervals (e.g., every 15-30 minutes) over the past 7 days. Calculate percentage of time each sensor spent in "Scanning" state vs. "Available" state. Aggregate across all sensors for pool-level utilization. Formula: <code>(Total scanning hours across all sensors) / (Total available hours across all sensors) Ã— 100</code>. Exclude sensors in "Offline" or "Maintenance" states from denominator to avoid inflating idle percentage.</li>
                                <li><strong>API Implementation:</strong> Use <code>GET /wi-controller/api/v1/sensors</code> for current sensor states. For historical utilization, query scan history: <code>GET /wi-controller/api/v1/scans?startDate={7_days_ago}&endDate={now}&status=Completed,Failed,Cancelled</code> returns scans with <code>sensorId</code>, <code>startTime</code>, <code>endTime</code>, <code>duration</code>. Sum scan durations per sensor over 7 days, divide by total available time (7 days Ã— 24 hours Ã— number of sensors).</li>
                                <li><strong>Scan Duration Handling:</strong> DAST scans are long-running (average 4-12 hours, some up to 48+ hours for complex applications). Unlike SAST (minutes to hours), a single DAST scan can monopolize a sensor for an entire day. Calculate utilization based on actual scan runtime, not job submission time. Include failed scans in utilization if sensor was actively working before failure (consumed capacity even if scan didn't complete successfully).</li>
                                <li><strong>Sensor Pool Management:</strong> Organizations often segregate DAST sensors into pools: (1) Standard scans (small/medium apps), (2) High-memory scans (large enterprise apps), (3) Compliance scans (PCI-DSS, scheduled). Use <code>sensorPoolId</code> to calculate per-pool utilization. Dashboard can show aggregate across all pools or drill-down view by pool type.</li>
                                <li><strong>Health Monitoring:</strong> Cross-reference sensor status with <code>lastHeartbeat</code> timestamp. Sensors not reporting heartbeat within 15 minutes should be flagged as "Offline" and excluded from available capacity calculations. Unlike SAST sensors (lightweight), DAST sensors may go offline due to network issues, high memory consumption, or Windows Update rebootsâ€”monitor health proactively.</li>
                                <li><strong>Scheduled vs. On-Demand Capacity:</strong> Consider tracking separately: (1) <strong>Scheduled utilization</strong>â€”scans triggered by recurring schedule (compliance, nightly builds). (2) <strong>On-demand utilization</strong>â€”ad-hoc scans initiated by engineers/security team. High scheduled utilization with low on-demand availability indicates insufficient capacity for exploratory testing. Requires tagging scans with source (scheduled vs. manual) via custom metadata.</li>
                                <li><strong>Performance Optimization:</strong> Cache utilization calculations and refresh every 30-60 minutes. Querying 7 days of DAST scan history is expensive due to long scan durations and large result sets. Store pre-aggregated hourly utilization in database table. Use incremental updates (only query scans completed since last cache refresh) to minimize API load.</li>
                                <li><strong>Data Quality Considerations:</strong> (1) Paused scansâ€”exclude paused time from utilization calculation (sensor was available during pause). (2) Sensor maintenance windowsâ€”exclude from available capacity during scheduled maintenance. (3) Scan retriesâ€”count each retry attempt separately in utilization (consumed capacity even if previous attempt failed). (4) Multi-scan workloadsâ€”some sensors can run multiple lightweight scans concurrently; adjust capacity calculation if parallel scanning is enabled.</li>
                                <li><strong>Threshold Configuration:</strong> Warning threshold (orange) typically 70-85% utilization for DAST (lower than SAST due to longer scan durations and less predictable runtime). Critical threshold (red) at 85%+ indicates capacity constraints. Thresholds should be configurable and may vary by sensor pool type (high-memory pools may tolerate higher utilization).</li>
                                <li><strong>Alternative/Supplemental Metrics:</strong> (1) <strong>Average scan queue wait time</strong>â€”time from scan submission to sensor assignment. (2) <strong>Number of queued scans</strong>â€”count of scans waiting for available sensor. (3) <strong>Longest running scan</strong>â€”identify scans monopolizing sensors for extended periods. (4) <strong>Scan success rate</strong>â€”percentage of scans completing successfully (low rate may indicate sensor instability, not capacity issue). (5) <strong>Per-application scan frequency</strong>â€”ensure high-priority apps get regular DAST coverage despite capacity constraints.</li>
                                <li><strong>Integration with SAST Utilization:</strong> Display both SAST and DAST utilization side-by-side to enable capacity investment decisions. If SAST utilization is 95% and DAST is 40%, prioritize adding SAST sensors. If both are high (>85%), consider whether some applications could shift from DAST to SAST-only or alternate scan frequencies (monthly DAST instead of weekly).</li>
                            </ul>
                        </div>
                    </details>

                    <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--accent); border-radius: 8px;">
                        <h4 style="margin-top: 0;">ðŸ“ˆ Summary Dashboard Value Rankings</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px;">
                            <div>
                                <h5 style="color: var(--accent);">Highest Value (9-10/10) â­</h5>
                                <ol style="margin: 8px 0;">
                                    <li>Application Coverage (9/10)</li>
                                    <li>Version Coverage (9/10)</li>
                                    <li>Entitlements (9/10)</li>
                                    <li>ScanCentral SAST Utilization (9/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--accent); margin-top: 16px;">High Value (7-8/10)</h5>
                                <ol start="5" style="margin: 8px 0;">
                                    <li>Open Source Components (8/10)</li>
                                    <li>Tech Stack (8/10)</li>
                                    <li>ScanCentral DAST Utilization (8/10)</li>
                                    <li>Applications (7/10)</li>
                                    <li>Scan Activity Trend (7/10)</li>
                                </ol>
                            </div>
                            <div>
                                <h5 style="color: var(--text-muted);">Medium Value (5-6/10)</h5>
                                <ol start="10" style="margin: 8px 0;">
                                    <li>Versions/Releases (6/10)</li>
                                    <li>Users (5/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--text-muted); margin-top: 16px;">Low Value (3-4/10) âš ï¸</h5>
                                <ol start="12" style="margin: 8px 0;">
                                    <li>Lines of Code (4/10)</li>
                                    <li>Files Scanned (3/10)</li>
                                </ol>
                            </div>
                        </div>
                        <p style="margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border);"><strong>Overall Assessment:</strong> The Program Dashboard effectively balances operational metrics (entitlements, activity, ScanCentral utilization) with strategic indicators (coverage, tech stack). The ScanCentral utilization metrics are critical for on-premise deployments, providing essential capacity planning and infrastructure monitoring capabilities. The coverage metrics remain standout KPIs, while the volume metrics (LOC, files) are weak vanity metrics that could be replaced with more actionable data (e.g., "Apps with outdated scans," "Critical apps without DAST coverage").</p>
                        </div>
                    </details>
                </div>
            </div>


            <!-- Design & Implementation Notes (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">Program Dashboard Design & Implementation Notes</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>AI-Generated Suggestions:</strong> Based on analysis of OpenAPI Specs for FoD/SSC and the Program Dashboard requirements. Consider this a starting point, not definitive.  There will be errors.</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Application Count - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design & Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count only active applications. Exclude archived, retired, or inactive apps.</li>
                                <li><strong>Fortify on Demand:</strong> <code>GET /api/v3/applications?offset=0&limit=50</code> returns <code>ApplicationListResponse</code> with fields: <code>applicationId</code> (int64), <code>applicationName</code> (string), <code>applicationDescription</code> (string), <code>applicationTypeId</code> (int32), <code>businessCriticalityType</code> (string enum: High|Medium|Low), <code>emailList</code> (string), <code>ownerId</code> (int32), <code>createdDate</code> (date-time). Use <code>totalCount</code> from response for accurate count. For active apps only, no specific filter existsâ€”rely on absence of deletion (deleted apps don't appear in results). Use pagination for large portfolios (max <code>limit=50</code> per request). Optional filters: <code>filters=businessCriticalityType:High</code> (field:value syntax, multiple with + for AND, | for OR). Optional sorting: <code>orderBy=applicationName</code>, <code>orderByDirection=ASC</code>. For YoY growth, filter by <code>modifiedStartDate</code> (ISO 8601 date-time format, e.g., <code>2024-01-01T00:00:00Z</code>) to count apps created in past year. <em>Limitations:</em> No explicit "retiredDate" filter like Releases APIâ€”applications are simply removed when deleted rather than soft-deleted. Cannot directly filter out applications with no releases; must cross-reference with <code>GET /api/v3/applications/{applicationId}/releases</code> if needed.</li>
                                <li><strong>Software Security Center:</strong> 
                                    <p><strong>âš ï¸ IMPLEMENTATION NOTE:</strong> SSC does not have a native "Application" entity separate from "Project Versions." SSC's data model is version-centric, where each Project contains multiple Project Versions (equivalent to FoD's Releases). To calculate Application Count in SSC, you must query project versions and deduplicate by unique project identifiers.</p>
                                    <p><strong>Primary Endpoint:</strong> <code>GET /api/v1/projectVersions?start=0&limit=200</code> returns <code>ApiResultListProjectVersion</code> with fields including: <code>id</code> (int64, projectVersion ID), <code>name</code> (string, version name), <code>description</code> (string), <code>project</code> (object containing project metadata), <code>project.id</code> (int64, unique project identifier), <code>project.name</code> (string, project/application name), <code>project.description</code> (string), <code>project.issueTemplateId</code> (string), <code>creationDate</code> (date-time), <code>active</code> (boolean), <code>committed</code> (boolean), <code>issueCount</code> (int64), <code>bugTrackerEnabled</code> (boolean). Use <code>count</code> field in response for total projectVersion count.</p>
                                    <p><strong>Application Count Calculation Workflow:</strong></p>
                                    <ol>
                                        <li><strong>Query all project versions:</strong> <code>GET /api/v1/projectVersions?start=0&limit=200&fields=id,name,project&includeInactive=false</code>. The <code>includeInactive=false</code> parameter (default) excludes inactive versions. Default limit is 200 (higher than FoD's 50). Use <code>start</code> for pagination offset.</li>
                                        <li><strong>Extract unique projects:</strong> Parse the <code>project.id</code> field from each projectVersion. Create a Set/Dictionary to collect unique <code>project.id</code> values.</li>
                                        <li><strong>Count unique projects:</strong> The size of the unique <code>project.id</code> set is your Application Count. For example, if you have 367 project versions across 123 unique <code>project.id</code> values, your Application Count = 123.</li>
                                        <li><strong>Optional project metadata enrichment:</strong> If you need project-level details (creation date, description), use <code>GET /api/v1/projects?start=0&limit=200&fields=id,name,description,issueTemplateId</code>. This endpoint provides project-level data but is less commonly used since most SSC workflows are version-centric.</li>
                                        <li><strong>YoY growth tracking:</strong> SSC does not expose <code>creationDate</code> at the project level directly via <code>/projects</code> endpoint schema (field not documented). Alternative: track the earliest <code>creationDate</code> among all project versions for a given <code>project.id</code> as a proxy for "application creation date." Filter versions created in past year using query parameter: <code>q=creationDate:[2024-01-01T00:00:00.000Z TO *]</code> (SSC uses Lucene-style date range queries). Then deduplicate by <code>project.id</code> to count new applications.</li>
                                        <li><strong>Handle inactive/retired projects:</strong> Use <code>includeInactive=false</code> (default behavior) to exclude inactive project versions. If a project has zero active versions remaining, it will naturally not appear in resultsâ€”this is equivalent to a "retired application" in FoD.</li>
                                    </ol>
                                    <p><strong>Query Parameters:</strong></p>
                                    <ul>
                                        <li><code>start</code> (int, default 0): Pagination offset. Increment by <code>limit</code> value for subsequent pages.</li>
                                        <li><code>limit</code> (int, default 200): Maximum results per request. SSC supports up to 200 (vs FoD's 50). Use -1 or 0 for unlimited (not recommended for large portfolios due to performance).</li>
                                        <li><code>fields</code> (string): Comma-separated list of fields to return. Optimize by requesting only <code>id,project</code> for deduplication. Example: <code>fields=id,project.id,project.name</code>.</li>
                                        <li><code>q</code> (string): Lucene-style search query. Examples: <code>q=project.name:MyApp*</code> (wildcard), <code>q=creationDate:[2024-01-01T00:00:00.000Z TO *]</code> (date range), <code>q=active:true</code> (boolean). Combine with AND: <code>q=active:true AND creationDate:[2024-01-01 TO *]</code>.</li>
                                        <li><code>qm</code> (string, default ""): Query mode. Use <code>qm=adv</code> for advanced Lucene syntax. Leave empty for basic search.</li>
                                        <li><code>includeInactive</code> (boolean, default false): Set to <code>true</code> to include inactive project versions. Typically leave as default to exclude retired versions.</li>
                                        <li><code>orderby</code> (string): Sort field. Example: <code>orderby=project.name</code> or <code>orderby=-creationDate</code> (descending with minus prefix).</li>
                                        <li><code>withoutCount</code> (boolean, default false): Set to <code>true</code> to disable computing total count (improves performance for large datasets if you only need paginated results).</li>
                                    </ul>
                                    <p><strong>Response Schema Key Fields:</strong></p>
                                    <ul>
                                        <li><code>data</code> (array): List of ProjectVersion objects.</li>
                                        <li><code>count</code> (int): Total number of project versions matching query (use for version count, not application count).</li>
                                        <li><code>data[].id</code> (int64): Project version unique identifier.</li>
                                        <li><code>data[].project</code> (object): Embedded project metadata.</li>
                                        <li><code>data[].project.id</code> (int64): **CRITICAL FIELD** for application deduplication. This is the unique project/application identifier.</li>
                                        <li><code>data[].project.name</code> (string): Project/application name.</li>
                                        <li><code>data[].creationDate</code> (date-time): ProjectVersion creation timestamp (ISO 8601 format). Use for YoY calculations.</li>
                                        <li><code>data[].active</code> (boolean): Whether project version is active (true) or inactive/retired (false).</li>
                                    </ul>
                                    <p><strong>âš ï¸ CRITICAL LIMITATIONS & UNCERTAINTIES:</strong></p>
                                    <ul>
                                        <li><strong>No native Application entity:</strong> Unlike FoD's <code>/applications</code> endpoint, SSC requires client-side deduplication logic. This adds complexity and potential for errors if not implemented carefully.</li>
                                        <li><strong>Project creation date uncertainty:</strong> SSC's <code>/projects</code> endpoint schema does not clearly document a <code>creationDate</code> field at project level. You must infer "application creation date" from the earliest <code>projectVersion.creationDate</code> for each <code>project.id</code>. This is a workaround, not a native feature.</li>
                                        <li><strong>Performance considerations:</strong> Querying all project versions (<code>limit=200</code> or higher) and deduplicating client-side can be slow for portfolios with 1000+ versions. Consider caching results and refreshing periodically (hourly/daily) rather than real-time queries.</li>
                                        <li><strong>No "deleted" state:</strong> SSC does not soft-delete projects like FoD. When a project is deleted, it's removed entirely from the database. Cannot track "applications deleted this year" for historical analysis without external audit log parsing.</li>
                                        <li><strong>Retired vs Inactive confusion:</strong> SSC uses <code>active</code> boolean flag on project versions. A project with all inactive versions is effectively "retired," but there's no explicit project-level retirement status. Ensure your deduplication logic excludes projects with only inactive versions if you want "active applications" count.</li>
                                        <li><strong>Advanced query syntax complexity:</strong> SSC's Lucene-style queries (<code>q</code> parameter) are powerful but less intuitive than FoD's simple <code>filters=field:value</code> syntax. Requires learning Lucene query syntax (wildcards, ranges, boolean operators). Example: <code>q=creationDate:[2024-01-01T00:00:00.000Z TO 2024-12-31T23:59:59.999Z] AND active:true</code>.</li>
                                    </ul>
                                    <p><strong>Implementation Best Practices:</strong></p>
                                    <ul>
                                        <li><strong>Optimize field selection:</strong> Use <code>fields=id,project.id,project.name,active,creationDate</code> to minimize payload size and improve performance.</li>
                                        <li><strong>Pagination strategy:</strong> For large portfolios (500+ versions), fetch in batches of 200. Accumulate all <code>project.id</code> values before deduplicating to ensure accurate count.</li>
                                        <li><strong>Caching:</strong> Application count changes infrequently. Cache results for 1-24 hours depending on your organization's onboarding velocity. Refresh on a schedule rather than per-user-request.</li>
                                        <li><strong>YoY delta calculation:</strong> Query versions created in past year: <code>q=creationDate:[2024-11-06T00:00:00.000Z TO *]</code> (replace with 1 year ago from current date). Deduplicate to get new applications. Subtract from current total to get prior year total. Calculate percentage change.</li>
                                        <li><strong>Error handling:</strong> Handle empty results gracefully (new SSC instance). Validate that <code>project.id</code> exists before deduplication (some versions may have null parent if corrupted).</li>
                                        <li><strong>Alternative approach:</strong> If your organization maintains a CMDB or external application registry, consider using that as source of truth for application count and treating SSC as secondary validation. SSC's version-centric model makes it suboptimal for application-level reporting.</li>
                                    </ul>
                                    <p><strong>Example Implementation (Pseudocode):</strong></p>
                                    <pre>uniqueProjects = new Set()
allVersions = []
start = 0
limit = 200

// Fetch all active project versions
do {
  response = GET "/api/v1/projectVersions?start={start}&limit={limit}&includeInactive=false&fields=id,project.id,project.name,active"
  allVersions.addAll(response.data)
  start += limit
} while (response.data.length == limit)  // Continue if full page returned

// Deduplicate by project.id
for (version in allVersions) {
  if (version.project && version.project.id) {
    uniqueProjects.add(version.project.id)
  }
}

applicationCount = uniqueProjects.size()</pre>
                                    <p><strong>Data Quality Considerations:</strong></p>
                                    <ul>
                                        <li><strong>Orphaned versions:</strong> Extremely rare, but if a project is deleted while versions exist (data corruption), those versions may have null <code>project</code> references. Filter these out: <code>if (version.project && version.project.id)</code>.</li>
                                        <li><strong>Test/sandbox versions:</strong> SSC has no built-in "application type" like FoD's <code>applicationTypeId</code>. Use naming conventions or custom attributes to filter out test applications. Example: <code>q=NOT project.name:*-test</code> or <code>q=NOT project.name:sandbox*</code>.</li>
                                        <li><strong>Multi-tenant environments:</strong> If your SSC instance serves multiple business units, use custom attributes or naming prefixes to segment applications by BU. SSC API does not have built-in BU/tenant filtering like some FoD deployments.</li>
                                    </ul>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD uses "Applications" while SSC uses "Projects" (containing versions). Normalize to "Applications" for display.</li>
                                <li><strong>YoY Calculation:</strong> Store historical snapshots monthly (ideally). As fallback, use application/version created date to exclude applicaitons or versions from the past year. Compare current count to same month previous year. For FoD, use <code>modifiedStartDate</code> filter to get apps modified/created since last year for delta calculation.</li>
                                <li><strong>Data Quality:</strong> Handle edge cases where apps have no scans (still count) vs. deleted/archived apps (exclude). FoD automatically excludes deleted apps from API responses.</li>
                                <li><strong>Implementation Gap:</strong> SSC requires manual tracking of application-level metadata since it's version-centric. May need custom attributes to track application lifecycle status.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Versions/Release Count - Value: 6/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design & Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Include active versions only. Exclude retired/archived versions.</li>
                                <li><strong>Fortify on Demand:</strong> <code>GET /api/v3/releases?offset=0&limit=50</code> returns <code>ReleaseListResponse</code> with fields: <code>releaseId</code> (int32), <code>releaseName</code> (string), <code>releaseDescription</code> (string), <code>releaseCreatedDate</code> (date-time), <code>applicationId</code> (int32), <code>applicationName</code> (string), <code>sdlcStatusType</code> (string enum: Production|QA|Development|Retired), <code>ownerId</code> (int32), <code>suspended</code> (boolean). Use <code>totalCount</code> for accurate count. To exclude retired releases: <code>filters=sdlcStatusType:Production|QA|Development</code> (OR condition using pipe separator). Cannot directly filter by <code>suspended=false</code> (filtering not supported for <code>suspended</code> field), so post-process results client-side to exclude suspended releases if needed. Alternative approach: use <code>GET /api/v3/applications/{applicationId}/releases</code> to get per-application release lists, then aggregate. Use <code>modifiedStartDate</code> parameter (ISO 8601 format) to filter releases created/modified after specific date for YoY calculations. Optional sorting: <code>orderBy=releaseName</code>, <code>orderByDirection=ASC|DESC</code>. Optional field selection: <code>fields=releaseId,releaseName,sdlcStatusType</code> (comma-separated). <em>Best Practice:</em> For production releases only, use <code>filters=sdlcStatusType:Production</code>. For all non-retired: <code>filters=sdlcStatusType:Production|QA|Development</code>.</li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âœ… DIRECT SUPPORT:</strong> Unlike Application Count, Version Count is natively supported in SSC since the platform is version-centric. Project Versions are the primary entity, making this metric straightforward.</p>
                                    <p><strong>Primary Endpoint:</strong> <code>GET /api/v1/projectVersions?start=0&limit=200&includeInactive=false</code> returns <code>ApiResultListProjectVersion</code>. Use the <code>count</code> field in response for total active project version countâ€”this is your Version Count metric.</p>
                                    <p><strong>Version Count Calculation Workflow:</strong></p>
                                    <ol>
                                        <li><strong>Simple count query:</strong> <code>GET /api/v1/projectVersions?start=0&limit=1&includeInactive=false</code>. You only need the <code>count</code> field from responseâ€”no need to fetch full data. Setting <code>limit=1</code> minimizes payload size while still returning total count.</li>
                                        <li><strong>Extract count:</strong> Parse <code>response.count</code> field. This is the total number of active project versions across all projects.</li>
                                        <li><strong>Filter by SDLC status (if needed):</strong> SSC doesn't have built-in SDLC status like FoD's <code>sdlcStatusType</code> enum. However, you can use custom attributes. If your org configured a custom attribute for environment (Production/QA/Dev), query with: <code>q=attributeName:Production</code>. Consult your SSC administrator for custom attribute names.</li>
                                        <li><strong>YoY growth tracking:</strong> Query versions created in past year: <code>GET /api/v1/projectVersions?q=creationDate:[2024-11-06T00:00:00.000Z TO *]&includeInactive=false&start=0&limit=1</code>. Use <code>count</code> field to get new versions created since 1 year ago. Subtract from current total to get prior year count. Calculate percentage change.</li>
                                        <li><strong>Exclude inactive/retired versions:</strong> Always use <code>includeInactive=false</code> (default behavior). This excludes versions marked as inactive, equivalent to FoD's "Retired" status.</li>
                                    </ol>
                                    <p><strong>Query Parameters:</strong></p>
                                    <ul>
                                        <li><code>start</code> (int, default 0): Pagination offset. Not needed if you only want <code>count</code>â€”use <code>start=0&limit=1</code>.</li>
                                        <li><code>limit</code> (int, default 200): Results per page. Use <code>limit=1</code> for count-only queries to optimize performance.</li>
                                        <li><code>includeInactive</code> (boolean, default false): **CRITICAL PARAMETER**. Set to <code>false</code> (or omit) to exclude retired/inactive versions. This gives you "active version count" matching FoD's non-retired releases.</li>
                                        <li><code>q</code> (string): Lucene-style query for filtering. Examples: <code>q=creationDate:[2024-01-01 TO *]</code> (versions created since Jan 2024), <code>q=project.name:MyApp*</code> (versions for specific project wildcard match), <code>q=issueCount:[1 TO *]</code> (versions with at least 1 issue). Combine filters with AND: <code>q=active:true AND creationDate:[2024-01-01 TO *]</code>.</li>
                                        <li><code>fields</code> (string): Comma-separated field list. For count-only queries, use <code>fields=id</code> to minimize response payload. Not necessary if using <code>limit=1</code>.</li>
                                        <li><code>withoutCount</code> (boolean, default false): Must be <code>false</code> (default) to return <code>count</code> field. Do not set to <code>true</code> for this metric.</li>
                                    </ul>
                                    <p><strong>Response Schema Key Fields:</strong></p>
                                    <ul>
                                        <li><code>count</code> (int): **PRIMARY METRIC FIELD**. Total number of project versions matching query criteria. This is your Version Count.</li>
                                        <li><code>data</code> (array): List of ProjectVersion objects. Empty or minimal if using <code>limit=1</code> for count-only query.</li>
                                        <li><code>data[].id</code> (int64): Project version unique identifier.</li>
                                        <li><code>data[].name</code> (string): Version name/label (e.g., "v2.5", "Release 3.0", "Sprint 42").</li>
                                        <li><code>data[].active</code> (boolean): Active status. Should always be <code>true</code> if <code>includeInactive=false</code>.</li>
                                        <li><code>data[].creationDate</code> (date-time): Version creation timestamp. Used for YoY calculations.</li>
                                    </ul>
                                    <p><strong>SDLC Environment Filtering (Custom Implementation):</strong></p>
                                    <p>âš ï¸ <strong>UNCERTAINTY:</strong> SSC does not have a native SDLC status field like FoD's <code>sdlcStatusType:Production|QA|Development|Retired</code>. Organizations typically implement this using:</p>
                                    <ul>
                                        <li><strong>Custom Attributes:</strong> SSC supports custom attributes on project versions. Common practice: create "Environment" attribute with values: Production, QA, Development, Retired. Query with <code>GET /api/v1/projectVersions/{id}/attributes</code> to retrieve. However, filtering by custom attributes in <code>/projectVersions</code> list query is complexâ€”requires attribute ID lookup first.</li>
                                        <li><strong>Naming Conventions:</strong> Some orgs encode environment in version name (e.g., "MyApp-v2.5-PROD", "MyApp-v2.5-QA"). Use Lucene queries: <code>q=name:*-PROD</code> to filter production versions. Less robust than custom attributes.</li>
                                        <li><strong>Separate SSC Instances:</strong> Enterprise deployments may run separate SSC instances for Production vs Non-Production. In this case, query each instance separately and aggregate counts.</li>
                                    </ul>
                                    <p><strong>Example: Count Only Production Versions (if using naming convention):</strong></p>
                                    <pre>GET /api/v1/projectVersions?q=name:*-PROD OR name:*-Production&includeInactive=false&start=0&limit=1</pre>
                                    <p>Response: <code>{ "count": 142, "data": [...] }</code> â†’ Production Version Count = 142</p>
                                    <p><strong>Example: YoY Growth Calculation:</strong></p>
                                    <ol>
                                        <li>Get current total: <code>GET /api/v1/projectVersions?includeInactive=false&limit=1</code> â†’ <code>count: 367</code></li>
                                        <li>Get versions created in past year: <code>GET /api/v1/projectVersions?q=creationDate:[2024-11-06T00:00:00.000Z TO *]&includeInactive=false&limit=1</code> â†’ <code>count: 52</code></li>
                                        <li>Calculate prior year total: 367 - 52 = 315</li>
                                        <li>Calculate YoY growth: ((367 - 315) / 315) Ã— 100 = +16.5%</li>
                                    </ol>
                                    <p><strong>Performance Optimization:</strong></p>
                                    <ul>
                                        <li><strong>Count-only queries:</strong> Use <code>limit=1&fields=id</code> to minimize data transfer. SSC still computes total <code>count</code> but returns minimal payload.</li>
                                        <li><strong>Caching:</strong> Version count changes more frequently than application count but still relatively stable. Cache for 15-60 minutes depending on your organization's deployment velocity.</li>
                                        <li><strong>Avoid large result sets:</strong> Never use <code>limit=-1</code> (unlimited) or <code>limit=0</code> for large portfolios (1000+ versions). Always use pagination with reasonable limits (200 max).</li>
                                    </ul>
                                    <p><strong>Data Quality Considerations:</strong></p>
                                    <ul>
                                        <li><strong>Active vs Inactive:</strong> Always use <code>includeInactive=false</code> for "current version count." Including inactive versions inflates numbers with historical/retired versions no longer relevant.</li>
                                        <li><strong>Test/sandbox versions:</strong> Unlike FoD, SSC has no built-in mechanism to exclude test versions. Options: (1) Use naming conventions and filter with <code>q=NOT name:*-test</code>, (2) Use custom attribute for "version type", (3) Create separate SSC applications for test vs production.</li>
                                        <li><strong>Committed vs Uncommitted:</strong> SSC project versions have a <code>committed</code> boolean field. Uncommitted versions are "in setup" and may not have scan results yet. Consider filtering: <code>q=committed:true</code> for "production-ready" version count.</li>
                                    </ul>
                                    <p><strong>âš ï¸ CRITICAL LIMITATIONS & UNCERTAINTIES:</strong></p>
                                    <ul>
                                        <li><strong>No native SDLC status:</strong> Unlike FoD's clean <code>sdlcStatusType</code> enum, SSC requires custom implementation (attributes, naming conventions, or separate instances). This adds complexity and potential inconsistency across teams.</li>
                                        <li><strong>Custom attribute filtering complexity:</strong> If using custom attributes for environment type, you cannot directly filter in <code>/projectVersions</code> list query by attribute value. Must either: (1) Fetch all versions and filter client-side (slow), or (2) Use advanced search with attribute GUID (requires knowing attribute definition IDs from <code>/api/v1/attributeDefinitions</code>).</li>
                                        <li><strong>Naming convention brittleness:</strong> Relying on <code>name:*-PROD</code> queries breaks if teams don't follow naming standards consistently. No enforcement mechanism.</li>
                                        <li><strong>Committed field interpretation:</strong> Not all organizations use <code>committed:true</code> consistently. Some commit immediately on creation; others delay. Check with SSC administrators before using as filter.</li>
                                    </ul>
                                    <p><strong>Implementation Best Practices:</strong></p>
                                    <ul>
                                        <li><strong>Start simple:</strong> Use <code>GET /api/v1/projectVersions?includeInactive=false&limit=1</code> to get total active version count. This is the most reliable baseline metric.</li>
                                        <li><strong>Add SDLC filtering cautiously:</strong> Only implement production/QA/dev filtering if your organization has consistent custom attribute usage or naming conventions. Document assumptions clearly.</li>
                                        <li><strong>YoY tracking:</strong> Use <code>creationDate</code> range queries for growth analysis. This is reliable since <code>creationDate</code> is system-generated and immutable.</li>
                                        <li><strong>Dashboard presentation:</strong> If unable to reliably filter by SDLC status, present total active version count only. Avoid showing misleading breakdowns (Production vs Non-Production) if data isn't trustworthy.</li>
                                        <li><strong>Alternative metric:</strong> Consider "Versions with Recent Scans" (versions with artifacts uploaded in past 90 days) as a proxy for "actively maintained versions" if SDLC filtering is unreliable. Query: <code>GET /api/v1/projectVersions?q=lastScanDate:[NOW-90DAYS TO *]</code> (date math syntax).</li>
                                    </ul>
                                </li>
                                <li><strong>Version Lifecycle:</strong> Define clear retention policy. SSC allows marking versions as "inactive"; FoD uses "retired" status via <code>sdlcStatusType:Retired</code>.</li>
                                <li><strong>Data Quality:</strong> Exclude "sandbox" or "test" versions that aren't real releases. Use naming conventions or custom attributes. In FoD, leverage <code>sdlcStatusType</code> field to filterâ€”only count Production, QA, and Development releases.</li>
                                <li><strong>Platform Parity:</strong> Both platforms support this metric well. No significant gaps. FoD has explicit SDLC status, making filtering cleaner than SSC's inactive flag.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. User Count - Value: 5/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design & Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count unique users who are not inactive. Exclude service accounts, API tokens, and system accounts.</li>
                                <li><strong>FoD vs SSC:</strong> 
                                    <ul>
                                        <li><strong>FoD:</strong> <code>GET /api/v3/users?offset=0&limit=50</code> returns <code>UserListResponse</code> with fields: <code>userId</code>, <code>userName</code>, <code>email</code>, <code>firstName</code>, <code>lastName</code>, <code>roleId</code>, <code>roleText</code>, <code>isSuspended</code>, <code>isPasswordExpirationEnabled</code>. Use <code>totalCount</code> for accurate user count. To exclude suspended/inactive users: <code>filters=isSuspended:false</code> (field:value syntax, use + for AND conditions). Cannot directly filter by "never logged in" or "last login date" via API (fields not exposed in filters)â€”may need to query individual user details with <code>GET /api/v3/users/{userId}</code> if login activity needed. To exclude service accounts, typically filter by naming convention if your org uses one (e.g., <code>filters=userName:svc_*</code> with NOT logic isn't supported, must filter client-side). Personal Access Tokens (PATs) and API Keys are managed separately via <code>/api/v3/personal-access-tokens</code> and <code>/api/v3/api-keys</code> endpoints (don't count toward user count). <em>Note:</em> FoD tracks all user types (local, SSO, LDAP) but login activity metadata isn't directly exposed in Users APIâ€”must use audit logs or separate analytics if you need "active users in past 30 days" metric.</li>
                                        <li><strong>SSC:</strong> <em>CRITICAL LIMITATION:</em> SSC does not reliably track user login activity, especially with SSO/LDAP. User count may not be available without custom tracking. No equivalent lightweight Users API like FoDâ€”must query LDAP/AD directly if SSO is used, or use SSC's limited local user management.</li>
                                    </ul>
                                </li>
                                <li><strong>SSO/LDAP Issue:</strong> With SSO integration, SSC doesn't maintain login timestamps. FoD integrates with SSO/LDAP but also doesn't expose detailed login activity via Users API. Consider excluding this metric for SSC deployments or using alternative data source (audit logs, SIEM data).</li>
                                <li><strong>Alternative Metric:</strong> For SSC, consider tracking "Active Contributors" (users who created/modified issues, ran scans) via audit log instead of raw user count. For FoD, consider querying <code>/api/v3/eventlogs/download</code> (last 24 hours of audit data in CSV) to identify truly active users, but this is resource-intensive.</li>
                                <li><strong>License Management:</strong> FoD has built-in user licensing tracked via Users API. SSC may require separate License Management (LIM) integration which isn't always configuredâ€”if you need licensed user counts, that's a different system.</li>
                                <li><strong>Recommendation:</strong> This card may not be appropriate for SSC environments due to SSO/LDAP limitations. Consider replacement metric like "Scans per Week" or "Active Development Teams" instead. For FoD, count total users with <code>isSuspended:false</code>, acknowledging you cannot easily filter out "dormant accounts that haven't logged in for 90+ days" without external audit log analysis.</li>
                                <li><strong>Data Quality:</strong> If implementing for FoD, filter out suspended accounts using <code>filters=isSuspended:false</code>. Manually exclude obvious service accounts by naming pattern (e.g., filter client-side to exclude usernames starting with "svc_" or "api_"). Do not count API keys or PATs (separate endpoints). Cannot programmatically distinguish "never logged in" vs "logged in last year" via Users API alone.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Lines of Code Scanned - Value: 4/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design & Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Sum LOC from all completed SAST scans. Use most recent scan per application version.  Exclude retired/inactive versions.</li>
                                <li><strong>Fortify on Demand:</strong> GET /api/v3/scans/{scanId}/summary returns ScanSummary with staticScanSummaryDetails.totalLinesOfCode (int32). First, query GET /api/v3/scans?completedOnStartDate=YYYY-MM-DDT00:00:00Z&completedOnEndDate=YYYY-MM-DDT23:59:59Z&offset=0&limit=50 to get list of scans (ScanListResponse with scanId, scanTypeId, releaseId, completedDateTime). Filter scanTypeId=1 (Static) and analysisStatusType=Completed. For each scanId, call GET /api/v3/scans/{scanId}/summary to get totalLinesOfCode. Response includes staticScanSummaryDetails (totalLinesOfCode, fileCount, engineVersion, rulePackVersion). Aggregate across all scans, deduplicating per release (use latest scan per releaseId). Query parameters: startedOnStartDate, startedOnEndDate, completedOnStartDate, completedOnEndDate (date-time format), modifiedStartDate, orderBy, orderByDirection (ASC|DESC), fields, offset, limit (max 50). Best practice: Filter by completedOnStartDate/completedOnEndDate for monthly aggregates, or fetch all scans and dedupe client-side for total portfolio LOC. Limitations: Scan summary endpoint may return 202 Accepted if scan processing not complete (retry after delay).</li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC does not expose a dedicated Lines of Code metric via REST API like FoD's <code>staticScanSummaryDetails.totalLinesOfCode</code>. LOC data may be embedded in artifact metadata or performance indicators, but access is inconsistent and scanner-dependent.</p>
                                    
                                    <p><strong>Primary Approach - Performance Indicators (Variable Availability):</strong></p>
                                    <p><code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories</code> may contain LOC data if configured. This endpoint returns historical metrics tracked for the project version.</p>
                                    <ol>
                                        <li><strong>Query performance indicators:</strong> <code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories?start=0&limit=200</code></li>
                                        <li><strong>Look for LOC-related indicators:</strong> Performance indicator names vary by configuration. Common names: <code>LOC</code>, <code>Lines of Code</code>, <code>Total LOC</code>, <code>NCLOC</code> (Non-Comment LOC). Check <code>indicator.name</code> field.</li>
                                        <li><strong>Extract latest value:</strong> Performance indicators have history arrays. Use most recent <code>value</code> entry.</li>
                                        <li><strong>âš ï¸ LIMITATION:</strong> Performance indicators are optional configuration. Many SSC deployments do not track LOC via this mechanism. If no LOC indicator exists, this approach fails.</li>
                                    </ol>
                                    
                                    <p><strong>Alternative Approach - Artifacts Metadata (Complex Parsing):</strong></p>
                                    <p>LOC data may be embedded in uploaded FPR artifact metadata. This requires parsing artifact details or downloading FPR files.</p>
                                    <ol>
                                        <li><strong>Query artifacts:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts?start=0&limit=200</code> returns list of uploaded scan artifacts.</li>
                                        <li><strong>Filter SAST artifacts:</strong> Look for artifacts with <code>artifact.scanType</code> matching static analysis: <code>engineType: "Fortify SCA"</code> or similar.</li>
                                        <li><strong>Check metadata fields:</strong> Some artifacts may have <code>artifact.metadata</code> object containing LOC. Field names vary: <code>LOC</code>, <code>linesOfCode</code>, <code>sourceFilesCount</code>. Schema is not standardized.</li>
                                        <li><strong>âš ï¸ CRITICAL LIMITATION:</strong> Artifact metadata structure is not consistently documented in SSC API spec. Field availability depends on Fortify SCA version, scan configuration, and upload method. This approach is unreliable without custom parsing logic per scanner version.</li>
                                        <li><strong>FPR parsing (not recommended):</strong> As last resort, download FPR file using <code>GET /api/v1/fileTokens</code> for token, then <code>/download/artifactDownload.html?mat={token}</code> to download FPR. Parse XML inside FPR for <code>&lt;LOC&gt;</code> tags. This is extremely complex and beyond typical REST API usageâ€”requires FPR SDK or custom XML parsing.</li>
                                    </ol>
                                    
                                    <p><strong>Response Schema (Performance Indicators):</strong></p>
                                    <ul>
                                        <li><code>data</code> (array): List of PerformanceIndicatorHistory objects.</li>
                                        <li><code>data[].id</code> (int64): Indicator ID.</li>
                                        <li><code>data[].name</code> (string): Indicator name (e.g., "LOC", "Lines of Code").</li>
                                        <li><code>data[].guid</code> (string): Unique identifier for indicator type.</li>
                                        <li><code>data[].history</code> (array): Time-series values.</li>
                                        <li><code>data[].history[].timestamp</code> (date-time): When value was recorded.</li>
                                        <li><code>data[].history[].value</code> (number): LOC count at that timestamp.</li>
                                    </ul>
                                    
                                    <p><strong>Response Schema (Artifacts):</strong></p>
                                    <ul>
                                        <li><code>data</code> (array): List of Artifact objects.</li>
                                        <li><code>data[].id</code> (int64): Artifact ID.</li>
                                        <li><code>data[].engineType</code> (string): Scanner type (e.g., "Fortify SCA", "WebInspect").</li>
                                        <li><code>data[].scanType</code> (string): Scan category (e.g., "STATIC", "DYNAMIC").</li>
                                        <li><code>data[].uploadDate</code> (date-time): When artifact was uploaded.</li>
                                        <li><code>data[].status</code> (string): Processing status (e.g., "PROCESS_COMPLETE").</li>
                                        <li><code>data[].metadata</code> (object): **MAY** contain LOC fields (not guaranteed, schema varies).</li>
                                    </ul>
                                    
                                    <p><strong>Example Implementation (Pseudocode):</strong></p>
                                    <pre>totalLOC = 0
versionsWithLOC = 0

projectVersions = GET "/api/v1/projectVersions?includeInactive=false&start=0&limit=200"

for (version in projectVersions.data) {
  // Try performance indicators first
  try {
    indicators = GET "/api/v1/projectVersions/{version.id}/performanceIndicatorHistories"
    locIndicator = indicators.data.find(ind => ind.name.match(/LOC|Lines of Code|NCLOC/i))
    
    if (locIndicator && locIndicator.history.length > 0) {
      // Get most recent value
      latestValue = locIndicator.history[locIndicator.history.length - 1].value
      totalLOC += latestValue
      versionsWithLOC++
      continue  // Found LOC via performance indicators, skip artifact parsing
    }
  } catch { }
  
  // Fallback: Try artifacts metadata
  try {
    artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts?orderby=-uploadDate&limit=1"
    if (artifacts.data.length > 0 && artifacts.data[0].engineType == "Fortify SCA") {
      artifact = artifacts.data[0]
      // Try various metadata field names
      loc = artifact.metadata?.LOC || artifact.metadata?.linesOfCode || artifact.metadata?.loc
      if (loc) {
        totalLOC += parseInt(loc)
        versionsWithLOC++
      }
    }
  } catch { }
}

// Display metric with confidence indicator
display = totalLOC + " LOC across " + versionsWithLOC + " versions (" + 
          (versionsWithLOC / projectVersions.count * 100) + "% coverage)"</pre>
                                    
                                    <p><strong>âš ï¸ CRITICAL LIMITATIONS & UNCERTAINTIES:</strong></p>
                                    <ul>
                                        <li><strong>No native LOC endpoint:</strong> Unlike FoD's <code>staticScanSummaryDetails.totalLinesOfCode</code>, SSC has no standardized LOC field in core API responses. LOC tracking is optional configuration, not default behavior.</li>
                                        <li><strong>Performance indicator availability:</strong> The <code>/performanceIndicatorHistories</code> endpoint only returns data if administrators configured LOC tracking as a performance indicator. Many SSC instances do not have this configured, making the metric unavailable.</li>
                                        <li><strong>Artifact metadata inconsistency:</strong> Even when LOC is embedded in artifact metadata, field names vary by Fortify SCA version (19.x vs 20.x vs 22.x vs 23.x vs 24.x). No standardized schema across versions.</li>
                                        <li><strong>Scanner-specific variations:</strong> Native Fortify SCA scans may include LOC metadata; imported scans from third-party SAST tools likely do not. This creates data gaps.</li>
                                        <li><strong>No aggregation API:</strong> Unlike FoD where you can aggregate scan summaries, SSC requires iterating every project version and every artifact individually to collect LOC dataâ€”extremely slow for 500+ versions.</li>
                                        <li><strong>Stale data risk:</strong> If performance indicators are used, values may be stale (updated monthly/quarterly rather than per scan). Artifact metadata is scan-specific but requires finding the most recent SAST artifact per version.</li>
                                        <li><strong>Counting methodology unknown:</strong> Even if LOC values are found, SSC does not document whether counts include comments, whitespace, generated code, or only executable lines. FoD consistently uses executable LOC; SSC methodology may vary by scanner configuration.</li>
                                        <li><strong>No API documentation clarity:</strong> The SSC OpenAPI spec does not explicitly define LOC fields or guarantee their presence. Implementation requires trial-and-error testing against your specific SSC instance.</li>
                                    </ul>
                                    
                                    <p><strong>Implementation Best Practices:</strong></p>
                                    <ul>
                                        <li><strong>Pre-implementation validation:</strong> BEFORE building this metric, sample 10-20 project versions in your SSC instance. Check if <code>/performanceIndicatorHistories</code> contains LOC indicators. If not, check artifact metadata. If both are unavailable, this metric cannot be reliably implemented.</li>
                                        <li><strong>Fallback metric:</strong> If LOC is unavailable, consider replacing this KPI with "Artifacts Uploaded" count or "Scan Frequency" (scans per month)â€”both are reliably available via <code>/artifacts</code> endpoint.</li>
                                        <li><strong>Coverage indicator:</strong> If LOC is available for only subset of versions (e.g., 60%), display coverage alongside metric: "74.2M LOC (60% of versions have LOC data)." This shows metric confidence.</li>
                                        <li><strong>Background aggregation required:</strong> Do not attempt real-time aggregation of LOC across 500+ versions. Implement nightly batch job that caches results. Querying performance indicators or artifacts for every version on every dashboard load will timeout.</li>
                                        <li><strong>Error handling:</strong> Gracefully handle missing LOC data. If a version has no performance indicators and no SAST artifacts, skip it (don't fail entire dashboard).</li>
                                        <li><strong>Scanner filtering:</strong> When parsing artifact metadata, filter to <code>engineType: "Fortify SCA"</code> only. WebInspect (DAST) and SCA tools do not produce LOC counts.</li>
                                        <li><strong>Metadata field name mapping:</strong> Create configuration map of known LOC field names by Fortify SCA version. Test against your SSC instance to determine which field names are used. Common variants: <code>LOC</code>, <code>linesOfCode</code>, <code>loc</code>, <code>totalLOC</code>, <code>NCLOC</code> (non-comment), <code>sourceLines</code>.</li>
                                        <li><strong>Custom attribute alternative:</strong> If your organization considers LOC important, configure SSC custom attributes to capture LOC from scan uploads. Then query via <code>/api/v1/projectVersions/{id}/attributes</code> with known attribute GUID. This is more reliable than parsing metadata but requires SSC administrator setup.</li>
                                    </ul>
                                    
                                    <p><strong>Data Quality Considerations:</strong></p>
                                    <ul>
                                        <li><strong>Partial coverage:</strong> Expect LOC data for only 40-70% of project versions depending on SSC configuration and scanner usage patterns. Document this limitation clearly.</li>
                                        <li><strong>Incremental vs full scans:</strong> Some artifacts represent incremental scans (delta), others full codebase scans. Metadata may not distinguish. This can cause double-counting or undercounting if not handled carefully.</li>
                                        <li><strong>Multi-language projects:</strong> Projects with mixed languages (Java + JavaScript) may have multiple artifacts with separate LOC counts. Decide whether to sum them (total codebase LOC) or deduplicate (avoid double-counting shared files).</li>
                                        <li><strong>Test code inclusion:</strong> Some Fortify SCA scans include test directories, others exclude them. LOC counts may vary significantly depending on scan configurationâ€”makes cross-project comparison unreliable.</li>
                                    </ul>
                                    
                                    <p><strong>Recommendation for SSC Environments:</strong></p>
                                    <p>Given the high uncertainty and complexity of extracting LOC from SSC, consider these alternatives:</p>
                                    <ul>
                                        <li><strong>Option 1: Replace with "Artifacts Uploaded" count:</strong> Reliable metric available via <code>/artifacts</code> endpoint. Shows scanning activity without LOC complexity.</li>
                                        <li><strong>Option 2: Use external LOC tool:</strong> Tools like SonarQube, Coverity, or GitHub Linguist provide reliable LOC metrics. Display those instead of extracting from SSC.</li>
                                        <li><strong>Option 3: Manual configuration:</strong> Have project teams manually enter LOC via SSC custom attributes during onboarding. Query attributes rather than artifact metadata.</li>
                                        <li><strong>Option 4: Display "Data Not Available":</strong> If SSC LOC extraction fails validation testing, show "N/A" in dashboard with tooltip explaining SSC limitation. Avoid showing misleading partial data.</li>
                                        <li><strong>Option 5: FoD-only metric:</strong> If your organization uses both platforms, only display LOC for FoD-scanned applications where data is reliable. Leave SSC applications blank or use alternative metric.</li>
                                    </ul>
                                </li>
                                <li><strong>LOC Counting Variability:</strong> Fortify SAST counts executable LOC (excludes comments/whitespace). FoD totalLinesOfCode field is consistent across scans; SSC may vary based on scanner version.</li>
                                <li><strong>Deduplication:</strong> Use latest SAST scan per version/release combination. For FoD, filter scans by releaseId, sort by completedDateTime DESC, take first result per release.</li>
                                <li><strong>Scan Type Limitation:</strong> This metric only applies to SAST scans. DAST and SCA don't measure LOC. Make this explicit in the card. Filter scans by scanTypeId=1 (Static) or scanType='Static' enum.</li>
                                <li><strong>Data Availability:</strong> 
                                    <ul>
                                        <li><strong>FoD:</strong> LOC readily available in all static scan results via staticScanSummaryDetails.totalLinesOfCode. Field is always populated for completed static scans.</li>
                                        <li><strong>SSC:</strong> LOC may not be available if scans imported from non-Fortify tools or if metadata wasn't captured. May show "N/A" for some scans.</li>
                                    </ul>
                                </li>
                                <li><strong>Performance:</strong> For large portfolios (1000+ releases), cache LOC totals and refresh daily rather than summing in real-time. Use batch queries with date filters to minimize API calls (GET /api/v3/scans with completedOnStartDate filter rather than per-scan lookups). Pagination required (max limit=50 scans per request).</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Files Scanned - Value: 3/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count all source files analyzed during SAST scans. Same logic as LOC - most recent SAST scan per version.  Exclude inactive/retired.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li>GET /api/v3/scans/{scanId}/summary returns ScanSummary with staticScanSummaryDetails.fileCount (int32). Same workflow as LOC metric: First query GET /api/v3/scans with filters (scanTypeId=1 for Static, analysisStatusType=Completed), then for each scanId call GET /api/v3/scans/{scanId}/summary to retrieve fileCount. Response structure: staticScanSummaryDetails object contains fileCount, totalLinesOfCode, engineVersion, rulePackVersion, buildDate, payLoadSize. Aggregate across all scans, deduplicating per releaseId (use latest scan per release). Filter parameters: completedOnStartDate, completedOnEndDate (ISO 8601 date-time e.g. 2024-01-01T00:00:00Z), offset, limit (max 50). Best practice: Fetch scans in monthly batches using completedOnStartDate/completedOnEndDate filters to improve performance. FoD counts only files actually analyzed by Fortify SAST (excludes files filtered out by scan configuration).</li>
                                        <li>Limitations: GET /api/v3/scans/{scanId}/summary may return 202 Accepted if scan still processingâ€”implement retry logic with exponential backoff. fileCount is SAST-only metric; not available for DAST/Mobile scans.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ SIMILAR LIMITATIONS TO LOC:</strong> SSC does not expose a standardized "files scanned" count via REST API like FoD's <code>staticScanSummaryDetails.fileCount</code>. This metric faces the same challenges as Lines of Codeâ€”optional configuration, inconsistent metadata, and scanner-dependent availability.</p>
                                    
                                    <p><strong>Primary Approach - Performance Indicators (Variable Availability):</strong></p>
                                    <p><code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories</code> may contain file count data if configured.</p>
                                    <ol>
                                        <li><strong>Query performance indicators:</strong> <code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories?start=0&limit=200</code></li>
                                        <li><strong>Look for file-related indicators:</strong> Common names: <code>Files</code>, <code>File Count</code>, <code>Files Scanned</code>, <code>Source Files</code>, <code>totalFiles</code>. Check <code>indicator.name</code> field.</li>
                                        <li><strong>Extract latest value:</strong> Use most recent <code>value</code> from history array.</li>
                                        <li><strong>âš ï¸ LIMITATION:</strong> Like LOC, file count indicators are optional. Most SSC deployments do not track this metric via performance indicators.</li>
                                    </ol>
                                    
                                    <p><strong>Alternative Approach - Artifacts Metadata:</strong></p>
                                    <ol>
                                        <li><strong>Query artifacts:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts?start=0&limit=200&orderby=-uploadDate</code></li>
                                        <li><strong>Filter SAST artifacts:</strong> <code>engineType: "Fortify SCA"</code>, <code>scanType: "STATIC"</code>, <code>status: "PROCESS_COMPLETE"</code></li>
                                        <li><strong>Parse metadata:</strong> Look for fields: <code>artifact.metadata.fileCount</code>, <code>artifact.metadata.filesScanned</code>, <code>artifact.metadata.sourceFiles</code>. Schema varies by Fortify SCA version.</li>
                                        <li><strong>Use latest artifact per version:</strong> Sort by <code>uploadDate</code> descending, take first SAST artifact.</li>
                                        <li><strong>âš ï¸ UNCERTAINTY:</strong> Metadata field availability is inconsistent. May require custom parsing logic per scanner version.</li>
                                    </ol>
                                    
                                    <p><strong>Alternative Approach - Source Files Endpoint (If Available):</strong></p>
                                    <p>âš ï¸ <strong>EXPERIMENTAL:</strong> SSC may have <code>GET /api/v1/projectVersions/{id}/sourceFiles</code> endpoint for retrieving analyzed source files. However, this is not well-documented and may not exist in all SSC versions.</p>
                                    <ol>
                                        <li><strong>Test endpoint availability:</strong> Try <code>GET /api/v1/projectVersions/{id}/sourceFiles?start=0&limit=1</code> for a known project version.</li>
                                        <li><strong>If available, use count field:</strong> Response may include <code>count</code> field showing total source files analyzed.</li>
                                        <li><strong>âš ï¸ WARNING:</strong> This endpoint may return ALL files in FPR (including libraries, resources), not just analyzed source code. Count may be inflated compared to FoD's fileCount.</li>
                                        <li><strong>Filtering challenges:</strong> No clear way to distinguish source files from dependencies or generated code via API.</li>
                                    </ol>
                                    
                                    <p><strong>Example Implementation (Pseudocode):</strong></p>
                                    <pre>totalFiles = 0
versionsWithFileCount = 0

projectVersions = GET "/api/v1/projectVersions?includeInactive=false&start=0&limit=200"

for (version in projectVersions.data) {
  // Try performance indicators
  try {
    indicators = GET "/api/v1/projectVersions/{version.id}/performanceIndicatorHistories"
    fileIndicator = indicators.data.find(ind => ind.name.match(/Files|File Count|Source Files/i))
    
    if (fileIndicator && fileIndicator.history.length > 0) {
      latestValue = fileIndicator.history[fileIndicator.history.length - 1].value
      totalFiles += latestValue
      versionsWithFileCount++
      continue
    }
  } catch { }
  
  // Fallback: Artifacts metadata
  try {
    artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts?orderby=-uploadDate&limit=1"
    if (artifacts.data.length > 0 && artifacts.data[0].engineType == "Fortify SCA") {
      artifact = artifacts.data[0]
      fileCount = artifact.metadata?.fileCount || artifact.metadata?.filesScanned || 
                  artifact.metadata?.sourceFiles || artifact.metadata?.files
      if (fileCount) {
        totalFiles += parseInt(fileCount)
        versionsWithFileCount++
      }
    }
  } catch { }
  
  // Last resort: Try sourceFiles endpoint (experimental)
  try {
    sourceFiles = GET "/api/v1/projectVersions/{version.id}/sourceFiles?start=0&limit=1"
    if (sourceFiles.count) {
      totalFiles += sourceFiles.count
      versionsWithFileCount++
    }
  } catch { }
}

display = totalFiles.toLocaleString() + " files across " + versionsWithFileCount + " versions"</pre>
                                    
                                    <p><strong>âš ï¸ CRITICAL LIMITATIONS & UNCERTAINTIES:</strong></p>
                                    <ul>
                                        <li><strong>No native file count endpoint:</strong> FoD provides <code>staticScanSummaryDetails.fileCount</code> consistently. SSC has no equivalent standardized field.</li>
                                        <li><strong>Performance indicator dependency:</strong> Like LOC, file count requires optional administrator configuration via performance indicators. Most SSC instances lack this.</li>
                                        <li><strong>Metadata inconsistency:</strong> Artifact metadata field names for file counts vary by Fortify SCA version (19.x, 20.x, 22.x, 23.x, 24.x) and may not exist at all in older versions.</li>
                                        <li><strong>sourceFiles endpoint uncertainty:</strong> The <code>/projectVersions/{id}/sourceFiles</code> endpoint is not documented in OpenAPI spec. It may exist in some SSC versions but not others. Behavior is unknown without testing.</li>
                                        <li><strong>Count definition ambiguity:</strong> If sourceFiles endpoint exists, it's unclear whether it counts: (a) only analyzed source code files, (b) all files in FPR (including libraries), or (c) all files submitted in upload payload. FoD's fileCount is specifically "analyzed files"â€”SSC may include non-analyzed files.</li>
                                        <li><strong>Scanner-specific variations:</strong> Native Fortify SCA scans may include file counts; imported scans from other SAST tools likely do not.</li>
                                        <li><strong>No aggregation support:</strong> Must iterate every project version individuallyâ€”no bulk query for total file counts across portfolio.</li>
                                        <li><strong>Even lower value than LOC:</strong> File count is already low-value metric (3/10 rating). Combined with SSC's data availability issues, this metric has minimal ROI for implementation effort.</li>
                                    </ul>
                                    
                                    <p><strong>Implementation Best Practices:</strong></p>
                                    <ul>
                                        <li><strong>Pre-implementation testing:</strong> Sample 10-20 project versions. Try all three approaches: (1) performance indicators, (2) artifact metadata, (3) sourceFiles endpoint. Document which approaches work in your SSC version.</li>
                                        <li><strong>Strong recommendation: Skip this metric:</strong> Given low inherent value (3/10 rating) and high SSC implementation complexity, consider omitting this KPI entirely for SSC dashboards. Replace with higher-value metric like "Issues Detected" or "Scan Frequency."</li>
                                        <li><strong>If implementing, set expectations:</strong> Display coverage indicator alongside metric: "1.9M files (45% of versions have file count data)." Makes partial data availability transparent.</li>
                                        <li><strong>Background batch job required:</strong> Do not query performance indicators or artifacts real-time. Cache results from nightly batch aggregation.</li>
                                        <li><strong>Graceful degradation:</strong> If file count unavailable for a version, skip silently. Do not fail dashboard rendering.</li>
                                        <li><strong>sourceFiles endpoint caution:</strong> If using experimental sourceFiles endpoint, validate counts against known baselines. If counts seem inflated (10x larger than expected), endpoint may be including library filesâ€”use with caution or exclude.</li>
                                        <li><strong>Metadata field name mapping:</strong> Create configuration for known field names: <code>fileCount</code>, <code>filesScanned</code>, <code>sourceFiles</code>, <code>files</code>, <code>totalFiles</code>, <code>sourceFileCount</code>. Test which field names your Fortify SCA version uses.</li>
                                    </ul>
                                    
                                    <p><strong>Data Quality Considerations:</strong></p>
                                    <ul>
                                        <li><strong>Partial coverage expected:</strong> Even lower data availability than LOC. Expect file counts for only 30-60% of versions depending on configuration.</li>
                                        <li><strong>Generated files ambiguity:</strong> Unclear whether counts include code-generated files, resource files, configuration files, or only hand-written source code.</li>
                                        <li><strong>Multi-language complexity:</strong> Projects with mixed languages may have multiple artifacts. Summing file counts could double-count shared files (e.g., JavaScript + Java in same repo).</li>
                                        <li><strong>Test files inclusion:</strong> Like LOC, file counts may include or exclude test directories depending on scan configurationâ€”makes comparisons unreliable.</li>
                                    </ul>
                                    
                                    <p><strong>Recommendation for SSC Environments:</strong></p>
                                    
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <ul>
                                        <li>API: GET /api/v1/projectVersions/{id}/artifacts â†’ parse artifact details for <code>filesScanned</code></li>
                                        <li>Some scan types (e.g., imported FPR files) may lack this metadata</li>
                                        <li>Use <code>artifact.scanType</code> to filter for SCA scans only (engineType: "SCA")</li>
                                    </ul>
                                </li>
                                <li><strong>Platform Gaps:</strong> SSC metadata reliability varies by scan origin (native Fortify SCA vs. imported third-party results). FoD has more consistent metadata via staticScanSummaryDetails. Both platforms: file count is SAST-only; SCA/DAST scans do not produce file counts.</li>
                                <li><strong>Data Quality:</strong> File counts fluctuate wildly based on scan configuration (include/exclude patterns, supported file types). Large codebases (millions of LOC) may have thousands of small files or hundreds of large filesâ€”both are valid. FoD fileCount reflects actual analyzed files, not submitted payload size. Metric provides no security value but useful for scan troubleshooting.</li>
                                <li><strong>Performance:</strong> For large portfolios (1000+ releases), use same caching strategy as LOC metric. Batch queries with date range filters, paginate through results (max 50 per page), cache results for 24 hours.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. Open Source Components - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count all unique open source components (libraries, frameworks, dependencies) detected by SCA scans. Include both direct and transitive dependencies. Optionally deduplicate across applications (count each unique component once globally) or show total instances (same library in 100 apps = 100). Clarify counting methodology in dashboard.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li>GET /api/v3/applications/open-source-components?offset=0&limit=50 returns OpenSourceComponentListResponse with totalCount, items array containing component details (componentName, componentVersionName, componentHash, packageUrl, licenses, vulnerabilityCounts, releases, scanTool). Query parameters: openSourceScanType (enum: Sonatype|CycloneDx|Debricked), filters (field:value syntax, + for AND, | for OR), orderBy, orderByDirection (ASC|DESC), fields (comma-separated), offset (0-based), limit (max 50). Response includes totalCount field for global component countâ€”use this for dashboard KPI. Filter examples: filters=componentName:spring-core to find specific components. Pagination required for large portfolios. Component metadata: licenses array (name field), vulnerabilityCounts array (severityId, severity, count), releases array (applicationId, applicationName, releaseId, releaseName showing where component is used). Best practice: Use totalCount from API response rather than counting items array. For unique global components, deduplicate by componentHash or componentName:componentVersionName key.</li>
                                        <li>Alternative per-release endpoint: GET /api/v3/releases/{releaseId}/components (not documented in spec but may exist). FoD provides rich component metadata (vendor, license, vulnerabilities, EPSS score for CVEs). Limitations: Filtering not supported for certain fieldsâ€”check API response. Max limit=50 requires pagination for portfolios with 1000+ components.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ CRITICAL DEPENDENCY:</strong> SSC's open source component tracking depends entirely on SCA tool integration and parser plugin configuration. Unlike FoD's native unified component tracking, SSC relies on third-party SCA tools uploading component data in FPR artifacts.</p>
                                    
                                    <p><strong>Primary Approach - Dependencies API:</strong> <code>GET /api/v1/projectVersions/{id}/dependencies</code> returns component dependency data for a specific project version. This endpoint provides detailed component information when SCA data is available.</p>
                                    
                                    <p><strong>Component Count Calculation Workflow:</strong></p>
                                    <ol>
                                        <li><strong>Identify project versions with SCA data:</strong> <code>GET /api/v1/projectVersions?start=0&limit=200&includeInactive=false</code>. Not all versions will have dependency dataâ€”depends on whether SCA scans have been uploaded.</li>
                                        <li><strong>Query dependencies per project version:</strong> For each <code>projectVersion.id</code>, call <code>GET /api/v1/projectVersions/{id}/dependencies?start=0&limit=200</code>. This returns <code>ApiResultListProjectVersionDependency</code> with dependency details.</li>
                                        <li><strong>Aggregate components across all versions:</strong> Collect all dependencies from all project versions. Each dependency represents one component instance in one version.</li>
                                        <li><strong>Deduplication strategy (choose one):</strong>
                                            <ul>
                                                <li><strong>Total instances count:</strong> Sum all dependency records across all versions (same component in 100 versions = 100 instances). This gives largest number, matching "total component footprint."</li>
                                                <li><strong>Unique global components:</strong> Deduplicate by <code>dependency.name</code> + <code>dependency.version</code> combination. This shows "how many unique library versions" across portfolio.</li>
                                                <li><strong>Unique library names:</strong> Deduplicate by <code>dependency.name</code> only (ignoring version). Shows "how many distinct libraries" regardless of version differences.</li>
                                            </ul>
                                        </li>
                                        <li><strong>Handle missing SCA data:</strong> If <code>/dependencies</code> endpoint returns empty or 404 for a version, that version has no SCA data. Count only versions with available dependency information. Document this limitation in dashboard.</li>
                                    </ol>
                                    
                                    <p><strong>Response Schema Key Fields:</strong></p>
                                    <ul>
                                        <li><code>data</code> (array): List of ProjectVersionDependency objects.</li>
                                        <li><code>count</code> (int): Total dependencies for this specific project version.</li>
                                        <li><code>data[].id</code> (string): Dependency unique identifier.</li>
                                        <li><code>data[].name</code> (string): Component/library name (e.g., "spring-core", "log4j").</li>
                                        <li><code>data[].version</code> (string): Component version (e.g., "5.3.21", "2.17.1").</li>
                                        <li><code>data[].description</code> (string): Component description/purpose.</li>
                                        <li><code>data[].licenses</code> (array): License information for the component.</li>
                                        <li><code>data[].direct</code> (boolean): **CRITICAL** - True if direct dependency, false if transitive. Use this to filter transitive dependencies if desired.</li>
                                        <li><code>data[].depth</code> (int): Dependency tree depth (0 = direct, 1+ = transitive levels).</li>
                                        <li><code>data[].source</code> (string): SCA tool that detected this dependency (e.g., "Sonatype", "Fortify SCA").</li>
                                    </ul>
                                    
                                    <p><strong>Query Parameters:</strong></p>
                                    <ul>
                                        <li><code>start</code> (int, default 0): Pagination offset.</li>
                                        <li><code>limit</code> (int, default 200): Results per page. May need multiple pages for versions with 1000+ dependencies.</li>
                                        <li><code>q</code> (string): Lucene-style filter. Examples: <code>q=direct:true</code> (only direct dependencies), <code>q=name:log4j*</code> (specific library wildcard search), <code>q=licenses:Apache*</code> (filter by license).</li>
                                        <li><code>orderby</code> (string): Sort field. Example: <code>orderby=name</code> or <code>orderby=-version</code> (descending with minus prefix).</li>
                                    </ul>
                                    
                                    <p><strong>Alternative Approach - Artifacts + Parser Plugins:</strong></p>
                                    <p>If the Dependencies API is not populated, check for SCA data in uploaded artifacts:</p>
                                    <ol>
                                        <li><strong>Query artifacts:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts?start=0&limit=200</code> returns uploaded FPR files and scan results.</li>
                                        <li><strong>Filter SCA artifacts:</strong> Look for artifacts with <code>artifact.scanType</code> matching SCA tools: "Sonatype", "Debricked", "Fortify SCA" (if configured to scan dependencies).</li>
                                        <li><strong>Parse artifact metadata:</strong> Some SCA data may be embedded in artifact metadata fields. Check <code>artifact.metadata</code> or download FPR and parse <code>dependencySnapshot.xml</code> (requires FPR file processingâ€”complex).</li>
                                        <li><strong>âš ï¸ LIMITATION:</strong> This approach is unreliable and requires FPR parsing, which is beyond REST API scope. Only use as fallback if Dependencies API is completely unavailable.</li>
                                    </ol>
                                    
                                    <p><strong>SCA Tool Integration Detection:</strong></p>
                                    <p>Before attempting component count, verify SCA integration exists:</p>
                                    <ol>
                                        <li><strong>Check plugin configuration:</strong> <code>GET /api/v1/plugins</code> returns installed parser plugins. Look for "Sonatype", "Debricked", or "Fortify SCA" entries with <code>pluginState:STARTED</code>.</li>
                                        <li><strong>Sample project versions:</strong> Query <code>/dependencies</code> for a few known project versions. If all return empty or 404, SCA data may not be populated in your SSC instance.</li>
                                        <li><strong>Admin verification:</strong> If no SCA data found, consult SSC administrator to confirm: (1) SCA tool licenses are active, (2) Parser plugins are installed and enabled, (3) Projects have SCA scans uploaded.</li>
                                    </ol>
                                    
                                    <p><strong>Example Implementation (Pseudocode):</strong></p>
                                    <pre>totalComponents = 0
uniqueComponents = new Set()  // For deduplication
versionsWithSCA = 0

// Get all active project versions
projectVersions = GET "/api/v1/projectVersions?includeInactive=false&start=0&limit=200"

for (version in projectVersions.data) {
  try {
    dependencies = GET "/api/v1/projectVersions/{version.id}/dependencies?start=0&limit=200"
    
    if (dependencies.data.length > 0) {
      versionsWithSCA++
      totalComponents += dependencies.count
      
      // Deduplicate by name:version
      for (dep in dependencies.data) {
        uniqueKey = dep.name + ":" + dep.version
        uniqueComponents.add(uniqueKey)
      }
    }
  } catch (404 or empty) {
    // This version has no SCA data, skip
  }
}

// Choose metric to display:
// Option 1: Total instances across all versions
displayMetric = totalComponents

// Option 2: Unique components (deduplicated)
displayMetric = uniqueComponents.size()

// Also track: versionsWithSCA / totalVersions = SCA coverage %</pre>
                                    
                                    <p><strong>âš ï¸ CRITICAL LIMITATIONS & UNCERTAINTIES:</strong></p>
                                    <ul>
                                        <li><strong>SCA integration dependency:</strong> Unlike FoD's built-in component tracking, SSC requires active SCA tool integration (Sonatype, Debricked, or native Fortify SCA with dependency scanning enabled). If your SSC instance does not have these integrations configured, component data will be completely unavailable.</li>
                                        <li><strong>Inconsistent SCA coverage:</strong> Not all project versions may have SCA scans. Some teams may only run SAST, leaving dependency data blank. This creates misleading "component count" that only reflects SCA-scanned applications.</li>
                                        <li><strong>No global component endpoint:</strong> Unlike FoD's <code>/applications/open-source-components</code> global endpoint, SSC requires iterating through all project versions and aggregating dependencies client-side. This is slow and resource-intensive for large portfolios (500+ versions).</li>
                                        <li><strong>Parser plugin variations:</strong> Different SCA tools (Sonatype vs Debricked vs native Fortify SCA) detect different dependency sets due to methodology differences. Switching SCA tools mid-project causes count fluctuations that aren't true changes in code dependencies.</li>
                                        <li><strong>Transitive dependency explosion:</strong> SSC includes transitive dependencies by default. A project with 10 direct dependencies might show 200+ total dependencies. Use <code>q=direct:true</code> filter to count only direct dependencies if desired, but this may undercount actual exposure.</li>
                                        <li><strong>API performance concerns:</strong> Querying <code>/dependencies</code> for 500+ project versions individually can take minutes. This is not suitable for real-time dashboard renderingâ€”requires background job with caching.</li>
                                        <li><strong>Version-level granularity only:</strong> SSC has no concept of "application-level component count" like FoD. You can only aggregate version-level data, making comparison difficult if FoD uses application-level deduplication.</li>
                                        <li><strong>License data reliability:</strong> The <code>licenses</code> array in dependency objects depends on SCA tool's license detection accuracy. May be incomplete or missing for obscure libraries.</li>
                                        <li><strong>No vulnerability count in dependencies endpoint:</strong> Unlike FoD's <code>vulnerabilityCounts</code> array in component response, SSC's <code>/dependencies</code> endpoint does not include CVE counts. Must cross-reference with <code>/issues</code> endpoint to get vulnerability data for componentsâ€”adds complexity.</li>
                                    </ul>
                                    
                                    <p><strong>Implementation Best Practices:</strong></p>
                                    <ul>
                                        <li><strong>Pre-implementation check:</strong> BEFORE building this metric, verify your SSC instance has active SCA integration by sampling <code>/dependencies</code> API for known project versions. If no data exists, this metric cannot be implemented without infrastructure changes (SCA tool deployment).</li>
                                        <li><strong>Background aggregation:</strong> Do not query all project versions real-time. Implement nightly batch job that aggregates component counts and caches results. Serve dashboard from cache.</li>
                                        <li><strong>Choose deduplication strategy:</strong> Decide upfront: total instances (highest number, shows full exposure) vs unique components (lower number, shows distinct library count). Document choice clearly in dashboard tooltip.</li>
                                        <li><strong>Filter transitive dependencies (optional):</strong> Consider showing both metrics: "Direct Dependencies" (<code>q=direct:true</code>) and "Total Dependencies" (including transitives). Direct dependencies are more actionable for development teams.</li>
                                        <li><strong>SCA coverage tracking:</strong> Alongside component count, display "% of versions with SCA data" to show metric confidence. Example: "58,432 components across 245 versions (67% of portfolio has SCA scans)."</li>
                                        <li><strong>Pagination handling:</strong> For versions with 500+ dependencies, paginate through all results: <code>limit=200</code>, increment <code>start</code> until <code>data.length &lt; limit</code>.</li>
                                        <li><strong>Error handling:</strong> Gracefully handle 404 or empty responses from <code>/dependencies</code> (indicates no SCA data for that version). Do not fail entire dashboard rendering.</li>
                                        <li><strong>Alternative metric if SCA unavailable:</strong> If SCA integration is missing, replace this KPI with "Artifacts Uploaded" or "Scan Frequency" metric insteadâ€”something achievable with available data.</li>
                                        <li><strong>Cross-platform comparison caveat:</strong> If comparing SSC to FoD, ensure both use same deduplication logic (total instances vs unique). FoD's <code>totalCount</code> from <code>/applications/open-source-components</code> typically represents unique global components, not total instances.</li>
                                    </ul>
                                    
                                    <p><strong>Data Quality Considerations:</strong></p>
                                    <ul>
                                        <li><strong>Stale dependency data:</strong> If a project version hasn't uploaded new SCA scans in 6+ months, component count reflects outdated dependencies. Consider filtering to versions with recent artifact uploads (<code>q=lastArtifactDate:[NOW-90DAYS TO *]</code> if field available).</li>
                                        <li><strong>Test/development versions:</strong> Include/exclude inactive versions using <code>includeInactive=false</code> in projectVersions query to avoid counting abandoned test versions.</li>
                                        <li><strong>SCA tool migration issues:</strong> If your org switched from Sonatype to Debricked mid-year, historical dependency data may have gaps or duplicates. Document SCA tool standardization status.</li>
                                        <li><strong>Monorepo scenarios:</strong> If your org uses monorepos with multiple applications in one codebase, SSC may create one project version with all shared dependencies, inflating count. Requires custom logic to apportion components across logical applications.</li>
                                    </ul>
                                    
                                    <p><strong>Recommendation for SSC Environments:</strong></p>
                                    <p>Given the complexity and dependencies of this metric in SSC, consider these alternatives if SCA integration is immature:</p>
                                    <ul>
                                        <li><strong>Option 1:</strong> Display "SCA Scan Coverage %" instead of component count (% of versions with dependency data).</li>
                                        <li><strong>Option 2:</strong> Show "High-Risk Components" count (components with known CVEs) from <code>/issues</code> API instead of total countâ€”more actionable.</li>
                                        <li><strong>Option 3:</strong> Pull component data from separate SBOM repository or dependency management tool (Artifactory, Nexus) if availableâ€”SSC may not be authoritative source.</li>
                                        <li><strong>Option 4:</strong> If FoD is also in use, display FoD component count even for SSC dashboardâ€”use single source of truth for cross-platform consistency.</li>
                                    </ul>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD uses "Components"; SSC uses "Dependencies" (same concept). Both platforms support SCA integration but with different data models. FoD openSourceScanType enum values: Sonatype, CycloneDx, Debricked.</li>
                                <li><strong>Platform Gaps:</strong> FoD has unified component tracking across all releases via global endpoint /api/v3/applications/open-source-components. SSC requires SCA tool integration (may not be enabled for all projects). If SSC project doesn't have SCA scans, component count will be zero or unavailable. Component vulnerability data is richer in FoD (EPSS integration, vulnerabilityCounts array with severity breakdown); SSC relies on CVE database sync.</li>
                                <li><strong>Data Quality:</strong> Transitive dependencies can inflate counts dramatically (one direct dependency might pull in 50+ transitives). Different SCA tools (Sonatype vs. CycloneDx vs. Debricked) detect different components due to detection methodologyâ€”filter by openSourceScanType for consistency. Version variations (library v1.0 vs v1.1) count as separate componentsâ€”massive duplication across apps using slightly different versions. Component deduplication strategy: Use componentHash for exact match deduplication, or componentName (ignoring version) for version-agnostic counting.</li>
                                <li><strong>Recommendations:</strong> Pair with "Components with Critical CVEs" metric to add actionability. Consider showing unique components (global dedupe by componentHash) vs. total instances (sum of items across all releases). For SSC, verify SCA integration is active before displaying this metric; add fallback message if no SCA data available. Use FoD totalCount field directlyâ€”no need to paginate and count items unless filtering or grouping required.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Application Coverage - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design & Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count applications with at least one completed scan. Exclude apps with only inactive/retired version/releases. Failed/incomplete scans do not count as completed.</li>
                                <li><strong>FoD vs SSC:</strong> 
                                    <ul>
                                        <li><strong>FoD Implementation:</strong>
                                            <ol>
                                                <li><strong>Get total applications:</strong> GET /api/v3/applications?offset=0&limit=50 returns ApplicationListResponse with totalCount field (denominator).</li>
                                                <li><strong>Get applications with completed scans by type:</strong> Use GET /api/v3/scans?analysisStatusType=Completed&offset=0&limit=50 to retrieve ScanListResponse with releaseId values. Extract unique applicationIds from releases. Alternative: Query GET /api/v3/releases?offset=0&limit=50, then for each release query GET /api/v3/scans?filters=releaseId:{id}+analysisStatusType:Completed.</li>
                                                <li><strong>Filter by scan type:</strong> In ScanListResponse, use scanTypeId field to categorize: 1=SAST (Static), 2=DAST (Dynamic), 3=Mobile, or use assessmentTypeId for more granular types. For SCA/Open Source: check if scan has Open Source component data via GET /api/v3/scans/{scanId}/summary and look for openSourceComponentCount &gt; 0.</li>
                                                <li><strong>Calculate coverage:</strong> For each scan type, count unique applicationIds that have at least one completed scan, then (uniqueAppsWithScanType / totalApplications) * 100.</li>
                                                <li><strong>Deduplication:</strong> Group scans by releaseId, extract applicationId from release data, use Set/distinct to count unique applications per scan type.</li>
                                            </ol>
                                        </li>
                                        <li><strong>Response Structure:</strong> ApplicationListResponse contains items array with applicationId, applicationName, applicationDescription. ScanListResponse contains items array with scanId, releaseId, scanTypeId (1=Static, 2=Dynamic, 3=Mobile), assessmentTypeId, analysisStatusType (Completed|Cancelled|Queued|In_Progress|Waiting), completedDateTime. Use releaseId to join with ReleaseListResponse (from GET /api/v3/releases) which contains applicationId.</li>
                                        <li><strong>Query Parameters:</strong> analysisStatusType=Completed filters only successful scans. Use modifiedStartDate for time-based filtering (e.g., scans in last 30 days). Pagination required: offset (0-based), limit (max 50). Use totalCount field to validate full dataset retrieval.</li>
                                        <li><strong>Performance Optimization:</strong> For large tenants (1000+ applications), query scans globally first (GET /api/v3/scans with date filters), extract unique releaseIds, then query releases (GET /api/v3/releases) to get applicationIdsâ€”fewer API calls than per-release scan queries. Cache results for 24 hours. Consider date filtering: completedOnStartDate/completedOnEndDate to limit to scans in last 30-90 days for "active coverage" metric.</li>
                                        <li><strong>SSC Implementation:</strong>
                                            <p><strong>Key Endpoints:</strong></p>
                                            <ol>
                                                <li><strong>Get unique projects (applications):</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id,project.id,project.name</code> â†’ extract unique <code>project.id</code> values (denominator)</li>
                                                <li><strong>Get artifacts per version:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts?orderby=-uploadDate&limit=1</code> â†’ filter by <code>engineType</code></li>
                                                <li><strong>Categorize scan types:</strong> Map <code>artifact.engineType</code>: "Fortify SCA" = SAST, "Fortify WebInspect" = DAST, "Sonatype"/"Debricked" = SCA</li>
                                                <li><strong>Calculate coverage:</strong> (projects with engineType / total projects) Ã— 100 per scan type</li>
                                            </ol>
                                            <p><strong>Implementation Logic:</strong></p>
                                            <pre>projectsWithSAST = new Set()
projectsWithDAST = new Set()
projectsWithSCA = new Set()

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id,project.id"

for (version in versions.data) {
  artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts"
  
  for (artifact in artifacts.data) {
    if (artifact.engineType == "Fortify SCA") projectsWithSAST.add(version.project.id)
    if (artifact.engineType == "Fortify WebInspect") projectsWithDAST.add(version.project.id)
    if (artifact.engineType.match(/Sonatype|Debricked/)) projectsWithSCA.add(version.project.id)
  }
}

sastCoverage = (projectsWithSAST.size / uniqueProjects.size) * 100</pre>
                                            <p><strong>âš ï¸ Key Limitations:</strong> engineType strings vary by SSC version and plugin configuration. No standardized scan type enum like FoD. Must aggregate across all versions per project (slow for 500+ versions). Recommend caching results and refreshing daily.</p>
                                        </li>
                                    </ul>
                                </li>
                                <li><strong>Scan Type Mapping:</strong> FoD has clear assessmentType enum. SSC requires parsing artifact engineType strings and may have custom scan types.</li>
                                <li><strong>Recency Definition:</strong> Consider future option to define "scanned" as having at least 1 scan in trailing X days. Configurable threshold recommended.</li>
                                <li><strong>Data Quality:</strong> Exclude failed/cancelled scans. Only count scans that produced findings (status='Completed', not 'Failed').</li>
                                <li><strong>Multi-Scan Apps:</strong> App can appear in multiple categories (SAST + DAST). Percentages don't sum to 100%â€”this is expected.</li>
                                                                <li><strong>Denominator ambiguity:</strong> As long as an App has at least one version/release scanned, it counts as scanned. If an app has 10 versions but only 5 are scanned, that counts as scanned.</li>
                                <li><strong>Implementation Gap:</strong> SSC may have partial data if scans imported from third-party tools. FoD only tracks its native scan types.</li>
                                <li><strong>Performance Note:</strong> For large portfolios (1000+ apps), cache scan counts and refresh hourly rather than real-time queries.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Version Coverage - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of application versions/releases scanned by each tool type.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Shows pipeline integration depthâ€”scanning apps vs. scanning every release. Lower than application coverage indicates manual/periodic scanning vs. automated.</li>
                                <li><strong>Engineers:</strong> Reveals CI/CD automation gaps. Identifies which apps scan manually vs. in pipelines.</li>
                                <li><strong>Insights gained:</strong> Automation maturity, pipeline integration success, scanning frequency patterns.</li>
                                <li><strong>Actions driven:</strong> Pipeline automation priorities, shift-left initiatives, breaking build policies, release gate enforcement.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Extremely valuableâ€”the gap between application coverage (82% SAST) and version coverage (68% SAST) reveals that ~14% of apps scan periodically rather than per-release. This identifies shift-left gaps. Version-level scanning is the gold standard for modern AppSec. Directly measures DevSecOps maturity.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Complexity:</strong> Two similar metrics (Application vs. Version Coverage) might confuse usersâ€”requires clear labeling.</li>
                                <li><strong>Recency bias:</strong> Old versions with scans inflate the percentageâ€”needs "scanned in last 30 days" filter.</li>
                                <li><strong>Potential frustration:</strong> Lower than application coverage might look like regression without explanation that it's actually a harder/better standard.</li>
                            </ul>
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count application versions/releases with at least one completed scan. Exclude retired/archived versions. Denominator = total active versions across all applications.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Get total active releases:</strong> GET /api/v3/releases?offset=0&limit=50 returns ReleaseListResponse with totalCount field (denominator). Filter by active releases onlyâ€”exclude suspended or retired releases by checking suspended (boolean) field. Count only releases where suspended=false.</li>
                                        <li><strong>Response Structure:</strong> ReleaseListResponse contains items array with releaseId (int32), releaseName (string), releaseDescription (string), applicationId (int32), applicationName (string), suspended (boolean), sdlcStatusType (string enum: Development, QA, Production), createdDate (date-time), currentAnalysisStatusType (string: Completed, In_Progress, Cancelled, Queued, Waiting), rating (int32 star rating 1-5).</li>
                                        <li><strong>Implementation Workflow:</strong>
                                            <ol>
                                                <li>Query all active releases: GET /api/v3/releases?filters=suspended:false&offset=0&limit=50. Use totalCount from response as denominator.</li>
                                                <li>For each release, query scans: GET /api/v3/scans?filters=releaseId:{id}+analysisStatusType:Completed&offset=0&limit=50. Check if any scans exist (items array length &gt; 0).</li>
                                                <li>Filter by scan type using scanTypeId: 1=SAST, 2=DAST, 3=Mobile. For SCA/Open Source, check scan summary: GET /api/v3/scans/{scanId}/summary and verify staticScanSummaryDetails.doSonatypeScan=true or dynamicScanSummaryDetails.openSourceScanType is not null.</li>
                                                <li>Calculate coverage per scan type: (releasesWith CompleteScans OfType / totalActiveReleases) * 100.</li>
                                            </ol>
                                        </li>
                                        <li><strong>Recency Filtering:</strong> Add completedOnStartDate and completedOnEndDate query parameters to filter scans completed in last 30/60/90 days for "active coverage" calculation. Example: GET /api/v3/scans?completedOnStartDate=2024-01-01T00:00:00Z&completedOnEndDate=2024-03-31T23:59:59Z&analysisStatusType=Completed. This prevents old, stale scans from inflating coverage percentages.</li>
                                        <li><strong>Query Parameters:</strong> filters (suspended:false for active releases, analysisStatusType:Completed for successful scans), orderBy (releaseName, createdDate, rating), orderByDirection (ASC|DESC), offset (0-based), limit (max 50), completedOnStartDate/completedOnEndDate (ISO 8601 date-time for scan recency).</li>
                                        <li><strong>Deduplication:</strong> Count each releaseId only once per scan typeâ€”if release has multiple SAST scans, it still counts as "1 release with SAST coverage." Use Set/distinct on releaseId when aggregating.</li>
                                        <li><strong>Performance Optimization:</strong> For large portfolios (1000+ releases), query scans globally first: GET /api/v3/scans?analysisStatusType=Completed&completedOnStartDate={90daysAgo}&offset=0&limit=50, extract unique releaseIds per scanTypeId, then count. This is faster than per-release scan queries. Cache results for 1-6 hours depending on dashboard refresh frequency. For real-time dashboards, consider webhook-based cache invalidation when new scans complete.</li>
                                        <li><strong>Data Quality Notes:</strong> suspended field is criticalâ€”suspended releases are inactive but not deleted, should be excluded from denominator. Some releases may have no scans (new releases, abandoned projects)â€”these count toward denominator but not numerator. currentAnalysisStatusType on release shows latest scan status but doesn't indicate scan typeâ€”must query scans endpoint for scanTypeId details.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Get active versions (denominator):</strong> <code>GET /api/v1/projectVersions?includeInactive=false&committed=true</code> â†’ use <code>count</code> field</li>
                                        <li><strong>Check artifacts per version:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts?start=0&limit=200</code></li>
                                        <li><strong>Filter by recency (optional):</strong> Add <code>q=uploadDate:[NOW-30DAYS TO *]</code> for scans in last 30 days</li>
                                        <li><strong>Categorize by engineType:</strong> "Fortify SCA" = SAST, "Fortify WebInspect" = DAST, "Sonatype"/"Debricked" = SCA</li>
                                    </ol>
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>versionsWithSAST = 0
versionsWithDAST = 0
versionsWithSCA = 0

versions = GET "/api/v1/projectVersions?includeInactive=false&committed=true"
totalVersions = versions.count

for (version in versions.data) {
  artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts?q=uploadDate:[NOW-30DAYS TO *]"
  
  engineTypes = artifacts.data.map(a => a.engineType)
  if (engineTypes.includes("Fortify SCA")) versionsWithSAST++
  if (engineTypes.includes("Fortify WebInspect")) versionsWithDAST++
  if (engineTypes.some(e => e.match(/Sonatype|Debricked/))) versionsWithSCA++
}

sastCoverage = (versionsWithSAST / totalVersions) * 100</pre>
                                    <p><strong>âš ï¸ Key Limitations:</strong> <code>committed=true</code> filter excludes versions in setup phase. Artifact <code>engineType</code> field names vary by parser plugin configuration. No <code>status</code> field to filter completed scansâ€”must check <code>artifact.status</code> or artifact existence. Performance: requires one API call per version (slow for 1000+ versions)â€”implement background caching.</p>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD "Releases" = SSC "Project Versions". Both represent deployable units but different mental models (FoD = environment-centric, SSC = branch-centric).</li>
                                <li><strong>Platform Gaps:</strong> FoD release lifecycle management is more explicit (active/retired states). SSC versions can become stale without clear retirementâ€”recommend filtering by <code>committed=true</code> to exclude abandoned versions. FoD has better scan recency tracking via scanStatusType; SSC requires manual date filtering on artifact uploadDate.</li>
                                <li><strong>Data Quality:</strong> Denominator calculation criticalâ€”exclude demo/test/archived versions or percentage will be artificially low. Recency threshold (30 days) should be configurable; some teams release monthly, others quarterly. Apps with many active versions (trunk-based development with feature branches) will have lower coverage than single-main-branch apps.</li>
                                <li><strong>Recommendation:</strong> Pair this metric with Application Coverage to tell full story. Gap between them (App Coverage 82%, Version Coverage 68%) = automation maturity indicator. Add tooltip explaining why Version Coverage is lower and harderâ€”signals shift-left success when high.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Scan Activity (12-month trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count all completed scans per month for last 12 months. Include successful scans only (exclude failed/cancelled scans unless explicitly tracking scan failures). Group by scan type (SAST, DAST, SCA, Other). This is an operational health metric, not a security outcome metric.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li>GET /api/v3/scans?startedOnStartDate=YYYY-MM-DDT00:00:00Z&startedOnEndDate=YYYY-MM-DDT23:59:59Z&offset=0&limit=50 returns ScanListResponse with items array (scanId, scanTypeId, scanType, analysisStatusTypeId, analysisStatusType, startedDateTime, completedDateTime, releaseId, releaseName, applicationId, applicationName, assessmentTypeId, totalIssues, issueCountCritical/High/Medium/Low, starRating). Query parameters: startedOnStartDate, startedOnEndDate, completedOnStartDate, completedOnEndDate (date-time format ISO 8601), modifiedStartDate, orderBy, orderByDirection (ASC|DESC), fields (comma-separated), offset (0-based), limit (max 50). Filter for completed scans using analysisStatusType or analysisStatusTypeId. Response includes totalCount field. scanTypeId enum: 1=Static, 2=Dynamic, 3=Mobile. Best practice: Query by completedOnStartDate/completedOnEndDate for monthly buckets (e.g., 2024-01-01T00:00:00Z to 2024-01-31T23:59:59Z for January). Group results client-side by month (parse completedDateTime) and scanTypeId. Use totalCount for validation but must paginate to retrieve all scans (max 50 per request). FoD provides analysisStatusType enum: Completed, Cancelled, Queued, In_Progress, Waitingâ€”filter for Completed only. Limitations: Large tenants (10,000+ scans/year) require pagination and date-range batching to avoid timeouts.</li>
                                        <li>Alternative: Use modifiedStartDate parameter to get all scans modified in last 12 months, then filter client-side by completedDateTime. FoD has good timestamp precision (ISO 8601 date-time format). scanType field is human-readable enum (Static|Dynamic|Mobile); use scanTypeId for reliable filtering.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query artifacts with date filtering:</strong> <code>GET /api/v1/projectVersions?fields=id&includeInactive=false</code> then for each: <code>GET /api/v1/projectVersions/{id}/artifacts?start=0&limit=200</code></li>
                                        <li><strong>Filter by date range (12 months):</strong> Add <code>q=uploadDate:[2024-01-01T00:00:00.000Z TO *]</code> parameter</li>
                                        <li><strong>Filter by scan status:</strong> Check <code>artifact.status</code> fieldâ€”use only <code>PROCESS_COMPLETE</code> (exclude <code>SCHED_PROCESSING</code>, <code>PROCESSING</code>, <code>ERROR_PROCESSING</code>)</li>
                                        <li><strong>Group by month and tool type:</strong> Extract month from <code>artifact.uploadDate</code>, group by <code>artifact.engineType</code> ("Fortify SCA" = SAST, "Fortify WebInspect" = DAST, "Sonatype"/"Debricked" = SCA)</li>
                                    </ol>
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>// Step 1: Initialize monthly buckets for 12 months
monthlyData = {} // e.g., {"2024-01": {SAST: 0, DAST: 0, SCA: 0}, ...}

// Step 2: Query all active versions
versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

// Step 3: Iterate versions and collect artifacts from last 12 months
startDate = 12MonthsAgo().toISOString()
for (version in versions.data) {
  artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts?q=uploadDate:[{startDate} TO *]"
  
  for (artifact in artifacts.data) {
    // Step 4: Filter by status
    if (artifact.status != "PROCESS_COMPLETE") continue
    
    // Step 5: Extract month (e.g., "2024-01")
    month = artifact.uploadDate.substring(0, 7)
    
    // Step 6: Categorize by engineType
    if (artifact.engineType.includes("Fortify SCA")) {
      monthlyData[month].SAST++
    } else if (artifact.engineType.includes("WebInspect")) {
      monthlyData[month].DAST++
    } else if (artifact.engineType.match(/Sonatype|Debricked/)) {
      monthlyData[month].SCA++
    } else {
      monthlyData[month].Other++
    }
  }
}

// Step 7: Render grouped bar chart from monthlyData</pre>
                                    <p><strong>âš ï¸ Key Limitations:</strong> No global artifacts endpointâ€”must iterate all versions to collect artifacts (performance-intensive). SSC's <code>uploadDate</code> may differ from actual scan date if results uploaded later. <code>engineType</code> values vary by parser configuration (no standardized enum). Artifacts include imported third-party results that may not represent actual scans. Consider caching monthly aggregates to avoid re-querying all versions daily. For 1000+ versions, expect 5-10 minute query timeâ€”implement as background batch job rather than real-time dashboard query.</p>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD "Scans" = SSC "Artifacts". Both represent security test results but SSC artifact model includes imported results from third-party tools. FoD scanType vs. SSC engineType mapping: scanTypeId=1 (Static) = engineType "Fortify SCA", scanTypeId=2 (Dynamic) = engineType "Fortify WebInspect", scanTypeId=3 (Mobile) = engineType "Mobile".</li>
                                <li><strong>Platform Gaps:</strong> FoD has clearer scan lifecycle tracking via analysisStatusType enum. SSC artifacts can be uploaded but fail processingâ€”ensure filtering for PROCESS_COMPLETE status. FoD assessmentTypeId is standardized; SSC engineType is string-based and varies (manual mapping required). Third-party scan results in SSC may not fit standard scan typesâ€”bucket into "Other" category. FoD completedDateTime is reliable; SSC uploadDate may not reflect actual scan date (use scanDate attribute if available).</li>
                                <li><strong>Data Quality:</strong> Re-scans inflate activity countsâ€”consider deduplicating by releaseId+completedDate (count one scan per release per day) if tracking unique scans. Failed scans excluded by default (analysisStatusType != Completed) but can be tracked separately as "Scan Failure Rate" metric. Seasonal variations normal (holidays, end-of-quarter pushes)â€”add YoY comparison to normalize. FoD remediationScan field (isRemediationScan boolean) can differentiate baseline vs. remediation scans for more granular analysis.</li>
                                <li><strong>Grouped Bar Chart Implementation:</strong> Display monthly data as grouped bars (not stacked) with y-axis showing scan count (0-500 scale) and x-axis showing months. Each scan type is a distinct bar color within monthly group. Include tooltips showing exact counts. For FoD: Use completedDateTime to bucket scans into months, group by scanTypeId, sum counts per type per month.</li>
                                <li><strong>Performance:</strong> For large portfolios, use date-range queries to limit result sets (12 separate monthly queries instead of one 12-month query). Cache monthly aggregates and refresh daily. Pagination requiredâ€”expect multiple requests per month if scan volume exceeds 50/month.</li>
                                <li><strong>Recommendation:</strong> Pair with "Scan Success Rate" metric (analysisStatusType=Completed / all scans) to add quality dimension. High activity with low success rate = tool configuration problems. Stable activity over time = mature automation. For FoD, use scanMethodTypeId field (IDE|CICD|Other enum) to segment automation vs. manual scans.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Entitlements (FoD only) - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> FoD-specific metric tracking subscription unit consumption. Include all active entitlements with purchased units, used units, contract start/end dates, and utilization percentage. Alert on high utilization (&gt;90%) or upcoming renewals (&lt;30 days). <strong>NOT APPLICABLE to SSC</strong>â€”SSC uses different licensing model (sensor-based or named-user licenses).</li>
                                <li><strong>Fortify on Demand Only:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/tenant-entitlements returns GetTenantEntitlementResponse containing entitlementTypeId (int32), entitlementType (string), subscriptionTypeId (int32), subscriptionType (string), and tenantEntitlements array of TenantEntitlement objects.</li>
                                        <li><strong>Response Structure:</strong> Each TenantEntitlement object includes:
                                            <ul>
                                                <li>entitlementId (int32) â€” Unique identifier</li>
                                                <li>entitlementDescription (string) â€” Human-readable name (e.g., "Static Assessment Subscription")</li>
                                                <li>unitsPurchased (int32) â€” Total units in contract</li>
                                                <li>unitsConsumed (int32) â€” Units used to date</li>
                                                <li>startDate (date-time, ISO 8601) â€” Contract start date</li>
                                                <li>endDate (date-time, ISO 8601) â€” Contract expiration date (may be null for perpetual)</li>
                                                <li>extendedProperties object â€” Contains assessmentTypeId, frequencyTypeId, frequencyType, subscriptionLength</li>
                                            </ul>
                                        </li>
                                        <li><strong>Utilization Calculation:</strong> (unitsConsumed / unitsPurchased) * 100. Example: 950 consumed / 1000 purchased = 95% utilization.</li>
                                        <li><strong>Entitlement Types:</strong> Static (SAST scans), Dynamic (DAST scans), Mobile (Mobile app assessments), OpenSource (SCA component analysis). assessmentTypeId maps to scan types (1=Static, 2=Dynamic, 3=Mobile, 4=OpenSource).</li>
                                        <li><strong>Frequency Types:</strong> SingleScan (one-time unit consumption, unit consumed per scan), Subscription (recurring allotment, monthly/annual refresh). frequencyType enum affects utilization calculation and renewal behavior.</li>
                                        <li><strong>Alert Thresholds:</strong>
                                            <ul>
                                                <li>High Risk: utilization &gt; 90% (immediate action required to avoid scan throttling)</li>
                                                <li>Renewal Warning: endDate within 30 days (urgent procurement needed)</li>
                                                <li>Planning Window: endDate within 90 days (initiate renewal workflow)</li>
                                            </ul>
                                        </li>
                                        <li><strong>Data Quality Notes:</strong> unitsConsumed updates in near real-time (within minutes of scan completion). Some entitlements are perpetual (endDate may be null or far-future date like 2099-12-31T23:59:59Z)â€”handle gracefully in UI by displaying "Perpetual" instead of date. tenantEntitlements array can contain multiple entitlements of same type (e.g., two SAST subscriptions with different renewal dates).</li>
                                        <li><strong>Best Practices:</strong> Query weekly, cache results for 24 hours, trigger automated alerts when thresholds crossed (email/Slack). For large organizations, break down utilization by business unit or application criticality to prioritize critical apps when units are limited. Add trend chart showing monthly consumption rate (units consumed per month over last 12 months) to predict depletion date.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center Alternative:</strong>
                                    <ul>
                                        <li><strong>CRITICAL:</strong> SSC does not have FoD-style entitlements API. SSC licensing is sensor-based (ScanCentral SAST/DAST sensors) or named-user licenses.</li>
                                        <li>Alternative metric for SSC: "Active Scan Sensors" showing deployed ScanCentral SAST/DAST sensors, scan pool utilization, sensor health status</li>
                                        <li>SSC sensor data: Query ScanCentral Controller API (separate from SSC) for sensor status, job queue depth, worker pool utilization</li>
                                        <li>Named-user licenses: SSC tracks user accounts but not license consumptionâ€”manual tracking required against license agreement</li>
                                    </ul>
                                </li>
                                <li><strong>Platform-Specific Implementation:</strong> For FoD: Display entitlement table with utilization alerts. For SSC: Replace with "Scan Infrastructure Health" showing ScanCentral sensor status, job queue metrics, or remove this card entirely. Hybrid environments: Show FoD entitlements for cloud scans, SSC sensor pool for on-premise scans.</li>
                                <li><strong>Data Quality:</strong> FoD entitlement consumption updates in near real-time. High utilization (&gt;90%) requires immediate action to avoid scan throttling. Contract end dates should trigger renewal workflows 60-90 days in advance. Some entitlements are perpetual (no end date)â€”handle gracefully in UI.</li>
                                <li><strong>Recommendation:</strong> For FoD: Essential metric for procurement teamsâ€”integrate with IT asset management systems. For SSC: Replace with operational metrics (sensor health, scan queue depth, average scan duration) that provide similar infrastructure visibility without licensing data.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">11. Apps by Tech Stack (Treemap) - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Aggregate applications by primary programming language/framework. Count each application once under its dominant technology. Group rare languages (&lt;5% of portfolio) into "Other" category. Consider weighting by LOC or business criticality instead of simple app counts for more meaningful distribution.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Data Source:</strong> GET /api/v3/scans/{scanId}/summary returns StaticScanSummaryDetails object containing technologyStack (string) and languageLevel (string) fields. These fields represent the technology stack and language version detected during static analysis.</li>
                                        <li><strong>technologyStack Field:</strong> Contains the primary technology/language (e.g., "JAVA", "DOTNET", "PYTHON", "JAVASCRIPT", "GO", "ABAP", "ANDROID_JAVA", "APEX", "ASP.NET", "C_CPP", "COBOL", "DART", "IOS_SWIFT", "KOTLIN", "PHP", "RUBY", "SCALA", "TYPESCRIPT", "VB.NET"). Values can be obtained by calling GET /api/v3/lookup-items?type=TechnologyTypes which returns a lookup list with text (display name), value (ID), and group fields.</li>
                                        <li><strong>languageLevel Field:</strong> Specific version/runtime (e.g., "1.8" for Java 8, "17" for Java 17, "4.8" for .NET Framework 4.8, "6.0" for .NET 6, "3.9" for Python 3.9). Values obtained via GET /api/v3/lookup-items?type=LanguageLevels (the group field in this response indicates the TechnologyTypeId for grouping).</li>
                                        <li><strong>Implementation Workflow:</strong>
                                            <ol>
                                                <li>Query all releases: GET /api/v3/releases?offset=0&limit=50</li>
                                                <li>For each release, get latest completed SAST scan: GET /api/v3/scans?filters=releaseId:{id}+scanTypeId:1+analysisStatusType:Completed&orderBy=completedDateTime&orderByDirection=DESC&limit=1</li>
                                                <li>Retrieve scan summary: GET /api/v3/scans/{scanId}/summary to extract staticScanSummaryDetails.technologyStack</li>
                                                <li>Group applications by technologyStack value, count occurrences</li>
                                                <li>Calculate percentage: (count / totalApps) * 100</li>
                                            </ol>
                                        </li>
                                        <li><strong>Multi-Language Applications:</strong> FoD scan results may detect multiple languages in mixed-technology projects (e.g., Java backend + JavaScript frontend). The technologyStack field represents the primary scan target configured during scan setup. For comprehensive language tracking, parse scan artifacts or use custom attributes to supplement automated detection.</li>
                                        <li><strong>Data Quality Notes:</strong> technologyStack is only available for completed SAST scans (scanTypeId=1). DAST-only applications (scanTypeId=2) will not have language data. Applications without recent scans may have stale or missing technology stack information. Some applications may show generic values like "MIXED" or "OTHER" for complex multi-language codebases.</li>
                                        <li><strong>Categorization Strategy:</strong> Group related technologies for cleaner visualization: "JAVA" + "ANDROID_JAVA" + "KOTLIN" = "JVM-based", "DOTNET" + "ASP.NET" + "VB.NET" = ".NET Family", "JAVASCRIPT" + "TYPESCRIPT" = "JavaScript/TypeScript". Combine languages representing <5% of portfolio into "Other" category to avoid visual clutter.</li>
                                        <li><strong>Best Practices:</strong> Cache scan summaries (24-hour TTL), as language detection doesn't change frequently. For large portfolios (1000+ applications), batch scan queries with date filters (e.g., only scans from last 90 days) to reduce API calls. Supplement automated detection with custom application attribute "Primary Language" for business-defined categorization.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ HIGH UNCERTAINTY:</strong> SSC has no native technology stack tracking like FoD. Requires custom attributes or artifact metadata parsing.</p>
                                    
                                    <p><strong>Key Endpoints (Custom Attribute Approach):</strong></p>
                                    <ol>
                                        <li><strong>Query project versions with attributes:</strong> <code>GET /api/v1/projectVersions?fields=id,name,project&includeInactive=false</code></li>
                                        <li><strong>Get custom attributes:</strong> <code>GET /api/v1/projectVersions/{id}/attributes</code></li>
                                        <li><strong>Look for language attributes:</strong> Common names vary: <code>Development Language</code>, <code>Primary Technology</code>, <code>Tech Stack</code>, <code>Language</code></li>
                                        <li><strong>Alternative - parse artifact metadata:</strong> <code>GET /api/v1/projectVersions/{id}/artifacts</code> and check <code>artifact.metadata</code> or <code>artifact.engineType</code> for language hints</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>// Step 1: Get attribute definitions to find language attribute
attributeDefs = GET "/api/v1/attributeDefinitions?limit=200"
languageAttrId = attributeDefs.data.find(a => 
  a.name.match(/language|technology|tech.*stack/i)
)?.id

if (!languageAttrId) {
  // No custom attribute found - fall back to artifact parsing
  console.warn("No language attribute configured in SSC")
}

// Step 2: Query all versions and their attributes
versions = GET "/api/v1/projectVersions?includeInactive=false"
techStackCounts = {}

for (version in versions.data) {
  attributes = GET "/api/v1/projectVersions/{version.id}/attributes"
  
  // Look for language attribute value
  langAttr = attributes.data.find(a => a.attributeDefinitionId == languageAttrId)
  
  if (langAttr && langAttr.value) {
    techStackCounts[langAttr.value] = (techStackCounts[langAttr.value] || 0) + 1
  }
}

// Step 3: Calculate percentages and render treemap
totalVersions = versions.count
for (tech in techStackCounts) {
  percentage = (techStackCounts[tech] / totalVersions) * 100
  // Render treemap box proportional to percentage
}</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>Custom attributes are completely deployment-specific</strong>â€”no standardized field names or values exist in SSC. Common attribute names include "Development Language", "Primary Technology", "Tech Stack", but these must be manually created and populated per project version. If no custom attributes exist, technology detection is extremely difficult. Artifact metadata parsing is unreliable as <code>engineType</code> indicates scan tool (e.g., "Fortify SCA") not language. <strong>Recommendation:</strong> For SSC environments without custom attributes, consider marking this KPI as "Not Available" or populate custom attributes as prerequisite for dashboard implementation. Alternatively, maintain language data in external CMDB and join with SSC project names.</p>
                                </li>
                                <li><strong>Technology Detection Methods:</strong> Both platforms: Language detection from SAST scans is most reliable but may show multiple languages per app (monorepo). Custom attributes require manual maintenance but allow business-defined categorization (e.g., "Java/Spring Boot" vs just "Java"). File extension analysis (.java, .cs, .py) is fallback but unreliable for mixed-language projects.</li>
                                <li><strong>Platform Gaps:</strong> Neither FoD nor SSC has authoritative built-in language trackingâ€”both rely on custom attributes or scan inference. FoD technologyStack in scan results is more structured than SSC's variable attribute names. Multi-language applications (e.g., Java backend + React frontend) require categorization rules (primary vs. all languages).</li>
                                <li><strong>Data Quality:</strong> Language categorization is subjectiveâ€”is "JavaScript/TypeScript" one category or two? Frameworks (React, Angular, Vue) vs. languages (JavaScript) distinction matters for tool selection. Legacy apps may have incorrect/outdated language tags. Apps without recent scans may lack language data.</li>
                                <li><strong>Treemap Implementation:</strong> Size boxes by application count (default) or LOC for better representation of portfolio complexity. Color-code by language family (e.g., all JVM languages in blue shades). Include tooltip showing exact counts and percentage. Consider adding trend indicator (growing/shrinking) if historical data available.</li>
                                <li><strong>Recommendation:</strong> Standardize on custom application attribute "Primary Language" in both platforms for consistency. Supplement with "All Languages" attribute for multi-language apps. Pair this metric with "Vulnerabilities by Language" to prioritize tool investments (e.g., 38% Java apps but 60% of critical vulns = Java tooling is critical).</li>
                            </ul>
                        </div>
                    </details>
                    </details>
                </div>
            </div>

            <!-- FoD API Enhancement Recommendations (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ’¡ Recommended FoD API Enhancements for Dashboard Implementation</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>Analysis Context:</strong> Based on implementation requirements for the 11 Program Dashboard KPIs, the following API enhancements would dramatically simplify implementation and improve performance. Current implementation requires 500-1000+ API calls with complex client-side aggregation. Proposed enhancements would reduce this to ~10 calls with 90-95% faster load times.</p>
                            
                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--sev-critical); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--sev-critical);">ðŸ”´ Critical Missing Endpoints (High ROI)</h4>
                                
                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. GET /api/v3/portfolio-metrics/summary â­â­â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> Currently requires 500-1000+ API calls to aggregate application count, version count, LOC, files scanned, and component count across entire portfolio. Each metric (KPIs #1, #2, #4, #5, #6) requires separate iteration through all applications/releases/scans.</p>
                                        
                                        <div style="margin: 16px 0; padding: 12px; background: #1a3a52; border-left: 4px solid #4a9eff; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: #4a9eff;">ðŸ“Š Relationship to Existing GET /api/v3/tenant-summary:</p>
                                            <p style="margin: 0 0 8px 0; font-size: 13px;"><strong>Current Endpoint Coverage:</strong> The existing <code>tenant-summary</code> endpoint provides vulnerability counts, star ratings, and basic application/scan counts but <strong>lacks critical metrics</strong> (LOC, files, OSS components) and <strong>advanced filtering</strong> needed for dashboard analytics.</p>
                                            <p style="margin: 0 0 8px 0; font-size: 13px;"><strong>Key Gaps:</strong></p>
                                            <ul style="margin: 4px 0 8px 0; font-size: 13px; padding-left: 20px;">
                                                <li>âŒ Missing: <code>totalLinesOfCode</code>, <code>totalFilesScanned</code>, <code>totalOpenSourceComponents</code></li>
                                                <li>âŒ Limited filtering: Only single <code>sdlcStatus</code> value (no multi-select, no app-level filters)</li>
                                                <li>âŒ Returns paginated list instead of single aggregate object</li>
                                                <li>âŒ No support for custom attribute filtering or complex filter logic (AND/OR)</li>
                                            </ul>
                                            <p style="margin: 0; font-size: 13px;"><strong>Recommendation:</strong> Create new <code>portfolio-metrics/summary</code> endpoint optimized for dashboard analytics rather than enhancing <code>tenant-summary</code> (which serves different administrative use cases and risks breaking existing consumers).</p>
                                        </div>
                                        
                                        <p><strong>Proposed Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "applications": {
    "total": 123,
    "withScans": 101,
    "newThisYear": 15
  },
  "versions": {
    "total": 367,
    "active": 342,
    "retired": 25
  },
  "totalLinesOfCode": 74238419,
  "totalFilesScanned": 1984234,
  "totalOpenSourceComponents": 58432,
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Reduces ~500 API calls to 1 call (99.8% reduction)</li>
                                            <li>Instant dashboard load (&lt;1s vs. 30-60s)</li>
                                            <li>Consistent calculations across all users (server-side aggregation)</li>
                                            <li>Server-side caching optimizes performance for all tenants</li>
                                            <li>Powers 5 Program Dashboard KPIs with single endpoint</li>
                                        </ul>
                                        
                                        <p><strong>Query Parameters (Filtering Support):</strong></p>
                                        <ul>
                                            <li><code>includeInactive</code> (boolean, default false) - Include retired/archived applications</li>
                                            <li><code>dateRange</code> (string) - Filter newThisYear to custom date range</li>
                                        </ul>
                                        
                                        <p><strong>Application-Level Attribute Filters:</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string, FoD filter syntax) - Apply application filters using existing FoD pattern:
                                                <ul>
                                                    <li><code>filters=businessCriticalityType:High</code> - High criticality apps only</li>
                                                    <li><code>filters=applicationTypeId:1</code> - Filter by application type</li>
                                                    <li><code>filters=businessCriticalityType:High+applicationTypeId:1</code> - AND condition (+ separator)</li>
                                                    <li><code>filters=businessCriticalityType:High|Medium</code> - OR condition (| separator)</li>
                                                    <li><code>filters=attributes.{attributeName}:{value}</code> - Custom application attributes</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>Release-Level Attribute Filters:</strong></p>
                                        <ul>
                                            <li><code>releaseFilters</code> (string, separate from app filters) - Filter by release/version attributes:
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code> - Production releases only</li>
                                                    <li><code>releaseFilters=sdlcStatusType:Production|QA</code> - Production OR QA</li>
                                                    <li><code>releaseFilters=suspended:false</code> - Active releases only</li>
                                                    <li><code>releaseFilters=attributes.{attributeName}:{value}</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>âš ï¸ CRITICAL REQUIREMENT:</strong> Support for custom attribute filtering is essential. Many customers define custom attributes like "Business Unit", "Compliance Scope", "Data Classification", "Geography", etc. The endpoint MUST support filtering on any application or release attribute, not just predefined fields.</p>
                                        
                                        <p><strong>Default Behavior (No Filters Applied):</strong></p>
                                        <ul>
                                            <li><strong>FoD:</strong> Exclude retired releases (<code>sdlcStatusType:Retired</code> automatically excluded). Include Development, QA, and Production releases by default.</li>
                                            <li><strong>SSC:</strong> Exclude inactive versions (<code>includeInactive=false</code> by default).</li>
                                            <li><strong>Both Platforms:</strong> Include all active applications and releases across all environments (Dev, QA, Production) unless user applies filters.</li>
                                            <li><strong>Note:</strong> Do NOT default to Production-only. Only filter to Production when explicitly requested via <code>releaseFilters=sdlcStatusType:Production</code>.</li>
                                        </ul>
                                        
                                        <p><strong>Example Filtered Queries:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default view: All active applications, all non-retired releases (Dev/QA/Prod)
GET /api/v3/portfolio-metrics/summary

// Production releases only (user-applied filter, NOT default)
GET /api/v3/portfolio-metrics/summary?releaseFilters=sdlcStatusType:Production

// High-criticality applications (all environments)
GET /api/v3/portfolio-metrics/summary?filters=businessCriticalityType:High

// High-criticality applications with production releases
GET /api/v3/portfolio-metrics/summary?filters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production

// Custom filter: Finance business unit, PCI-compliant apps, production only
GET /api/v3/portfolio-metrics/summary?filters=attributes.BusinessUnit:Finance+attributes.ComplianceScope:PCI&releaseFilters=sdlcStatusType:Production

// User-driven filter: Medium/High criticality, QA or Production environments
GET /api/v3/portfolio-metrics/summary?filters=businessCriticalityType:High|Medium&releaseFilters=sdlcStatusType:Production|QA</code></pre>
                                        
                                        <p><strong>Response with Filtering Context:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "applications": {
    "total": 123,
    "withScans": 101,
    "newThisYear": 15
  },
  "versions": {
    "total": 367,
    "active": 342,
    "retired": 25
  },
  "totalLinesOfCode": 74238419,
  "totalFilesScanned": 1984234,
  "totalOpenSourceComponents": 58432,
  "appliedFilters": {  // NEW: Echo back applied filters
    "applicationFilters": "businessCriticalityType:High",
    "releaseFilters": "sdlcStatusType:Production",
    "effectiveApplicationCount": 48,  // Subset after filtering
    "effectiveReleaseCount": 142      // Subset after filtering
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                    </div>
                                </details>

                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. GET /api/v3/scans/activity-by-month â­â­â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> KPI #9 (Scan Activity 12-month Trend) requires querying 12 separate month date ranges, paginating through 50-scan batches, then client-side grouping by month and scan type. Current implementation takes 15-30 API calls minimum.</p>
                                        
                                        <p><strong>Proposed Parameters:</strong></p>
                                        <ul>
                                            <li><code>startMonth</code> (string, format: "YYYY-MM", example: "2024-01")</li>
                                            <li><code>endMonth</code> (string, format: "YYYY-MM", example: "2024-12")</li>
                                            <li><code>groupBy</code> (enum: month|week|day, default: month)</li>
                                            <li><code>scanTypes</code> (optional CSV filter: "static,dynamic,mobile,openSource")</li>
                                            <li><code>includeFailures</code> (boolean, default false) - Include cancelled/failed scans</li>
                                        </ul>
                                        
                                        <p><strong>Proposed Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "months": [
    {
      "month": "2024-01",
      "static": 145,
      "dynamic": 87,
      "mobile": 23,
      "openSource": 134,
      "total": 389,
      "failed": 12
    },
    {
      "month": "2024-02",
      "static": 152,
      "dynamic": 91,
      "mobile": 19,
      "openSource": 128,
      "total": 390,
      "failed": 8
    }
    // ... 10 more months
  ],
  "summary": {
    "totalScans": 4682,
    "avgScansPerMonth": 390,
    "successRate": 97.6
  }
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Eliminates 12+ separate date-range queries</li>
                                            <li>Pre-aggregated data (no client-side grouping/counting)</li>
                                            <li>Supports flexible time ranges (weekly/daily for operational dashboards)</li>
                                            <li>Enables scan failure trend analysis (new metric opportunity)</li>
                                            <li>Consistent with FoD API design pattern (follows /applications, /releases structure)</li>
                                        </ul>
                                        
                                        <p><strong>Application & Release Attribute Filters:</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string) - Application-level filters (same syntax as endpoint #1):
                                                <ul>
                                                    <li><code>filters=businessCriticalityType:High</code></li>
                                                    <li><code>filters=attributes.BusinessUnit:Finance</code></li>
                                                </ul>
                                            </li>
                                            <li><code>releaseFilters</code> (string) - Release-level filters:
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code></li>
                                                    <li><code>releaseFilters=attributes.Environment:Production</code></li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>Example: Production scans for high-criticality apps only</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>GET /api/v3/scans/activity-by-month?startMonth=2024-01&endMonth=2024-12&filters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production</code></pre>
                                    </div>
                                </details>

                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. Enhance GET /api/v3/portfolio-metrics/summary - Add Coverage Statistics â­â­â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> KPIs #7 & #8 (Application Coverage & Version Coverage) require complex cross-referencing: applications â†’ releases â†’ scans, with deduplication logic to count unique applications/versions per scan type. Currently 200-500+ API calls.</p>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #28a745; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">ðŸ’¡ Why Add to portfolio-metrics/summary Instead of Separate Endpoint?</p>
                                            <p style="margin: 0; font-size: 13px;">Instead of creating a separate <code>GET /api/v3/applications/coverage-statistics</code> endpoint, integrate coverage stats into the already-proposed <code>portfolio-metrics/summary</code> endpoint (Enhancement #1). This provides:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Single API call</strong> for all Program Dashboard KPIs (inventory + coverage)</li>
                                                <li>âœ… <strong>Consistent filtering</strong> - Reuse existing <code>filters</code> + <code>releaseFilters</code> parameters already defined in enhancement #1</li>
                                                <li>âœ… <strong>Simpler API surface</strong> - No need to duplicate filter logic across multiple endpoints</li>
                                                <li>âœ… <strong>Better performance</strong> - Server can calculate inventory and coverage in single database query</li>
                                                <li>âœ… <strong>Easier client implementation</strong> - One fetch() call populates entire dashboard</li>
                                            </ul>
                                        </div>
                                        
                                        <p><strong>Proposed Enhancement to Response Schema:</strong></p>
                                        <p style="font-size: 13px; margin: 8px 0;">Add <code>scanCoverage</code> section to the existing <code>portfolio-metrics/summary</code> response:</p>
                                        
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "applications": {
    "total": 123,
    "withScans": 101,
    "newThisYear": 15
  },
  "versions": {
    "total": 367,
    "active": 342,
    "retired": 25
  },
  "totalLinesOfCode": 74238419,
  "totalFilesScanned": 1984234,
  "totalOpenSourceComponents": 58432,
  
  // ========== NEW: SCAN COVERAGE SECTION ==========
  "scanCoverage": {
    "applicationLevel": {
      "sast": { "covered": 101, "total": 123, "percentage": 82.1 },
      "dast": { "covered": 79, "total": 123, "percentage": 64.2 },
      "mobile": { "covered": 12, "total": 123, "percentage": 9.8 },
      "openSource": { "covered": 95, "total": 123, "percentage": 77.2 }
    },
    "versionLevel": {
      "sast": { "covered": 249, "total": 367, "percentage": 67.8 },
      "dast": { "covered": 198, "total": 367, "percentage": 53.9 },
      "mobile": { "covered": 18, "total": 367, "percentage": 4.9 },
      "openSource": { "covered": 287, "total": 367, "percentage": 78.2 }
    }
  },
  // ================================================
  
  "appliedFilters": {
    "applicationFilters": null,
    "releaseFilters": null
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>Query Parameters (Reuse from Enhancement #1):</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string) - Application-level filters (already defined in enhancement #1):
                                                <ul>
                                                    <li><code>filters=businessCriticalityType:High</code> - High-criticality apps only</li>
                                                    <li><code>filters=attributes.BusinessUnit:Finance</code> - Custom attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>releaseFilters</code> (string) - Release-level filters (already defined in enhancement #1):
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code> - Production releases only</li>
                                                    <li><code>releaseFilters=attributes.ComplianceScope:PCI</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>Additional Coverage-Specific Parameters:</strong></p>
                                        <ul>
                                            <li><code>includeCoverage</code> (boolean, default: true) - Include scanCoverage section in response (set false to reduce payload if coverage not needed)</li>
                                            <li><code>coverageLookbackDays</code> (integer, default: null = all-time) - Only count scans from last N days for "active coverage" metric
                                                <ul>
                                                    <li>Common values: 30, 60, 90 days</li>
                                                    <li>Example: <code>coverageLookbackDays=30</code> means "apps/versions with at least one scan in last 30 days"</li>
                                                    <li>Prevents stale scans from inflating coverage percentages</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>Example Queries:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default: All KPIs including coverage, all-time scans
GET /api/v3/portfolio-metrics/summary

// Production releases only, all KPIs
GET /api/v3/portfolio-metrics/summary?releaseFilters=sdlcStatusType:Production

// High-criticality apps, "active coverage" in last 30 days
GET /api/v3/portfolio-metrics/summary?filters=businessCriticalityType:High&coverageLookbackDays=30

// Finance BU, production only, recent scans (30 days)
GET /api/v3/portfolio-metrics/summary?filters=attributes.BusinessUnit:Finance&releaseFilters=sdlcStatusType:Production&coverageLookbackDays=30

// Minimal payload: Skip coverage section to reduce response size
GET /api/v3/portfolio-metrics/summary?includeCoverage=false</code></pre>
                                        
                                        <p><strong>Benefits Over Separate Endpoint:</strong></p>
                                        <ul>
                                            <li>âœ… <strong>Single API call</strong> for entire Program Dashboard (inventory + coverage) instead of 2 calls</li>
                                            <li>âœ… <strong>Consistent filtering</strong> - Same <code>filters</code>/<code>releaseFilters</code> apply to both inventory and coverage calculations</li>
                                            <li>âœ… <strong>No filter duplication</strong> - Avoids complex multi-attribute filter syntax on separate coverage endpoint</li>
                                            <li>âœ… <strong>Database efficiency</strong> - Server calculates all metrics in single query pass</li>
                                            <li>âœ… <strong>Simpler client code</strong> - One fetch, one response object, populate all KPIs</li>
                                            <li>âœ… <strong>Cleaner API surface</strong> - Fewer total endpoints to document/maintain</li>
                                        </ul>
                                        
                                        <p><strong>Implementation Notes:</strong></p>
                                        <ul>
                                            <li><strong>Coverage Calculation:</strong> Application is "covered" if it has at least one release with a completed scan of the specified type (SAST/DAST/Mobile/SCA). Version is "covered" if it has at least one completed scan of the specified type.</li>
                                            <li><strong>Deduplication:</strong> Count each application/version only once per scan type. Multiple SAST scans on same release = still counts as 1 covered version.</li>
                                            <li><strong>Scan Types:</strong> Map to FoD scanTypeId: SAST=1 (Static), DAST=2 (Dynamic), Mobile=3, OpenSource=4 (SCA)</li>
                                            <li><strong>Lookback Window:</strong> When <code>coverageLookbackDays</code> is set, filter scans by <code>completedDateTime >= (now - N days)</code>. This gives "active coverage" metric (e.g., "apps scanned in last 30 days") instead of "ever scanned".</li>
                                            <li><strong>Filter Consistency:</strong> The <code>filters</code> and <code>releaseFilters</code> parameters affect BOTH inventory counts (applications.total, versions.total) AND coverage counts (scanCoverage totals). This ensures percentages are calculated correctly for filtered subsets.</li>
                                            <li><strong>Performance:</strong> Coverage calculation can be optimized with JOIN between releases and scans tables, aggregated with COUNT DISTINCT. Cache results for 1 hour (coverage changes slowly).</li>
                                        </ul>
                                        
                                        <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-left: 3px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">ðŸŽ¯ Example Use Case: High-Criticality Production Coverage</p>
                                            <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px; margin: 8px 0;"><code>GET /api/v3/portfolio-metrics/summary?filters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production&coverageLookbackDays=30</code></pre>
                                            <p style="margin: 0; font-size: 13px;"><strong>Response:</strong></p>
                                            <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px; margin: 8px 0 0 0;"><code>{
  "applications": { "total": 48 },  // 48 high-criticality apps
  "versions": { "total": 142 },     // 142 production releases
  "scanCoverage": {
    "applicationLevel": {
      "sast": { "covered": 45, "total": 48, "percentage": 93.8 },  // 93.8% of critical apps scanned in last 30 days
      "dast": { "covered": 42, "total": 48, "percentage": 87.5 }   // 87.5% have DAST coverage
    },
    "versionLevel": {
      "sast": { "covered": 138, "total": 142, "percentage": 97.2 },  // 97.2% of prod releases scanned
      "dast": { "covered": 125, "total": 142, "percentage": 88.0 }
    }
  },
  "appliedFilters": {
    "applicationFilters": "businessCriticalityType:High",
    "releaseFilters": "sdlcStatusType:Production"
  }
}</code></pre>
                                            <p style="margin: 8px 0 0 0; font-size: 13px; color: var(--text);"><strong>Insight:</strong> "93.8% of our critical apps in production have recent SAST coverage - but 3 critical apps (6.2%) have no scans in last 30 days (investigation needed)"</p>
                                        </div>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--sev-high); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--sev-high);">ðŸŸ¡ High-Value Enhancements to Existing Endpoints</h4>
                                
                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-high);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. GET /api/v3/applications - Add includeSummaryMetrics parameter â­â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Current Behavior:</strong> Returns basic application metadata only (applicationId, applicationName, description, businessCriticalityType, ownerId). Requires separate queries to get release counts, scan status, LOC, component counts per application. Note: Need to align on if we report LOC/Files/Components at the Release vs App level.  If we report at the release level (probably?), we should make sure this data is available at the release endpoint.</p>
                                        
                                        <p><strong>Proposed Enhancement:</strong> Add optional <code>?includeSummaryMetrics=true</code> query parameter</p>
                                        
                                        <p><strong>Enhanced Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "items": [
    {
      "applicationId": 12345,
      "applicationName": "MyApp",
      "applicationDescription": "Customer Portal",
      "businessCriticalityType": "High",
      // ... existing fields ...
      "summaryMetrics": {  // NEW OBJECT
        "activeReleases": 3,
        "totalScans": 47,
        "latestScanDate": "2024-11-05T14:32:00Z",
        "primaryTechnologyStack": "JAVA",
        "totalLinesOfCode": 245819,
        "totalOpenSourceComponents": 142,
        "hasCompletedSAST": true,
        "hasCompletedDAST": false,
        "hasCompletedMobile": false,
        "hasCompletedOpenSource": true,
        "starRating": 3
      }
    }
  ],
  "totalCount": 123
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Eliminates need for per-application drill-down queries (releases, scans, components)</li>
                                            <li>Powers filtering: "show me applications without DAST coverage"</li>
                                            <li>Enables rich sorting: orderBy=totalLinesOfCode, orderBy=latestScanDate</li>
                                            <li>Reduces N+1 query pattern (1 apps query + N per-app queries)</li>
                                            <li>Minimal backend effort (data already joined internally for other endpoints)</li>
                                        </ul>
                                        
                                        <p><strong>Optional Sub-Parameters:</strong></p>
                                        <ul>
                                            <li><code>summaryMetrics.includeHistorical</code> - Add YoY growth trends (releases added this year, scans YoY %)</li>
                                        </ul>
                                    </div>
                                </details>

                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-high);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. GET /api/v3/releases - Add latestScans embedded object â­â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Current Behavior:</strong> Returns release metadata. Requires separate <code>GET /api/v3/scans?filters=releaseId:{id}</code> query per release to check scan status/types.</p>
                                        
                                        <p><strong>Proposed Enhancement:</strong> Add optional <code>?includeLatestScans=true</code> query parameter</p>
                                        
                                        <p><strong>Enhanced Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "items": [
    {
      "releaseId": 67890,
      "releaseName": "v2.5-PROD",
      "applicationId": 12345,
      "sdlcStatusType": "Production",
      "suspended": false,
      // ... existing fields ...
      "latestScans": {  // NEW OBJECT
        "static": {
          "scanId": 111222,
          "completedDate": "2024-11-01T08:15:00Z",
          "analysisStatusType": "Completed",
          "linesOfCode": 245819,
          "fileCount": 3421,
          "starRating": 3
        },
        "dynamic": null,  // No DAST scan for this release
        "mobile": null,
        "openSource": {
          "scanId": 111223,
          "completedDate": "2024-11-01T08:20:00Z",
          "analysisStatusType": "Completed",
          "componentCount": 142
        }
      }
    }
  ],
  "totalCount": 367
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Version Coverage (KPI #8) calculation becomes single API call instead of 367+ queries</li>
                                            <li>Eliminates classic N+1 query pattern</li>
                                            <li>60% fewer API calls for dashboard rendering</li>
                                            <li>Enables filtering: <code>filters=latestScans.static:null</code> â†’ "releases without SAST scan"</li>
                                            <li>Powers scan recency alerting (releases not scanned in 90+ days)</li>
                                        </ul>
                                    </div>
                                </details>

                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-high);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. GET /api/v3/scans/{scanId}/summary - Add cached parameter â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Current Behavior:</strong> Returns <code>202 Accepted</code> if scan processing incomplete, requiring polling/retry logic. Dashboard implementers must build retry loops with exponential backoff.</p>
                                        
                                        <p><strong>Proposed Enhancement:</strong> Add optional <code>?cached=true</code> query parameter</p>
                                        
                                        <p><strong>Enhanced Behavior:</strong></p>
                                        <ul>
                                            <li>Always returns <code>200 OK</code> (never 202)</li>
                                            <li>Includes <code>"processingComplete": false</code> flag if scan still processing</li>
                                            <li>Returns available fields immediately (LOC, fileCount) even if full analysis pending</li>
                                            <li>Graceful degradation: partial data better than no data</li>
                                        </ul>
                                        
                                        <p><strong>Enhanced Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "scanId": 111222,
  "processingComplete": false,  // NEW FLAG
  "processingStatus": "In_Progress",  // NEW FIELD
  "staticScanSummaryDetails": {
    "totalLinesOfCode": 245819,  // Available immediately
    "fileCount": 3421,            // Available immediately
    "technologyStack": "JAVA",    // Available immediately
    "totalIssues": null,          // Not yet available
    "issueCountCritical": null    // Not yet available
  }
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Instant dashboard load (no waiting for scan processing to complete)</li>
                                            <li>Eliminates retry loop complexity in client code</li>
                                            <li>Reduced server load (fewer repeated polling requests)</li>
                                            <li>Better user experience (show partial data vs. loading spinner)</li>
                                            <li>Backwards compatible (default behavior unchanged without <code>?cached=true</code>)</li>
                                        </ul>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--accent); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--accent);">ðŸŸ¢ Nice-to-Have Enhancements</h4>
                                
                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. GET /api/v3/applications/technology-distribution â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> KPI #11 (Tech Stack Treemap) requires querying latest scan summary per release to extract <code>technologyStack</code> field, then client-side aggregation/counting. Currently 100-400 API calls.</p>
                                        
                                        <p><strong>Proposed Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "technologies": [
    { 
      "name": "JAVA", 
      "applicationCount": 47, 
      "percentage": 38.2,
      "totalLinesOfCode": 28350000,
      "averageLOC": 603191
    },
    { 
      "name": "JAVASCRIPT", 
      "applicationCount": 30, 
      "percentage": 24.4,
      "totalLinesOfCode": 12450000,
      "averageLOC": 415000
    },
    { "name": "DOTNET", "applicationCount": 22, "percentage": 17.9 },
    { "name": "PYTHON", "applicationCount": 12, "percentage": 9.8 },
    { "name": "GO", "applicationCount": 7, "percentage": 5.7 },
    { "name": "OTHER", "applicationCount": 5, "percentage": 4.1 }
  ],
  "totalApplications": 123
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Eliminates 100+ scan summary queries</li>
                                            <li>Supports treemap sizing by LOC (more meaningful than app count)</li>
                                            <li>Consistent technology grouping across customers</li>
                                        </ul>
                                    </div>
                                </details>

                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. GET /api/v3/portfolio-metrics/technology-stack-summary (NEW endpoint) â­</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Statement:</strong> Technology treemap (KPI #8) needs LOC and application count statistics per technology type to size bubbles accurately. Currently no endpoint provides this aggregated view - would require querying all applications/releases and calculating client-side.</p>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš ï¸ Why Not Enhance GET /api/v3/lookup-items?</p>
                                            <p style="margin: 0; font-size: 13px;"><code>lookup-items</code> is a utility endpoint designed for enum/reference data (key:value pairs for dropdown population, validation). Adding portfolio statistics would violate single responsibility principle and confuse API consumers expecting static lookup data. Better to create purpose-built endpoint for portfolio analytics.</p>
                                        </div>
                                        
                                        <p><strong>Alternative Considered:</strong> Could extend existing <code>GET /api/v3/portfolio-metrics/summary</code> (recommended in enhancement #1) with a <code>technologyStackBreakdown</code> section. This would work if portfolio-metrics/summary is implemented. If not, create standalone endpoint below.</p>
                                        
                                        <p><strong>Proposed New Endpoint:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>GET /api/v3/portfolio-metrics/technology-stack-summary</code></pre>
                                        
                                        <p><strong>Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "technologyStacks": [
    {
      "technologyType": "JAVA",
      "technologyTypeId": 1,
      "totalLinesOfCode": 28350000,
      "applicationCount": 47,
      "releaseCount": 142,
      "averageLOCPerApplication": 603191,
      "percentOfPortfolioLOC": 0.42,
      "languageLevels": [
        {
          "languageLevel": "17",
          "applicationCount": 28,
          "totalLOC": 17800000,
          "percentOfTechnology": 0.63
        },
        {
          "languageLevel": "11",
          "applicationCount": 12,
          "totalLOC": 7200000,
          "percentOfTechnology": 0.25
        },
        {
          "languageLevel": "8",
          "applicationCount": 7,
          "totalLOC": 3350000,
          "percentOfTechnology": 0.12
        }
      ]
    },
    {
      "technologyType": "JAVASCRIPT",
      "technologyTypeId": 2,
      "totalLinesOfCode": 12450000,
      "applicationCount": 30,
      "releaseCount": 89,
      "averageLOCPerApplication": 415000,
      "percentOfPortfolioLOC": 0.18,
      "languageLevels": [
        {
          "languageLevel": "ES2023",
          "applicationCount": 18,
          "totalLOC": 8200000,
          "percentOfTechnology": 0.66
        },
        {
          "languageLevel": "ES2020",
          "applicationCount": 12,
          "totalLOC": 4250000,
          "percentOfTechnology": 0.34
        }
      ]
    },
    {
      "technologyType": "DOTNET",
      "technologyTypeId": 3,
      "totalLinesOfCode": 15800000,
      "applicationCount": 38,
      "releaseCount": 94,
      "averageLOCPerApplication": 415789,
      "percentOfPortfolioLOC": 0.23,
      "languageLevels": [
        {
          "languageLevel": "6.0",
          "applicationCount": 22,
          "totalLOC": 9800000,
          "percentOfTechnology": 0.62
        },
        {
          "languageLevel": "8.0",
          "applicationCount": 10,
          "totalLOC": 4200000,
          "percentOfTechnology": 0.27
        },
        {
          "languageLevel": "4.8",
          "applicationCount": 6,
          "totalLOC": 1800000,
          "percentOfTechnology": 0.11
        }
      ]
    },
    {
      "technologyType": "PYTHON",
      "technologyTypeId": 4,
      "totalLinesOfCode": 8200000,
      "applicationCount": 22,
      "releaseCount": 53,
      "averageLOCPerApplication": 372727,
      "percentOfPortfolioLOC": 0.12,
      "languageLevels": [
        {
          "languageLevel": "3.11",
          "applicationCount": 14,
          "totalLOC": 5600000,
          "percentOfTechnology": 0.68
        },
        {
          "languageLevel": "3.9",
          "applicationCount": 8,
          "totalLOC": 2600000,
          "percentOfTechnology": 0.32
        }
      ]
    }
    // ... other technologies
  ],
  "portfolioTotals": {
    "totalLinesOfCode": 67500000,
    "totalApplications": 142,
    "totalReleases": 412,
    "uniqueTechnologyTypes": 12
  },
  "appliedFilters": {
    "applicationFilters": null,
    "releaseFilters": null
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>Query Parameters:</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string, optional) - Application-level filters using FoD filter syntax:
                                                <ul>
                                                    <li><code>filters=businessCriticalityType:High</code> - High-criticality apps only</li>
                                                    <li><code>filters=attributes.BusinessUnit:Engineering</code> - Custom attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>releaseFilters</code> (string, optional) - Release-level filters:
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code> - Production releases only</li>
                                                    <li><code>releaseFilters=attributes.ComplianceScope:SOC2</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>includeLanguageLevels</code> (boolean, default: true) - Include version/runtime breakdown per technology</li>
                                            <li><code>minApplicationCount</code> (integer, default: 1) - Exclude technologies with fewer than N applications (reduces noise)</li>
                                        </ul>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>âœ… <strong>Purpose-built for analytics:</strong> Dedicated endpoint for portfolio technology insights (vs. overloading lookup-items utility endpoint)</li>
                                            <li>âœ… <strong>Enables accurate treemap sizing:</strong> Bubble size = LOC (reflects actual complexity) instead of app count</li>
                                            <li>âœ… <strong>Supports drill-down:</strong> languageLevels array shows version distribution (e.g., "28 apps on Java 17, 12 on Java 11, 7 on Java 8 EOL")</li>
                                            <li>âœ… <strong>Investment prioritization:</strong> "42% of portfolio LOC is Java â†’ prioritize Java-specific SAST tools"</li>
                                            <li>âœ… <strong>Migration planning:</strong> Identify EOL version usage (e.g., ".NET Framework 4.8 still represents 11% of .NET LOC")</li>
                                            <li>âœ… <strong>Consistent with other portfolio endpoints:</strong> Uses same filters/releaseFilters architecture as portfolio-metrics/summary</li>
                                            <li>âœ… <strong>Single API call:</strong> Replaces 100+ calls to query all applications/releases for LOC aggregation</li>
                                        </ul>
                                        
                                        <p><strong>Example Use Cases:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default: All active applications/releases, include language versions
GET /api/v3/portfolio-metrics/technology-stack-summary

// Production releases only (most accurate for "what's in prod" analysis)
GET /api/v3/portfolio-metrics/technology-stack-summary?releaseFilters=sdlcStatusType:Production

// High-criticality apps, exclude technologies with <3 apps (reduce noise)
GET /api/v3/portfolio-metrics/technology-stack-summary?filters=businessCriticalityType:High&minApplicationCount=3

// Minimal payload (just top-level stats, no version breakdown)
GET /api/v3/portfolio-metrics/technology-stack-summary?includeLanguageLevels=false</code></pre>
                                        
                                        <p><strong>Implementation Notes:</strong></p>
                                        <ul>
                                            <li><strong>LOC Source:</strong> Aggregate from most recent scan per release. Use <code>totalLinesOfCode</code> field from scan metadata.</li>
                                            <li><strong>Deduplication:</strong> If application has multiple releases, count LOC once per release (releases may have different LOC). For application count, count unique applications.</li>
                                            <li><strong>Language Level Grouping:</strong> Group by <code>languageLevel</code> field within each <code>technologyType</code>. Some apps may have null languageLevel (handle gracefully).</li>
                                            <li><strong>Percentage Calculations:</strong> percentOfPortfolioLOC = technology LOC / portfolio total LOC. percentOfTechnology = language level LOC / technology total LOC.</li>
                                            <li><strong>Sorting:</strong> Order technologies by totalLinesOfCode descending (largest first). Order language levels by totalLOC descending within each technology.</li>
                                            <li><strong>Caching:</strong> Cache for 1 hour (technology stack changes slowly). Invalidate on new scan completion.</li>
                                        </ul>
                                        
                                        <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-left: 3px solid #28a745; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">ðŸ’¡ Alternative: Extend Existing portfolio-metrics/summary Endpoint</p>
                                            <p style="margin: 0 0 8px 0; font-size: 13px;">If <code>GET /api/v3/portfolio-metrics/summary</code> (Enhancement #1) is implemented, could add <code>technologyStackBreakdown</code> section to that response instead of creating standalone endpoint:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li><strong>Pro:</strong> Single call for all Program Dashboard KPIs (portfolio summary + technology stats)</li>
                                                <li><strong>Con:</strong> Larger response payload, couples two distinct analytics dimensions</li>
                                                <li><strong>Recommendation:</strong> Start with standalone endpoint for flexibility, consider consolidation later if performance data shows benefit</li>
                                            </ul>
                                        </div>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: linear-gradient(135deg, #1e3a5f 0%, #2a5a8f 100%); border-radius: 8px; color: white;">
                                <h4 style="margin-top: 0;">ðŸ“Š Performance Impact Summary</h4>
                                
                                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-top: 16px;">
                                    <div style="background: rgba(0,0,0,0.2); padding: 12px; border-radius: 4px;">
                                        <h5 style="margin-top: 0; color: #ff6b6b;">âŒ Current State (Without Enhancements)</h5>
                                        <ul style="margin: 8px 0; font-size: 14px;">
                                            <li>Program Dashboard load time: <strong>45-90 seconds</strong></li>
                                            <li>Total API calls: <strong>500-1000+</strong></li>
                                            <li>API rate limiting risk: <strong>High</strong> (burst traffic)</li>
                                            <li>Client-side processing: <strong>Heavy</strong> (deduplication, aggregation, filtering)</li>
                                            <li>Memory footprint: <strong>Large</strong> (10-50 MB JSON payloads)</li>
                                            <li>Network bandwidth: <strong>High</strong> (repeated round-trips)</li>
                                        </ul>
                                    </div>
                                    
                                    <div style="background: rgba(0,0,0,0.2); padding: 12px; border-radius: 4px;">
                                        <h5 style="margin-top: 0; color: #51cf66;">âœ… Future State (With Top 3 Endpoints)</h5>
                                        <ul style="margin: 8px 0; font-size: 14px;">
                                            <li>Program Dashboard load time: <strong>2-5 seconds</strong> ðŸš€</li>
                                            <li>Total API calls: <strong>~10</strong></li>
                                            <li>API rate limiting risk: <strong>Low</strong> (minimal calls)</li>
                                            <li>Client-side processing: <strong>Minimal</strong> (render pre-aggregated data)</li>
                                            <li>Memory footprint: <strong>Small</strong> (&lt;1 MB JSON payloads)</li>
                                            <li>Network bandwidth: <strong>Low</strong> (single round-trips)</li>
                                        </ul>
                                    </div>
                                </div>
                                
                                <div style="margin-top: 16px; padding: 16px; background: rgba(255,255,255,0.1); border-radius: 4px; border: 2px solid #51cf66;">
                                    <p style="margin: 0; font-size: 18px; font-weight: 600; text-align: center;">
                                        <span style="color: #51cf66;">âš¡ Performance Improvement: 90-95% reduction in load time</span>
                                    </p>
                                    <p style="margin: 8px 0 0 0; font-size: 14px; text-align: center; opacity: 0.9;">
                                        From ~60 seconds to ~3 seconds average dashboard render time
                                    </p>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--bg-surface); border: 3px solid #ffc107; border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--text);">âš ï¸ CRITICAL CROSS-CUTTING REQUIREMENT: Application & Release Attribute Filtering</h4>
                                
                                <p style="color: var(--text); margin-bottom: 12px;"><strong>Issue:</strong> All three critical endpoints (#1, #2, #3) MUST support comprehensive filtering by application-level and release-level attributes. This is non-negotiable for real-world dashboard implementations.</p>
                                
                                <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 12px; color: #333;">
                                    <p style="margin-top: 0;"><strong>Why This Matters:</strong></p>
                                    <ul style="margin-bottom: 12px;">
                                        <li><strong>Flexible Filtering:</strong> Dashboards must support filtering to production-only views when needed, but should default to all SDLC statuses (Dev/QA/Production), excluding only inactive/retired/suppressed items</li>
                                        <li><strong>Business Criticality:</strong> Executives may want to focus on high-criticality apps while excluding testing/sandbox apps</li>
                                        <li><strong>Custom Attributes:</strong> Customers define attributes like "Business Unit", "Compliance Scope", "Geography", "Data Classification" and need to filter by these</li>
                                        <li><strong>User-Driven Filtering:</strong> Dashboard users must apply filters via UI controls (dropdowns, multi-select) that translate to API filters</li>
                                        <li><strong>Segmentation:</strong> "Show me coverage for Finance BU production apps only" is a common use case</li>
                                    </ul>
                                    
                                    <p><strong>Recommended Filter Architecture:</strong></p>
                                    <ul style="margin-bottom: 12px;">
                                        <li><code>filters</code> parameter - Application-level attribute filtering (businessCriticalityType, applicationTypeId, custom attributes)</li>
                                        <li><code>releaseFilters</code> parameter - Release-level attribute filtering (sdlcStatusType, suspended, custom release attributes)</li>
                                        <li>Use existing FoD filter syntax: <code>field:value</code>, <code>+</code> for AND, <code>|</code> for OR</li>
                                        <li>Support custom attributes: <code>attributes.{attributeName}:{value}</code> pattern</li>
                                        <li>Return <code>appliedFilters</code> object in response showing what filters were applied and effective counts</li>
                                    </ul>
                                    
                                    <p><strong>Common Filter Patterns:</strong></p>
                                    <pre style="background: #f5f5f5; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 12px; margin: 8px 0;"><code>// Production releases only (optional filter for production-specific views)
?releaseFilters=sdlcStatusType:Production

// High-criticality apps, production releases
?filters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production

// Custom: Finance BU, PCI-compliant, production/QA
?filters=attributes.BusinessUnit:Finance+attributes.ComplianceScope:PCI&releaseFilters=sdlcStatusType:Production|QA

// Multi-select: High or Medium criticality, active releases
?filters=businessCriticalityType:High|Medium&releaseFilters=suspended:false</code></pre>
                                    
                                    <p style="margin-bottom: 0;"><strong>Implementation Note:</strong> Without robust attribute filtering, these endpoints become "all or nothing" - useful for global metrics but not for the segmented, filtered views that real dashboards require. This filtering capability should be designed into all three critical endpoints from the start, not added as an afterthought.</p>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸŽ¯ Implementation Priority Ranking</h4>
                                
                                <table style="width: 100%; border-collapse: collapse; font-size: 14px;">
                                    <thead>
                                        <tr style="background: var(--accent); color: white;">
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">Priority</th>
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">Endpoint / Enhancement</th>
                                            <th style="padding: 8px; text-align: center; border: 1px solid var(--border);">Impact</th>
                                            <th style="padding: 8px; text-align: center; border: 1px solid var(--border);">Effort</th>
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">KPIs Powered</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-critical);">#1</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">
                                                <strong>Portfolio Metrics Summary</strong>
                                                <div style="font-size: 12px; color: #666; margin-top: 4px;">âš ï¸ <em>Partial coverage by existing <code>tenant-summary</code> (lacks LOC/files/components, advanced filtering)</em></div>
                                            </td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­â­ High</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#1, #2, #4, #5, #6</td>
                                        </tr>
                                        <tr style="background: rgba(0,0,0,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-critical);">#2</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Scan Activity by Month</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­â­ High</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#9</td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-critical);">#3</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Coverage Statistics</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­â­ High</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Medium</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#7, #8</td>
                                        </tr>
                                        <tr style="background: rgba(0,0,0,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-high);">#4</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Applications + Summary Metrics</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­ Medium</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Multiple (filtering/sorting)</td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-high);">#5</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Releases + Latest Scans</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­ Medium</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Medium</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#8 (Version Coverage)</td>
                                        </tr>
                                        <tr style="background: rgba(0,0,0,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-medium);">#6</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Scan Summary Caching</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­ Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">UX improvement</td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-medium);">#7</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Technology Distribution</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­ Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#11 (Tech Stack)</td>
                                        </tr>
                                        <tr style="background: rgba(0,0,0,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-low);">#8</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">TechnologyTypes + LOC</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­ Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Very Low</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">#11 (enhancement)</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--bg-surface); border: 2px solid #ffc107; border-radius: 8px; color: var(--text);">
                                <h4 style="margin-top: 0; color: var(--text);">ðŸ’¡ Additional Benefits Beyond Dashboard Performance</h4>
                                <ul style="margin: 8px 0;">
                                    <li><strong>Reduced Server Load:</strong> Aggregation happens once server-side vs. thousands of times client-side</li>
                                    <li><strong>Consistent Calculations:</strong> All users see same metrics (no client-side calculation drift)</li>
                                    <li><strong>API Rate Limit Relief:</strong> Customers won't hit rate limits loading dashboards</li>
                                    <li><strong>Better Caching:</strong> Server can cache aggregated results for all users (vs. raw data per-user)</li>
                                    <li><strong>Attribute Filtering Unlocks Segmentation:</strong> Production-only views, business unit dashboards, compliance-scoped metrics all become trivial to implement</li>
                                    <li><strong>Custom Attribute Support:</strong> Customers can filter by ANY custom attribute they've defined (Business Unit, Geography, Compliance Scope, etc.) without API changes</li>
                                    <li><strong>User-Driven Filtering:</strong> Dashboard UI controls (dropdowns, multi-select) can directly translate to API filter parameters</li>
                                    <li><strong>Role-Based Dashboards:</strong> Different users see different data based on their scope (e.g., Finance team sees only Finance BU apps)</li>
                                    <li><strong>Mobile-Friendly:</strong> Lightweight responses enable mobile dashboard apps</li>
                                    <li><strong>Third-Party Integration:</strong> Partners can build dashboards without complex aggregation logic or filter implementation</li>
                                    <li><strong>Future-Proof:</strong> Endpoints support additional filters/groupings as needs evolve (just add new custom attributes)</li>
                                    <li><strong>Audit Trail:</strong> Response includes appliedFilters showing exactly what data user is viewing (important for compliance/governance)</li>
                                </ul>
                            </div>

                            <p style="margin-top: 24px; padding-top: 16px; border-top: 1px solid var(--border);"><strong>Recommendation:</strong> Implement top 3 endpoints (#1, #2, #3) as Phase 1 <strong>with full attribute filtering support from day one</strong>. These deliver 80%+ of performance gains with minimal backend effort. Attribute filtering is not optional - it's what makes these endpoints useful for real-world dashboards. Phase 2 can add enhancements #4-#5 based on customer demand. Endpoints #6-#8 are optional polish.</p>
                        </div>
                    </details>
                </div>
            </div>

        </section>

        <!-- ===================== RISK EXPOSURE DASHBOARD ===================== -->
        <section id="page-risk" class="page" aria-label="Risk Exposure Dashboard">
            <!-- ROW 1: Star Rating + Policy Compliance -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Versions by Policy Compliance</h3>
                    <div class="donut-wrap">
                        <div class="donut" style="background:conic-gradient(
                var(--compliance-pass) 0% 60%,
                var(--compliance-fail) 0% 85%,
                var(--compliance-unassessed) 0% 100%
              ) !important;"></div>
                        <div>
                            <div class="kpi">60% <span class="muted">Passing</span> <span style="color:var(--compliance-pass);font-weight:500;">(220)</span></div>
                            <div class="muted">25% Failing <span style="color:var(--compliance-fail);font-weight:500;">(92)</span></div>
                            <div class="muted">15% Unassessed <span style="color:var(--compliance-unassessed);font-weight:500;">(55)</span></div>
                            <div class="legend-coverage" aria-hidden="true">
                                <span class="sw"><i style="background:var(--compliance-pass)"></i>Passing</span>
                                <span class="sw"><i style="background:var(--compliance-fail)"></i>Failing</span>
                                <span class="sw"><i
                                        style="background:var(--compliance-unassessed)"></i>Unassessed</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="card col-6">
                    <h3>Versions by Star Rating</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:#9CA3AF"></i>0â˜… (not scanned)</span>
                        <span class="sw"><i style="background:var(--sev-critical)"></i>1â˜… (criticals)</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>2â˜… (highs)</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>3â˜… (mediums)</span>
                        <span class="sw"><i style="background:#1aa364"></i>4â˜…â€“5â˜… (low/none)</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Versions by star rating">
                        <div class="hlabel">0â˜…</div>
                        <div class="hbar" style="--pct:8; --fill-color:#9CA3AF">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">8% (29)</div>
                        <div class="hlabel">1â˜…</div>
                        <div class="hbar" style="--pct:6; --fill-color:var(--sev-critical)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">6% (22)</div>
                        <div class="hlabel">2â˜…</div>
                        <div class="hbar" style="--pct:12; --fill-color:var(--sev-high)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">12% (44)</div>
                        <div class="hlabel">3â˜…</div>
                        <div class="hbar" style="--pct:26; --fill-color:var(--sev-medium)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">26% (95)</div>
                        <div class="hlabel">4â˜…</div>
                        <div class="hbar" style="--pct:28; --fill-color:#1aa364">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">28% (103)</div>
                        <div class="hlabel">5â˜…</div>
                        <div class="hbar" style="--pct:20; --fill-color:#0E9F6E">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">20% (74)</div>
                    </div>
                </div>
            </div>

            <!-- ROW 2: Open Issues by Severity (All & Production with total count) + Vulnerability Density -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>Open Issues by Severity (All)</h3>
                    <div class="hmeta">134,299 Total</div>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="donut-wrap">
                        <div class="donut" style="background:conic-gradient(
                var(--sev-critical) 0% 12%,
                var(--sev-high) 12% 30%,
                var(--sev-medium) 30% 52%,
                var(--sev-low) 52% 100%
              ) !important;"></div>
                        <div>
                            <div style="font-size: 24px; color: var(--text-muted);">12% Critical <span
                                    style="color:var(--sev-critical);font-weight:500;">(16,116)</span></div>
                            <div class="muted">18% High <span
                                    style="color:var(--sev-high);font-weight:500;">(24,173)</span></div>
                            <div class="muted">22% Medium <span
                                    style="color:var(--sev-medium);font-weight:500;">(29,546)</span></div>
                            <div class="muted">48% Low <span
                                    style="color:var(--sev-low);font-weight:500;">(64,464)</span></div>
                        </div>
                    </div>
                </div>


                <div class="card col-4">
                    <h3>Open Issues by Severity (Production)</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div style="display: flex; flex-direction: column; gap: 16px; margin-top: 16px;">
                        <div style="display: flex; align-items: center; gap: 12px;">
                            <span style="width: 70px; color: var(--sev-critical); font-weight: 600;">Critical</span>
                            <div
                                style="height: 28px; background: var(--sev-critical); border-radius: 6px; position: relative; width: 19%; min-width: 32px; max-width: 100%;">
                                <span
                                    style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%); color: #fff; font-weight: 600;">9</span>
                            </div>
                        </div>
                        <div style="display: flex; align-items: center; gap: 12px;">
                            <span style="width: 70px; color: var(--sev-high); font-weight: 600;">High</span>
                            <div
                                style="height: 28px; background: var(--sev-high); border-radius: 6px; position: relative; width: 30%; min-width: 32px; max-width: 100%;">
                                <span
                                    style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%); color: #fff; font-weight: 600;">14</span>
                            </div>
                        </div>
                        <div style="display: flex; align-items: center; gap: 12px;">
                            <span style="width: 70px; color: var(--sev-medium); font-weight: 600;">Medium</span>
                            <div
                                style="height: 28px; background: var(--sev-medium); border-radius: 6px; position: relative; width: 38%; min-width: 32px; max-width: 100%;">
                                <span
                                    style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%); color: #fff; font-weight: 600;">18</span>
                            </div>
                        </div>
                        <div style="display: flex; align-items: center; gap: 12px;">
                            <span style="width: 70px; color: var(--sev-low); font-weight: 600;">Low</span>
                            <div style="height: 28px; background: var(--sev-low); border-radius: 6px; position: relative; width: 100%; min-width: 32px; max-width: 100%;">
                                <span style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%); color: #fff; font-weight: 600;">48</span>
                            </div>
                        </div>
                        <div style="display: flex; align-items: center; gap: 12px; padding-top: 8px; border-top: 1px solid var(--border); margin-top: 8px;">
                            <span style="width: 70px; color: var(--text); font-weight: 600;">Total</span>
                            <div style="flex: 1; text-align: right;">
                                <span style="font-size: 24px; font-weight: 700; color: var(--text);">89</span>
                                <span style="font-size: 14px; color: var(--text-muted); margin-left: 4px;">issues</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="card col-4">
                    <h3>Vulnerability Density</h3>
                    <div class="kpi">7.4 <span class="muted">vulnerabilities per version</span></div>
                    <span class="delta negative" aria-label="Down 38 percent year over year" title="-8% vs. last year">
                        <i>â–¼</i> 8% YoY
                    </span>
                </div>
            </div>

            <!-- ROW 3: New Issues by Severity (Monthly, grouped bar chart with axes) -->
            <div class="grid row">
                <div class="card col-12">
                    <h3>New Issue Detection Trend</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="trend-grouped">
                        <div class="y-axis-label">Number of New Issues</div>
                        <div class="y-axis">
                            <div>400</div>
                            <div>320</div>
                            <div>240</div>
                            <div>160</div>
                            <div>80</div>
                            <div>0</div>
                        </div>
                        <div class="chart-area">
                            <!-- Jan -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:35px; background:var(--sev-critical)" title="Critical: 70"></div>
                                <div class="grouped-bar" style="height:70px; background:var(--sev-high)" title="High: 140"></div>
                                <div class="grouped-bar" style="height:140px; background:var(--sev-medium)" title="Medium: 280"></div>
                                <div class="grouped-bar" style="height:55px; background:var(--sev-low)" title="Low: 110"></div>
                            </div>
                            <!-- Feb -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:65px; background:var(--sev-critical)" title="Critical: 130"></div>
                                <div class="grouped-bar" style="height:125px; background:var(--sev-high)" title="High: 250"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--sev-medium)" title="Medium: 160"></div>
                                <div class="grouped-bar" style="height:90px; background:var(--sev-low)" title="Low: 180"></div>
                            </div>
                            <!-- Mar -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:95px; background:var(--sev-critical)" title="Critical: 190"></div>
                                <div class="grouped-bar" style="height:55px; background:var(--sev-high)" title="High: 110"></div>
                                <div class="grouped-bar" style="height:160px; background:var(--sev-medium)" title="Medium: 320"></div>
                                <div class="grouped-bar" style="height:45px; background:var(--sev-low)" title="Low: 90"></div>
                            </div>
                            <!-- Apr -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:40px; background:var(--sev-critical)" title="Critical: 80"></div>
                                <div class="grouped-bar" style="height:140px; background:var(--sev-high)" title="High: 280"></div>
                                <div class="grouped-bar" style="height:70px; background:var(--sev-medium)" title="Medium: 140"></div>
                                <div class="grouped-bar" style="height:105px; background:var(--sev-low)" title="Low: 210"></div>
                            </div>
                            <!-- May -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:85px; background:var(--sev-critical)" title="Critical: 170"></div>
                                <div class="grouped-bar" style="height:110px; background:var(--sev-high)" title="High: 220"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-medium)" title="Medium: 190"></div>
                                <div class="grouped-bar" style="height:65px; background:var(--sev-low)" title="Low: 130"></div>
                            </div>
                            <!-- Jun -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:75px; background:var(--sev-critical)" title="Critical: 150"></div>
                                <div class="grouped-bar" style="height:65px; background:var(--sev-high)" title="High: 130"></div>
                                <div class="grouped-bar" style="height:125px; background:var(--sev-medium)" title="Medium: 250"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--sev-low)" title="Low: 160"></div>
                            </div>
                            <!-- Jul -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:60px; background:var(--sev-critical)" title="Critical: 120"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-high)" title="High: 190"></div>
                                <div class="grouped-bar" style="height:105px; background:var(--sev-medium)" title="Medium: 210"></div>
                                <div class="grouped-bar" style="height:40px; background:var(--sev-low)" title="Low: 80"></div>
                            </div>
                            <!-- Aug -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:50px; background:var(--sev-critical)" title="Critical: 100"></div>
                                <div class="grouped-bar" style="height:135px; background:var(--sev-high)" title="High: 270"></div>
                                <div class="grouped-bar" style="height:65px; background:var(--sev-medium)" title="Medium: 130"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-low)" title="Low: 190"></div>
                            </div>
                            <!-- Sep -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:70px; background:var(--sev-critical)" title="Critical: 140"></div>
                                <div class="grouped-bar" style="height:60px; background:var(--sev-high)" title="High: 120"></div>
                                <div class="grouped-bar" style="height:145px; background:var(--sev-medium)" title="Medium: 290"></div>
                                <div class="grouped-bar" style="height:75px; background:var(--sev-low)" title="Low: 150"></div>
                            </div>
                            <!-- Oct -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:55px; background:var(--sev-critical)" title="Critical: 110"></div>
                                <div class="grouped-bar" style="height:100px; background:var(--sev-high)" title="High: 200"></div>
                                <div class="grouped-bar" style="height:115px; background:var(--sev-medium)" title="Medium: 230"></div>
                                <div class="grouped-bar" style="height:60px; background:var(--sev-low)" title="Low: 120"></div>
                            </div>
                            <!-- Nov -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:45px; background:var(--sev-critical)" title="Critical: 90"></div>
                                <div class="grouped-bar" style="height:120px; background:var(--sev-high)" title="High: 240"></div>
                                <div class="grouped-bar" style="height:90px; background:var(--sev-medium)" title="Medium: 180"></div>
                                <div class="grouped-bar" style="height:85px; background:var(--sev-low)" title="Low: 170"></div>
                            </div>
                            <!-- Dec -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:65px; background:var(--sev-critical)" title="Critical: 130"></div>
                                <div class="grouped-bar" style="height:75px; background:var(--sev-high)" title="High: 150"></div>
                                <div class="grouped-bar" style="height:135px; background:var(--sev-medium)" title="Medium: 270"></div>
                                <div class="grouped-bar" style="height:70px; background:var(--sev-low)" title="Low: 140"></div>
                            </div>
                        </div>
                        <div class="x-axis">
                            <div class="month-label">Jan</div>
                            <div class="month-label">Feb</div>
                            <div class="month-label">Mar</div>
                            <div class="month-label">Apr</div>
                            <div class="month-label">May</div>
                            <div class="month-label">Jun</div>
                            <div class="month-label">Jul</div>
                            <div class="month-label">Aug</div>
                            <div class="month-label">Sep</div>
                            <div class="month-label">Oct</div>
                            <div class="month-label">Nov</div>
                            <div class="month-label">Dec</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ROW 4: Top Categories + Dependency Risk -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Most Prevalent Vulnerabilities</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Category</th>
                                <th style="text-align:right;">Open Issues</th>
                                <th style="text-align:right;">Critical</th>
                                <th style="text-align:right;">High</th>
                                <th style="text-align:right;">Versions</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Command Injection</td>
                                <td style="text-align:right">342</td>
                                <td style="text-align:right">58</td>
                                <td style="text-align:right">104</td>
                                <td style="text-align:right">28</td>
                            </tr>
                            <tr>
                                <td>SQL Injection</td>
                                <td style="text-align:right">298</td>
                                <td style="text-align:right">42</td>
                                <td style="text-align:right">87</td>
                                <td style="text-align:right">24</td>
                            </tr>
                            <tr>
                                <td>Cross-Site Scripting (XSS)</td>
                                <td style="text-align:right">212</td>
                                <td style="text-align:right">29</td>
                                <td style="text-align:right">64</td>
                                <td style="text-align:right">19</td>
                            </tr>
                            <tr>
                                <td>XML External Entity (XXE)</td>
                                <td style="text-align:right">166</td>
                                <td style="text-align:right">18</td>
                                <td style="text-align:right">51</td>
                                <td style="text-align:right">17</td>
                            </tr>
                            <tr>
                                <td>Server-Side Request Forgery (SSRF)</td>
                                <td style="text-align:right">144</td>
                                <td style="text-align:right">22</td>
                                <td style="text-align:right">48</td>
                                <td style="text-align:right">15</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="card col-6">
                    <h3>Security Technical Debt Aging</h3>
                    <div class="matrix" role="table" aria-label="Open issues by age and severity">
                        <div></div>
                        <div class="mx-h">&lt;30d</div>
                        <div class="mx-h">30â€“60d</div>
                        <div class="mx-h">60â€“90d</div>
                        <div class="mx-h">&gt;90d</div>
                        <div class="mx-rowlabel">Critical</div>
                        <div class="mx-cell crit"><span class="badge">42</span></div>
                        <div class="mx-cell crit"><span class="badge">35</span></div>
                        <div class="mx-cell crit"><span class="badge">22</span></div>
                        <div class="mx-cell crit"><span class="badge">18</span></div>
                        <div class="mx-rowlabel">High</div>
                        <div class="mx-cell high"><span class="badge">88</span></div>
                        <div class="mx-cell high"><span class="badge">73</span></div>
                        <div class="mx-cell high"><span class="badge">51</span></div>
                        <div class="mx-cell high"><span class="badge">39</span></div>
                        <div class="mx-rowlabel">Medium</div>
                        <div class="mx-cell med"><span class="badge">132</span></div>
                        <div class="mx-cell med"><span class="badge">104</span></div>
                        <div class="mx-cell med"><span class="badge">76</span></div>
                        <div class="mx-cell med"><span class="badge">55</span></div>
                        <div class="mx-rowlabel">Low</div>
                        <div class="mx-cell low"><span class="badge">165</span></div>
                        <div class="mx-cell low"><span class="badge">142</span></div>
                        <div class="mx-cell low"><span class="badge">118</span></div>
                        <div class="mx-cell low"><span class="badge">94</span></div>
                    </div>
                </div>

            </div>

            <!-- ROW 5: Dependency Risk + License Risk -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Open Source Security Risk</h3>
                    <div class="donut-wrap">
                        <div class="donut" style="--pct:38; --ring-color:var(--status-error)"></div>
                        <div>
                            <div class="kpi">22,204 (38%) <span class="muted">components vulnerable</span></div>
                            <div class="muted">36,228 (62%) components with no vulnerabilities</div>
                            <div class="legend" aria-hidden="true">
                                <span class="sw"><i style="background:var(--status-error)"></i>Vulnerable
                                    dependencies</span>
                                <span class="sw"><i style="background:var(--track)"></i>Secure dependencies</span>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card col-6">
                    <h3>Open Source License Risk</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:#DC2626"></i>Strong Copy Left</span>
                        <span class="sw"><i style="background:#F97316"></i>Weak Copy Left</span>
                        <span class="sw"><i style="background:#10B981"></i>Permissive</span>
                        <span class="sw"><i style="background:#6B7280"></i>Unknown</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Open source license risk">
                        <div class="hlabel">Strong Copy Left</div>
                        <div class="hbar" style="--pct:8; --fill-color:#DC2626">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">8% (4,674)</div>
                        <div class="hlabel">Weak Copy Left</div>
                        <div class="hbar" style="--pct:14; --fill-color:#F97316">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">14% (8,180)</div>
                        <div class="hlabel">Permissive</div>
                        <div class="hbar" style="--pct:72; --fill-color:#10B981">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">72% (42,071)</div>
                        <div class="hlabel">Unknown</div>
                        <div class="hbar" style="--pct:6; --fill-color:#6B7280">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">6% (3,507)</div>
                    </div>
                </div>
            </div>

            <!-- KPI Analysis (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ“Š Risk Exposure Dashboard KPI Analysis</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>Target Personas:</strong> Director of Application Security, Application Security Engineers, CISOs, Risk Management Teams</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Versions by Policy Compliance - Value: 10/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of application versions that pass, fail, or haven't been assessed against security policies (e.g., "no criticals in production," "OWASP Top 10 free").</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> Executive-level risk indicator for board reporting. 25% failing = immediate escalation. Tracks policy enforcement effectiveness.</li>
                                <li><strong>Engineers:</strong> Identifies which versions need urgent remediation to pass policy gates. Drives release readiness decisions.</li>
                                <li><strong>Risk Teams:</strong> Quantifies policy compliance for audit/regulatory requirements. Shows governance maturity.</li>
                                <li><strong>Insights gained:</strong> Policy effectiveness, release quality trends, enforcement gaps, audit readiness.</li>
                                <li><strong>Actions driven:</strong> Release blocks, emergency fixes, policy refinement, compliance reporting, audit preparation.</li>
                            </ul>
                            <p><strong>Value Rating: 10/10</strong><br><em>Reasoning:</em> Perfect KPIâ€”directly measures security policy effectiveness and business risk. The 25% failing rate is an immediate red flag requiring action. Binary pass/fail aligns with decision gates (deploy vs. don't deploy). Unassessed 15% reveals coverage gaps. Audit-friendly and executive-friendly. Directly ties security activities to business outcomes. This is a north-star metric for mature AppSec programs.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Policy quality dependency:</strong> Only as good as the underlying policiesâ€”weak policies give false confidence.</li>
                                <li><strong>Binary oversimplification:</strong> A version failing by 1 critical vs. 100 criticals both count as "failing"â€”lacks severity depth.</li>
                                <li><strong>Policy drift:</strong> Policies may not reflect current threat landscape if not regularly updated.</li>
                                <li><strong>Pressure to game:</strong> Teams might pressure to weaken policies to improve pass rates rather than fix issues.</li>
                                <li><strong>Unassessed ambiguity:</strong> 15% unassessedâ€”is this new releases not yet scanned or legacy versions abandoned? Needs clarification.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Versions by Star Rating - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Application versions categorized by security quality: 0â˜… (not scanned), 1â˜… (critical vulns), 2â˜… (high, no critical), 3â˜… (medium or lower), 4-5â˜… (low/none). Current: 48% are 4-5â˜… (healthy), 18% are 1-2â˜… (critical risk), 8% unscanned.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> High-level portfolio health indicator. 48% in 4-5â˜… shows program success. 18% in 1-2â˜… = risk concentration requiring investment.</li>
                                <li><strong>Engineers:</strong> Prioritization frameworkâ€”focus on moving 1-2â˜… versions up the scale. Gamification element (improve star ratings).</li>
                                <li><strong>Insights gained:</strong> Portfolio risk distribution, remediation effectiveness, quality trends over time.</li>
                                <li><strong>Actions driven:</strong> Targeted remediation campaigns, resource allocation to low-star apps, celebrating improvements.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Excellent simplification of complex risk data into intuitive rating system. Easy for non-technical stakeholders to understand (everyone knows 5â˜… > 1â˜…). Provides clear quality tiers for prioritization. Motivational framing (improve ratings). Shows distribution across the spectrum rather than just pass/fail. Loses 2 points because star definitions need context (what's the threshold for 4â˜… vs 5â˜…?) and because it could encourage gaming the thresholds.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Threshold ambiguity:</strong> What's the exact criteria for each star level? Must be clearly documented or creates confusion.</li>
                                <li><strong>Threshold gaming:</strong> Teams might focus on barely crossing thresholds rather than comprehensive security improvement.</li>
                                <li><strong>One-dimensional:</strong> Only considers vulnerability severity, not exploitability, business criticality, or exposure.</li>
                                <li><strong>False equivalence:</strong> All 4â˜… apps aren't equalâ€”one might have 2 low vulns, another 200 low vulns.</li>
                                <li><strong>8% unscanned (0â˜…):</strong> Why are these not scanned? Coverage gap or abandoned versions? Needs investigation.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. Open Issues by Severity - All Environments - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Total vulnerability backlog across all environments, showing severity distribution. 16,116 critical, 24,173 high, 29,546 medium, 64,516 low = 134,351 total open issues.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Backlog health checkâ€”12% critical is concerning but manageable. Shows overall security debt scale. Trend matters (growing or shrinking?).</li>
                                <li><strong>Engineers:</strong> Workload visibilityâ€”134K issues is overwhelming without prioritization. Validates need for risk-based approaches.</li>
                                <li><strong>Insights gained:</strong> Security debt magnitude, severity distribution patterns, resource needs assessment.</li>
                                <li><strong>Actions driven:</strong> Prioritization strategy development, resource planning, tooling tuning (too noisy?), risk acceptance decisions.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Important baseline metric showing total exposure and severity distribution. The 12%/18% split (critical/high) is useful for prioritization. Donut chart efficiently shows proportions. However, absolute numbers (134K) can be paralyzing without context. Lacks trend data (is this improving?), environment context (dev vs prod), and exploitability/priority dimensions. Useful but not actionable on its own.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Analysis paralysis:</strong> 134,351 issues is overwhelmingâ€”users need filtering/prioritization, not just counts.</li>
                                <li><strong>No trend context:</strong> Is 16K critical issues good or bad? Better or worse than last month? Lacks time dimension.</li>
                                <li><strong>Environment mixing:</strong> Combining dev/test/prod obscures true production risk. A critical in dev â‰  critical in prod.</li>
                                <li><strong>Severity inflation:</strong> Are these CVSS scores, tool defaults, or analyst-reviewed? Tool noise inflates counts.</li>
                                <li><strong>Lacks prioritization:</strong> Not all criticals are equalâ€”needs exploit availability, asset criticality, exposure factors.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Open Issues by Severity - Production Only - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Production-only vulnerability backlogâ€”the issues that truly matter for business risk. 2,106 critical, 3,508 high, 5,614 medium, 12,158 low = 23,386 total production issues.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> True business risk indicator. 2,106 critical in production = board-level concern. Shows real-world exposure.</li>
                                <li><strong>Engineers:</strong> Clear remediation priority queue. Production criticals = drop everything and fix. Environment filter reduces noise by 82% (134Kâ†’23K).</li>
                                <li><strong>Risk Teams:</strong> Actual risk exposure for insurance, compliance, risk registers. Dev/test issues don't count.</li>
                                <li><strong>Insights gained:</strong> Real business exposure, release gate effectiveness (how did 2K criticals reach prod?), remediation urgency.</li>
                                <li><strong>Actions driven:</strong> Emergency patches, production rollbacks, release process improvements, executive escalations.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional KPIâ€”filters out dev/test noise to show true business risk. The 6Ã— reduction (134Kâ†’23K) proves value of environment filtering. 2,106 critical in production is an urgent, actionable number. This metric directly correlates to breach risk and should drive immediate action. Only loses 1 point for lacking exploitability context (are these actually exploitable?) and criticality weighting (criticals in low-value apps vs. crown-jewel apps).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Definition dependency:</strong> Requires accurate environment taggingâ€”misclassification undermines the metric.</li>
                                <li><strong>Still lacks prioritization:</strong> 2,106 critical is more manageable than 16K but still overwhelming without exploit/asset context.</li>
                                <li><strong>False sense of security:</strong> Lower numbers might create complacencyâ€”still 2K+ critical exposures in production!</li>
                                <li><strong>Comparison confusion:</strong> Users might compare "All" (134K) to "Prod" (23K) and think 111K issues disappearedâ€”just filtered.</li>
                                <li><strong>Release gate questions:</strong> Why do 2K criticals exist in prod? Are gates too weak or is there emergency override abuse?</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Vulnerability Density - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Average number of vulnerabilities per application version, normalized metric showing vulnerability burden. Downward trend (â–¼8%) indicates improving code quality.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Program effectiveness metricâ€”declining density shows tools/training/processes working. Useful for demonstrating ROI to executives.</li>
                                <li><strong>Engineers:</strong> Benchmarking toolâ€”compare app density to portfolio average. High-density apps need architectural review or refactoring.</li>
                                <li><strong>Insights gained:</strong> Code quality trends, remediation effectiveness, tool noise levels, secure coding adoption.</li>
                                <li><strong>Actions driven:</strong> Secure coding training, tool tuning, architectural improvements, celebrating improvements.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Excellent normalized metric that accounts for portfolio growth. 7.4 vulns/version provides meaningful context that raw counts lack. The â–¼8% YoY trend is valuableâ€”shows program improving despite growing app portfolio. Useful for benchmarking and identifying outlier apps. Great for demonstrating program ROI. Loses 2 points because density doesn't distinguish severity (7.4 low vulns â‰  7.4 critical) and can be manipulated by version proliferation.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Severity-blind:</strong> 7.4 low vulns is very different from 7.4 critical vulnsâ€”needs severity weighting.</li>
                                <li><strong>Gaming potential:</strong> Can be artificially improved by creating more versions (same vulns spread across more versions = lower density).</li>
                                <li><strong>Version definition issues:</strong> What counts as a version? Minor patch vs. major release? Inconsistent definitions skew density.</li>
                                <li><strong>Tool noise impact:</strong> If tools become noisier (more false positives), density increases even if true security improves.</li>
                                <li><strong>Context missing:</strong> Is 7.4 good or bad? Industry benchmarks or portfolio historical context needed.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. New Issues by Severity (12 Month Trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Monthly flow of newly discovered vulnerabilities by severity over 12 months, showing issue introduction rate patterns.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Leading indicator of future remediation work. Spikes indicate process breakdowns (e.g., new tool deployment, insecure release). Trend stability = mature processes.</li>
                                <li><strong>Engineers:</strong> Workload forecastingâ€”high new issue rates = growing backlog. Helps identify root causes (new apps, insecure frameworks, poor training).</li>
                                <li><strong>Insights gained:</strong> Issue introduction patterns, shift-left effectiveness, seasonal trends, tool deployment impacts.</li>
                                <li><strong>Actions driven:</strong> Developer training, secure coding standards, architectural reviews, tool tuning, root cause analysis.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Valuable leading indicator showing issue flow into the pipeline. Spikes or trends reveal process health. Stacked view shows severity mix over time. Useful for forecasting remediation capacity needs. However, "new" issues are ambiguous (new code vulnerabilities vs. new tool detections vs. newly scanned old code). Lacks actionability without root cause context. Better as an operational diagnostic than a business metric.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>"New" ambiguity:</strong> New code vulns vs. new tool rules vs. newly scanned legacy code? Very different root causes and actions.</li>
                                <li><strong>Tool deployment noise:</strong> New tool or rule deployment causes spike in "new" issues that aren't actually new risks.</li>
                                <li><strong>Onboarding confusion:</strong> Onboarding a legacy app with 1,000 existing issues creates a massive "new issues" spikeâ€”misleading.</li>
                                <li><strong>No remediation correlation:</strong> Needs paired with "issues closed" to show net backlog change (new - closed = backlog growth).</li>
                                <li><strong>Scale ambiguity:</strong> No Y-axis valuesâ€”is this 10 issues/month or 10,000? Relative heights insufficient for planning.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Most Prevalent Vulnerabilities (Top 5 categories) - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Top vulnerability categories with open issue counts, critical/high breakdowns, and affected version counts. Command Injection leads (342 issues, 58 critical, 28 versions).</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Strategic prioritization toolâ€”top categories = training focus, tool investment areas. Command/SQL injection dominance = input validation problem.</li>
                                <li><strong>Engineers:</strong> Tactical remediation priorities. Identifies systemic issues requiring architectural fixes vs. one-off bugs. Informs secure coding focus areas.</li>
                                <li><strong>Training Teams:</strong> Data-driven training curriculumâ€”focus on Command Injection, SQL Injection, XSS (top 3).</li>
                                <li><strong>Insights gained:</strong> Systemic weakness patterns, training gaps, architectural anti-patterns, tool effectiveness per category.</li>
                                <li><strong>Actions driven:</strong> Targeted training programs, framework adoption (ORMs for SQL injection), SAST rule tuning, architectural patterns.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional strategic KPIâ€”transforms raw vulnerability counts into actionable intelligence. Identifies systemic patterns requiring programmatic solutions, not just one-off fixes. Multi-dimensional view (total issues, critical/high, affected versions) enables sophisticated analysis. Directly informs training, tooling, and architectural decisions. Loses 1 point for lacking trend data (are these categories improving or worsening?) and remediation difficulty context.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Static snapshot:</strong> No trend dataâ€”is Command Injection improving or getting worse? Historical context missing.</li>
                                <li><strong>Category definition variance:</strong> Different tools categorize differentlyâ€”consistency issues in multi-tool environments.</li>
                                <li><strong>No remediation difficulty:</strong> RCE (96 issues) might be harder to fix than CSRF (109 issues)â€”effort not reflected.</li>
                                <li><strong>Version spread ambiguity:</strong> 342 issues across 28 versionsâ€”is that 12 issues/version or concentrated in a few apps?</li>
                                <li><strong>Potential tunnel vision:</strong> Teams might hyper-focus on top categories and neglect emerging issues in lower ranks.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Open Issues by Age & Severity Matrix - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> 2D matrix showing vulnerability backlog by age buckets (<30d, 30-60d, 60-90d, >90d) and severity. 18 critical issues >90 days old = urgent legacy debt.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Remediation velocity indicatorâ€”issues aging = slow response. 18 critical >90d = process failure requiring escalation.</li>
                                <li><strong>Engineers:</strong> Priority matrixâ€”focus on top-left (recent critical) and top-right (aging critical). Shows backlog age distribution for capacity planning.</li>
                                <li><strong>SLA Teams:</strong> Measures compliance with remediation SLAs (e.g., "fix critical in 30 days"). 18 critical >90d = SLA violations.</li>
                                <li><strong>Insights gained:</strong> Remediation velocity, SLA compliance, backlog aging patterns, resource adequacy.</li>
                                <li><strong>Actions driven:</strong> Emergency fix sprints for aging critical issues, SLA policy enforcement, resource augmentation, risk acceptance workflows.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional multi-dimensional view combining severity and timeâ€”two critical prioritization factors. Age dimension reveals velocity problems that severity alone misses. The >90d column is a "hall of shame" showing persistent risks. Directly supports SLA monitoring and capacity planning. Matrix format efficiently displays 16 data points. Loses 1 point for lacking reasons why issues age (false positives? business blockers? resource gaps?).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Age clock ambiguity:</strong> Does age start at discovery, triage, or assignment? Clock-start definition affects interpretation.</li>
                                <li><strong>No remediation context:</strong> Why are 18 critical issues >90d old? False positives? Architectural blockers? Lack of ownership?</li>
                                <li><strong>Threshold arbitrariness:</strong> Why 30/60/90 day buckets? Different orgs have different SLAsâ€”needs customization.</li>
                                <li><strong>False positive masking:</strong> Aging issues might be false positives awaiting closure, not real risksâ€”inflates urgency.</li>
                                <li><strong>No production filter:</strong> Aging critical in dev is different from aging critical in prodâ€”environment matters.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Open Source Security Risk - Value: 10/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Supply chain risk exposureâ€”38% of open source dependencies have known vulnerabilities. 22,204 vulnerable components out of 58,432 total tracked.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> Board-level supply chain risk metric post-SolarWinds/Log4Shell. 38% vulnerable = significant attack surface. Demonstrates SCA program necessity.</li>
                                <li><strong>Engineers:</strong> Dependency hygiene indicator. 38% = systematic dependency update problem. Drives dependency upgrade campaigns.</li>
                                <li><strong>Risk/Compliance:</strong> Supply chain risk quantification for regulatory requirements (SBOM, EO 14028). Essential for cyber insurance questionnaires.</li>
                                <li><strong>Insights gained:</strong> Dependency debt scale, update velocity, supply chain attack surface, dependency management maturity.</li>
                                <li><strong>Actions driven:</strong> Dependency update initiatives, automated dependency updates (Dependabot), vulnerable component blocking, SBOM generation.</li>
                            </ul>
                            <p><strong>Value Rating: 10/10</strong><br><em>Reasoning:</em> Perfect KPI for modern supply chain risk. 38% vulnerable is alarmingly high and actionableâ€”clear target to drive down. Post-Log4Shell, this metric has C-suite visibility. Normalizes across portfolio size (percentage, not raw count). Simple, understandable, and directly actionable. Supports regulatory compliance (SBOM requirements). This metric alone justifies SCA tooling investment. Critical for cyber insurance and board reporting.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Severity-blind:</strong> 38% vulnerable includes all severitiesâ€”needs breakdown (what % have critical CVEs vs. low?).</li>
                                <li><strong>Transitive dependencies:</strong> Many vulnerable components are indirect dependenciesâ€”harder to fix, outside direct control.</li>
                                <li><strong>Exploitability unknown:</strong> Known vulnerability â‰  exploitableâ€”many CVEs have no known exploit. Lacks exploit intel integration.</li>
                                <li><strong>Update feasibility:</strong> Some vulnerabilities can't be fixed (no patch available, dependency conflicts, breaking changes).</li>
                                <li><strong>Alert fatigue:</strong> 22,204 vulnerable components is overwhelmingâ€”needs intelligent prioritization (EPSS, reachability analysis).</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Open Source License Risk - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> License compliance risk across open source dependencies. 8% strong copyleft (4,674) = potential legal/IP risk. 6% unknown (3,507) = compliance blind spot.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/Legal:</strong> IP risk and compliance indicator. Strong copyleft licenses (GPL) require legal review before commercial use. Essential for M&A due diligence.</li>
                                <li><strong>Engineers:</strong> Dependency selection guidanceâ€”avoid problematic licenses. Drives license policy enforcement in CI/CD.</li>
                                <li><strong>Legal/Compliance:</strong> License obligation tracking for commercial software. Identifies attribution requirements, source disclosure obligations.</li>
                                <li><strong>Insights gained:</strong> License compliance posture, IP risk exposure, dependency selection patterns, audit readiness.</li>
                                <li><strong>Actions driven:</strong> License policy creation, problematic component replacement, legal reviews, M&A due diligence, attribution file generation.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Important legal/compliance metric often overlooked in security-focused dashboards. 8% strong copyleft could represent significant IP risk for commercial software. 6% unknown is a red flag for compliance gaps. 72% permissive is healthy baseline. Essential for regulated industries and M&A. Loses 3 points because: (1) most security engineers don't prioritize license risk, (2) lacks context on which licenses are actually prohibited by policy, (3) impact is legal/business, not security.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>License complexity:</strong> "Strong copyleft" oversimplifiesâ€”GPL v2 vs. v3 vs. AGPL have very different implications.</li>
                                <li><strong>Use case dependency:</strong> Copyleft licenses acceptable for internal tools but not customer-facing productsâ€”context missing.</li>
                                <li><strong>Unknown components:</strong> 6% unknown (3,507 components) is concerningâ€”could hide high-risk licenses. Needs investigation priority.</li>
                                <li><strong>False sense of safety:</strong> Even permissive licenses have obligations (attribution, warranty disclaimers)â€”not "risk-free."</li>
                                <li><strong>Audience mismatch:</strong> Security engineers may not understand license implicationsâ€”needs legal context or training.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--sev-critical); border-radius: 8px;">
                        <h4 style="margin-top: 0;">ðŸ“ˆ Summary Dashboard Value Rankings</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px;">
                            <div>
                                <h5 style="color: var(--sev-critical);">Highest Value (10/10) ðŸ†</h5>
                                <ol style="margin: 8px 0;">
                                    <li>Versions by Policy Compliance (10/10)</li>
                                    <li>Open Source Security Risk (10/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--accent); margin-top: 16px;">Exceptional Value (9/10) â­</h5>
                                <ol start="3" style="margin: 8px 0;">
                                    <li>Open Issues (Production Only) (9/10)</li>
                                    <li>Most Prevalent Vulnerabilities (9/10)</li>
                                    <li>Open Issues by Age & Severity (9/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--accent); margin-top: 16px;">High Value (8/10)</h5>
                                <ol start="6" style="margin: 8px 0;">
                                    <li>Versions by Star Rating (8/10)</li>
                                    <li>Vulnerability Density (8/10)</li>
                                </ol>
                            </div>
                            <div>
                                <h5 style="color: var(--text-muted);">Good Value (7/10)</h5>
                                <ol start="8" style="margin: 8px 0;">
                                    <li>Open Issues (All Environments) (7/10)</li>
                                    <li>New Issues by Severity (Monthly) (7/10)</li>
                                    <li>Open Source License Risk (7/10)</li>
                                </ol>
                            </div>
                        </div>
                        <div style="margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border);">
                            <h5>ðŸŽ¯ Key Insights</h5>
                            <ul style="margin: 8px 0;">
                                <li><strong>Policy Compliance is the north-star:</strong> Binary pass/fail aligns perfectly with release gates and executive reporting.</li>
                                <li><strong>Production filtering is critical:</strong> The 6Ã— reduction (134Kâ†’23K issues) by filtering to production-only demonstrates the value of environment-aware prioritization.</li>
                                <li><strong>Supply chain risk dominates:</strong> 38% vulnerable dependencies and 22K affected components shows modern apps are mostly third-party code.</li>
                                <li><strong>Multi-dimensional views win:</strong> AgeÃ—Severity matrix and CategoryÃ—Severity table provide richer insights than single-dimension metrics.</li>
                                <li><strong>Trend data missing:</strong> Most metrics lack historical contextâ€”adding trend arrows (YoY, MoM) would significantly increase value.</li>
                            </ul>
                        </div>
                        <p style="margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border);"><strong>Overall Assessment:</strong> The Risk Exposure Dashboard is exceptionalâ€”heavily focused on actionable risk intelligence rather than vanity metrics. Policy compliance and production filtering demonstrate maturity. Supply chain risk visibility is critical for modern threat landscape. The multi-dimensional views (ageÃ—severity, category breakdowns) enable sophisticated prioritization. Only improvement needed: add trend data to show whether risk is increasing or decreasing over time.</p>
                        </div>
                    </details>
                </div>
            </div>

            <!-- Design/Implementation Notes (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">Risk Exposure Dashboard Design/Implementation Notes</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>AI Generated Suggestions:</strong> Based on analysis of OpenAPI Specs for FoD/SSC and the Program Dashboard requirements. Consider this a starting point, not definitive. There will be errors.</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Versions by Policy Compliance - Value: 10/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Evaluate each active application version/release against defined security policies (e.g., "No Critical/High vulnerabilities in Production", "OWASP Top 10 compliance"). Categorize as Pass (meets all policies), Fail (violates one or more policies), or Unassessed (no scan data or policy not applicable). Exclude retired/archived versions. <strong>This is the single most important security KPI.</strong></li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases?offset=0&limit=50 returns ReleaseListResponse with isPassed (boolean) field indicating policy compliance status for each release.</li>
                                        <li><strong>Response Structure:</strong> Each release object includes:
                                            <ul>
                                                <li>isPassed (boolean) â€” true if release passes all assigned policies, false if any policy violated, null if not yet assessed</li>
                                                <li>rating (int32) â€” Star rating 0-5 (0=not scanned, 1=criticals present, 2=highs, 3=mediums, 4-5=low/none)</li>
                                                <li>critical (int32), high (int32), medium (int32), low (int32) â€” Issue counts by severity</li>
                                                <li>sdlcStatusType (string) â€” Development, QA, Production (use to filter production-only compliance)</li>
                                                <li>currentAnalysisStatusType (string) â€” Completed, In_Progress, Cancelled, etc. (filter for Completed to ensure assessment done)</li>
                                            </ul>
                                        </li>
                                        <li><strong>Calculation Logic:</strong>
                                            <ul>
                                                <li>Passed: isPassed = true</li>
                                                <li>Failed: isPassed = false</li>
                                                <li>Unassessed: isPassed = null OR currentAnalysisStatusType != "Completed"</li>
                                                <li>Calculate percentages: (count / totalActiveReleases) * 100</li>
                                            </ul>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (suspended:false for active releases only, sdlcStatusType:Production for production-only metrics), offset (0-based), limit (max 50), orderBy (releaseName, rating, critical), orderByDirection (ASC|DESC).</li>
                                        <li><strong>Policy Configuration:</strong> Policies are defined at the tenant level in FoD UI under Settings > Policies. Each policy has thresholds (e.g., "0 Critical, â‰¤5 High"). Policies can be assigned globally or per-application. isPassed field evaluates automatically after scan completion based on assigned policies.</li>
                                        <li><strong>Best Practices:</strong> Filter by suspended=false to exclude inactive releases. For production-only compliance, add sdlcStatusType:Production filter. Cache results for 1 hourâ€”policy status changes only after new scans. Query totalCount from response to validate pagination completeness. Group by isPassed value (true/false/null) and count occurrences.</li>
                                        <li><strong>Performance:</strong> Single API call retrieves all releases with compliance statusâ€”no need for per-release detail queries. For large portfolios (1000+ releases), use pagination with offset increments of 50. Consider date filtering (modifiedStartDate) to track only recently updated releases for trend analysis.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC does not have FoD's built-in policy engine with <code>isPassed</code> field. Must use Performance Indicators (PIs) or custom issue count thresholds.</p>
                                    
                                    <p><strong>Key Endpoints (Performance Indicator Approach):</strong></p>
                                    <ol>
                                        <li><strong>Query project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id,name,committed</code></li>
                                        <li><strong>Get performance indicators:</strong> <code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories</code></li>
                                        <li><strong>Check for policy PIs:</strong> Look for indicators like <code>FoD Security Rating</code>, <code>Critical Priority Issues</code>, or custom-defined compliance metrics</li>
                                        <li><strong>Fallback - direct issue count:</strong> <code>GET /api/v1/projectVersions/{id}/issues?q=friority:[Critical,High]&start=0&limit=1</code> (use <code>count</code> field only)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>passingVersions = 0
failingVersions = 0
unassessedVersions = 0

versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  // Try performance indicators first
  pis = GET "/api/v1/projectVersions/{version.id}/performanceIndicatorHistories"
  
  // Look for policy-related PIs (deployment-specific names)
  policyPI = pis.data.find(pi => 
    pi.name.match(/Policy|Compliance|Security Rating|Gate/i)
  )
  
  if (policyPI && policyPI.value !== null) {
    // PI found - evaluate threshold (example: value >= 4 = Pass for 0-5 scale)
    if (policyPI.value >= POLICY_THRESHOLD) {
      passingVersions++
    } else {
      failingVersions++
    }
  } else {
    // Fallback: Count critical/high issues
    issues = GET "/api/v1/projectVersions/{version.id}/issues?q=friority:[Critical,High]&start=0&limit=1"
    
    if (issues.count === 0) {
      passingVersions++
    } else if (issues.count > 0) {
      failingVersions++
    } else {
      unassessedVersions++  // No scan data
    }
  }
}

passingPct = (passingVersions / versions.count) * 100</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>SSC Performance Indicators are completely custom-configured</strong>â€”no standard "policy compliance" PI exists out of box. PI names, scales (0-5, 0-100, boolean), and thresholds vary by deployment. If no PIs configured, must fall back to raw issue counts (less sophisticated than FoD's policy engine). <code>friority</code> filter values: <code>Critical</code>, <code>High</code>, <code>Medium</code>, <code>Low</code> (case-sensitive). For production-only filtering, use custom attributes as SSC has no native SDLC field. <strong>Recommendation:</strong> Work with SSC admin to identify which PIs or custom attributes represent policy compliance for this deployment, or implement simple threshold-based logic (e.g., "0 Critical + 0 High = Pass").</p>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD "Policy Compliance Status" = SSC "Performance Indicators". FoD has built-in policy engine with clear Pass/Fail; SSC requires manual PI configuration and threshold evaluation.</li>
                                <li><strong>Platform Gaps:</strong> FoD policy engine is more mature with automated evaluation, template policies, and clear UI. SSC PIs are powerful but require significant configurationâ€”no "out of box" policies. FoD policies can block releases (gate integration); SSC PIs are primarily reporting metrics. For hybrid environments: standardize policy definitions across platforms but implement differently.</li>
                                <li><strong>Policy Definition Best Practices:</strong> Start with simple policies (e.g., "0 Critical in Prod") and evolve to sophisticated rules (e.g., "0 Critical, &lt;5 High, OWASP Top 10 free, no RCE/SQLi"). Align policy thresholds with business risk tolerance and remediation capacity. Version policies by environment (stricter for Prod than Dev). Review/update policies quarterly to reflect threat landscape.</li>
                                <li><strong>Data Quality:</strong> "Unassessed" category requires investigationâ€”are these newly onboarded apps, failed scans, or policy configuration gaps? High failure rates may indicate unrealistic policies rather than security problems. Policy compliance is binary but underlying issue counts provide context for failure severity.</li>
                                <li><strong>Recommendation:</strong> Essential metric for release gate automation. Integrate with CI/CD pipelines to block deployments on policy failure. Pair with "Top Policy Violations" detail view to show which policies fail most often. For SSC: invest in custom PI development to match FoD policy capabilities.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Versions by Star Rating - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Assign star rating (0-5â˜…) to each active application version based on highest severity vulnerability present. Typical scale: 0â˜… = Not Scanned, 1â˜… = Critical vulns present, 2â˜… = High (no Critical), 3â˜… = Medium (no Critical/High), 4â˜… = Low only, 5â˜… = No open issues. Exclude suppressed/false positive issues from rating calculation. Document rating criteria clearly in dashboard.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases?offset=0&limit=50 returns ReleaseListResponse where each release includes rating (int32) field with values 0-5.</li>
                                        <li><strong>Rating Field:</strong> rating (int32) represents star rating based on highest severity vulnerability:
                                            <ul>
                                                <li>0 = Not scanned (no completed scans, currentAnalysisStatusType != "Completed")</li>
                                                <li>1 = Critical vulnerabilities present (critical > 0)</li>
                                                <li>2 = High vulnerabilities (high > 0, critical = 0)</li>
                                                <li>3 = Medium vulnerabilities (medium > 0, high = 0, critical = 0)</li>
                                                <li>4 = Low vulnerabilities only (low > 0, medium = 0, high = 0, critical = 0)</li>
                                                <li>5 = No open issues (critical = 0, high = 0, medium = 0, low = 0)</li>
                                            </ul>
                                        </li>
                                        <li><strong>Supporting Fields:</strong> critical (int32), high (int32), medium (int32), low (int32) â€” Exact vulnerability counts per severity level. currentAnalysisStatusType (string) â€” Use to identify unscanned releases ("Completed" = scanned, other values = not scanned/in progress).</li>
                                        <li><strong>Implementation Workflow:</strong>
                                            <ol>
                                                <li>Query all active releases: GET /api/v3/releases?filters=suspended:false&offset=0&limit=50</li>
                                                <li>Group releases by rating value (0, 1, 2, 3, 4, 5)</li>
                                                <li>Count releases in each group</li>
                                                <li>Calculate distribution: (countPerRating / totalReleases) * 100</li>
                                                <li>Optionally combine 4â˜… and 5â˜… into single "healthy" category (4-5â˜…)</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (suspended:false, sdlcStatusType:Production for production-only view), orderBy (rating to sort by security quality), orderByDirection (ASC shows worst-first for remediation prioritization), offset/limit (max 50 per page).</li>
                                        <li><strong>Best Practices:</strong> Cache results for 1-6 hoursâ€”ratings update only after new scans complete. For drill-down, provide release lists per rating category (e.g., "Show all 1â˜… releases"). Add trend tracking (compare current distribution to previous month) to show portfolio improvement. Filter by sdlcStatusType=Production to focus on production risk exposure.</li>
                                        <li><strong>Data Quality Notes:</strong> rating field is calculated automatically by FoD based on severity countsâ€”no manual rating possible. Suppressed vulnerabilities are excluded from rating calculation. Rating updates immediately after scan completion. 0â˜… releases need investigation: new releases not yet scanned, scan failures, or abandoned versionsâ€”check currentAnalysisStatusType for root cause.</li>
                                        <li><strong>Performance:</strong> Single API call retrieves all releases with ratingsâ€”no per-release detail queries needed. For large portfolios (1000+ releases), paginate with offset increments of 50. Use totalCount from response to validate full dataset retrieval.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC does not have native star rating field like FoD. Must calculate ratings from issue counts.</p>
                                    
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id,name</code></li>
                                        <li><strong>Get issue counts by severity:</strong> <code>GET /api/v1/projectVersions/{id}/issues?groupingtype=friority&start=0&limit=0</code> (use <code>groupBySet</code> response for severity counts)</li>
                                        <li><strong>Alternative - direct count queries:</strong> <code>GET /api/v1/projectVersions/{id}/issues?q=friority:Critical+analysis:exploitable&start=0&limit=1</code> (use <code>count</code> field only, repeat for each severity)</li>
                                        <li><strong>Optional - check for custom rating PI:</strong> <code>GET /api/v1/projectVersions/{id}/performanceIndicatorHistories</code> (look for "FoD Security Rating" or similar)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>ratingDistribution = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  // Get issue counts by severity
  issues = GET "/api/v1/projectVersions/{version.id}/issues?groupingtype=friority"
  
  criticalCount = issues.groupBySet.find(g => g.id == "Critical")?.totalCount || 0
  highCount = issues.groupBySet.find(g => g.id == "High")?.totalCount || 0
  mediumCount = issues.groupBySet.find(g => g.id == "Medium")?.totalCount || 0
  lowCount = issues.groupBySet.find(g => g.id == "Low")?.totalCount || 0
  
  // Apply star rating algorithm
  if (criticalCount + highCount + mediumCount + lowCount == 0) {
    // Check if version has artifacts (scanned)
    artifacts = GET "/api/v1/projectVersions/{version.id}/artifacts?limit=1"
    rating = artifacts.count > 0 ? 5 : 0  // 5â˜… if scanned with no issues, 0â˜… if never scanned
  } else if (criticalCount > 0) {
    rating = 1
  } else if (highCount > 0) {
    rating = 2
  } else if (mediumCount > 0) {
    rating = 3
  } else {
    rating = 4
  }
  
  ratingDistribution[rating]++
}

// Calculate percentages
for (rating in ratingDistribution) {
  percentage = (ratingDistribution[rating] / versions.count) * 100
}</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> No native star rating fieldâ€”requires client-side calculation from issue counts (slow for 1000+ versions). <code>groupingtype=friority</code> returns aggregated counts but requires one API call per version. Filtering issues by <code>analysis:exploitable</code> recommended but field may not be populated. No equivalent to FoD's <code>currentAnalysisStatusType</code>â€”must check for artifacts to detect unscanned versions. Performance: For 500 versions, expect 3-5 minute calculation time. <strong>Recommendation:</strong> Implement as background batch job (nightly/weekly), cache results as custom attributes, and display from cache rather than calculating real-time.</p>
                                </li>
                                <li><strong>Star Rating Algorithm:</strong> Standardize calculation across platforms: Count open issues by severity (exclude Not an Issue, Suppressed). Apply rating: Has Critical? â†’ 1â˜…. Has High (no Critical)? â†’ 2â˜…. Has Medium (no High/Critical)? â†’ 3â˜…. Has Low only? â†’ 4â˜…. Has zero issues? â†’ 5â˜…. No scan data? â†’ 0â˜….</li>
                                <li><strong>Platform Gaps:</strong> FoD provides native star ratings with clear visibility in UI/API. SSC requires custom implementationâ€”no built-in star rating system. For SSC: create scheduled job to calculate star ratings and update custom attributes; expose via PI or custom dashboard. Ratings may diverge if issue suppression rules differ between platforms.</li>
                                <li><strong>Data Quality:</strong> Star ratings are snapshot in timeâ€”can fluctuate as scans run and issues are remediated/introduced. Suppression/false positive handling criticalâ€”unsuppressed FPs deflate ratings unfairly. 0â˜… (unscanned) requires investigationâ€”new apps, failed scans, or coverage gaps. Rating thresholds should align with organizational risk tolerance.</li>
                                <li><strong>Gamification Considerations:</strong> Star ratings naturally motivate teams to "level up"â€”leverage for remediation campaigns. Publish top-rated apps/teams to celebrate success. Avoid punitive use (shaming 1â˜… apps)â€”focus on improvement trajectory. Pair ratings with remediation support (training, tooling, architectural guidance).</li>
                                <li><strong>Recommendation:</strong> For SSC deployments, invest in automated star rating calculation (custom PI + scheduled job). Standardize rating criteria across FoD and SSC to enable cross-platform comparisons. Add trend indicator (rating improving/declining) to show progress. Pair with "Apps by Star Rating Over Time" chart to visualize portfolio improvement.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. Open Issues by Severity - All Environments - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count all open vulnerabilities across all application versions and environments (Dev, Test, QA, Staging, Production). Include all severities. Exclude suppressed issues and false positives (status: "Not an Issue"). This is a total backlog metricâ€”useful for understanding scale but overwhelming without prioritization. <strong>Consider hiding this metric</strong> and showing Production-only instead.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint for Issue Counts:</strong> Use GET /api/v3/releases?offset=0&limit=50 which includes aggregated vulnerability counts in each release object: critical (int32), high (int32), medium (int32), low (int32) fields. This avoids expensive per-release vulnerability queries.</li>
                                        <li><strong>Alternative - Direct Vulnerability Query:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?offset=0&limit=50 returns VulnerabilityListResponse with items array containing individual vulnerabilities. Filter by severityString (Critical|High|Medium|Low) and isSuppressed=false.</li>
                                        <li><strong>Aggregation Workflow:</strong>
                                            <ol>
                                                <li>Query all active releases: GET /api/v3/releases?filters=suspended:false&offset=0&limit=50</li>
                                                <li>Sum severity counts from release objects: totalCritical = Î£(release.critical), totalHigh = Î£(release.high), totalMedium = Î£(release.medium), totalLow = Î£(release.low)</li>
                                                <li>Calculate total: totalIssues = totalCritical + totalHigh + totalMedium + totalLow</li>
                                                <li>Calculate percentages: (severityCount / totalIssues) * 100</li>
                                            </ol>
                                        </li>
                                        <li><strong>Key Response Fields:</strong> critical, high, medium, low (int32) â€” Counts of open vulnerabilities per severity. suspended (boolean) â€” Exclude suspended=true releases. currentAnalysisStatusType (string) â€” Filter for "Completed" to ensure scan data exists.</li>
                                        <li><strong>Query Parameters:</strong> filters (suspended:false, currentAnalysisStatusType:Completed), offset (0-based), limit (max 50), orderBy (critical DESC to find worst releases), totalCount (validate pagination).</li>
                                        <li><strong>Performance:</strong> Aggregating counts from release objects is much faster than querying individual vulnerabilities. Single API call per 50 releases vs. thousands of vulnerability API calls. Cache results for 1-6 hoursâ€”issue counts change only after scans complete.</li>
                                        <li><strong>Data Quality Notes:</strong> Severity counts on release objects exclude suppressed issues automatically. Issues in "removed code" (primaryLocationRemoved=true) are excluded from counts. For exact counts including suppressed issues, must query vulnerabilities endpoint directly with isSuppressed filter.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query all project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id,name</code></li>
                                        <li><strong>Get issue counts by severity:</strong> <code>GET /api/v1/projectVersions/{id}/issues?groupingtype=friority&start=0&limit=0</code> (use <code>groupBySet</code> for aggregated counts)</li>
                                        <li><strong>Filter for open issues:</strong> Add <code>q=analysis:exploitable+removed:false</code> to exclude suppressed and deleted-code issues</li>
                                        <li><strong>Aggregate across all versions:</strong> Sum <code>totalCount</code> from each severity group across all versions</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>severityCounts = {Critical: 0, High: 0, Medium: 0, Low: 0}
versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  // Get grouped counts by friority (SSC's severity field)
  issues = GET "/api/v1/projectVersions/{version.id}/issues?groupingtype=friority&q=analysis:exploitable+removed:false"
  
  for (group in issues.groupBySet) {
    if (group.id == "Critical") severityCounts.Critical += group.totalCount
    if (group.id == "High") severityCounts.High += group.totalCount
    if (group.id == "Medium") severityCounts.Medium += group.totalCount
    if (group.id == "Low") severityCounts.Low += group.totalCount
  }
}

totalIssues = severityCounts.Critical + severityCounts.High + severityCounts.Medium + severityCounts.Low
criticalPct = (severityCounts.Critical / totalIssues) * 100</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> SSC uses <code>friority</code> (Fortify Priority combining severity + likelihood) not simple severity. <code>analysis:exploitable</code> filter excludes "Not an Issue" suppressions but field population depends on manual review. SSC has complex issue states (New, Reviewed, Remediated, Suppressed)â€”filtering is critical to get accurate "open" counts. No native environment field (Dev/QA/Prod)â€”all-environment metric includes noise from non-production scans. Performance: requires one API call per version (slow for 1000+ versions). <strong>Recommendation:</strong> Cache results and refresh hourly; consider showing Production-only view instead (requires custom attributes for environment tagging).</p>
                                </li>
                                <li><strong>Nomenclature Differences:</strong> FoD uses "Severity" (Critical/High/Medium/Low); SSC uses "Friority" (Fortify Priority combining severity + likelihood). Both map to same 4-tier scale but calculation differs. FoD "Status: Open" = SSC "Analysis: Exploitable + Not Remediated".</li>
                                <li><strong>Platform Gaps:</strong> FoD has simpler issue lifecycle (Open â†’ Fixed). SSC has complex workflow states (New â†’ Reviewed â†’ Remediated, with Suppression side paths). FoD status is clearer; SSC requires careful filtering to get "truly open" issues. Suppression handling differsâ€”FoD suppressions still count in some queries; SSC suppressions have multiple types (FP, Mitigated, Risk Accepted).</li>
                                <li><strong>Data Quality:</strong> All-environment counts include noise from dev/test scansâ€”not representative of production risk. Tool-assigned severities (especially from SAST) can over-inflate Critical/High counts without manual review. Stale issues in abandoned project versions inflate backlog. Large portfolios will have 100K+ issuesâ€”this metric causes analysis paralysis without drill-down capabilities.</li>
                                <li><strong>Performance Considerations:</strong> Querying all issues across all versions is expensiveâ€”cache results and refresh hourly rather than real-time. For portfolios &gt;500 versions, consider paginated queries or aggregation endpoints to avoid timeouts.</li>
                                <li><strong>Recommendation:</strong> <strong>De-emphasize or remove this metric</strong> from executive dashboardsâ€”too overwhelming and lacks actionability. Replace with Production-only view or add filters (environment, business criticality, age). If showing all-environment counts, pair with "Production vs. Non-Production" breakdown to provide context. Add trend line (monthly backlog over 12 months) to show if issue debt is growing or shrinking.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Open Issues by Severity - Production Only - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count open vulnerabilities for releases/versions tagged as "Production" environment only. Exclude Dev, Test, QA, Staging environments. This is the <strong>most important vulnerability metric</strong>â€”represents actual business risk exposure. Group by severity (Critical, High, Medium, Low). Exclude suppressed issues unless tracking separately.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases?filters=sdlcStatusType:Production+suspended:false&offset=0&limit=50 returns production releases with aggregated issue counts.</li>
                                        <li><strong>Production Filtering:</strong> Use sdlcStatusType field to filter releases. Values: Development, QA, Production. Filter for sdlcStatusType:Production to get production-only releases. Each release includes critical, high, medium, low (int32) counts.</li>
                                        <li><strong>Aggregation Workflow:</strong>
                                            <ol>
                                                <li>Query production releases: GET /api/v3/releases?filters=sdlcStatusType:Production+suspended:false+currentAnalysisStatusType:Completed&offset=0&limit=50</li>
                                                <li>Sum severity counts: totalCritical = Î£(release.critical), totalHigh = Î£(release.high), totalMedium = Î£(release.medium), totalLow = Î£(release.low)</li>
                                                <li>Calculate total production issues: totalProdIssues = totalCritical + totalHigh + totalMedium + totalLow</li>
                                                <li>Calculate percentages per severity: (severityCount / totalProdIssues) * 100</li>
                                            </ol>
                                        </li>
                                        <li><strong>Key Response Fields:</strong> sdlcStatusType (string enum) â€” Values: Development, QA, Production, Retired. critical/high/medium/low (int32) â€” Open issue counts per severity (excludes suppressed). suspended (boolean) â€” Always filter suspended=false. currentAnalysisStatusType (string) â€” Ensure "Completed" for valid scan data.</li>
                                        <li><strong>Query Parameters:</strong> filters (sdlcStatusType:Production, suspended:false, currentAnalysisStatusType:Completed), offset (0-based), limit (max 50), orderBy (critical DESC to prioritize worst production releases), totalCount (track pagination progress).</li>
                                        <li><strong>Environment Classification:</strong> sdlcStatusType is set during release creation in FoD UI or via API. Must be accurately maintained as releases promote through environments. Some orgs use separate releases per environment (App_DEV, App_QA, App_PROD); others update sdlcStatusType on single release as it promotes.</li>
                                        <li><strong>Best Practices:</strong> Validate sdlcStatusType accuracy quarterlyâ€”audit high-risk apps to ensure production releases correctly tagged. Cache results for 1-6 hours. For drill-down, provide list of production releases sorted by critical count DESC. Compare to "All Environments" (KPI 3) to show noise reduction (e.g., 82% fewer issues when filtering to production).</li>
                                        <li><strong>Performance:</strong> Production filter dramatically reduces dataset size (typically 10-20% of total releases are production). Aggregating from release objects avoids expensive vulnerability API calls. Single query per 50 releases with pagination.</li>
                                        <li><strong>Data Quality Notes:</strong> Accuracy depends on proper sdlcStatusType maintenance. Releases may linger in "QA" status after production deploymentâ€”establish process to update status. Emergency hotfixes sometimes skip proper environment taggingâ€”audit workflows. Production issues in suspended releases should be excluded (suspended=true likely means decommissioned).</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>âš ï¸ HIGH UNCERTAINTY:</strong> SSC has no native "production" environment field. Must use custom attributes or naming conventions.</p>
                                    
                                    <p><strong>Key Endpoints (Custom Attribute Approach):</strong></p>
                                    <ol>
                                        <li><strong>Find environment attribute definition:</strong> <code>GET /api/v1/attributeDefinitions</code> (look for attribute named "Environment", "SDLC Status", or similar)</li>
                                        <li><strong>Query versions with attributes:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id,name</code></li>
                                        <li><strong>Filter for production versions:</strong> <code>GET /api/v1/projectVersions/{id}/attributes</code> (check if attribute value == "Production" or "Prod")</li>
                                        <li><strong>Get production issue counts:</strong> <code>GET /api/v1/projectVersions/{id}/issues?groupingtype=friority&q=analysis:exploitable+removed:false</code></li>
                                    </ol>
                                    
                                    <p><strong>Fallback Approach (No Custom Attributes):</strong></p>
                                    <ol>
                                        <li><strong>Use naming convention:</strong> Filter versions with <code>q=name:*-PROD*</code> or <code>q=name:*Production*</code></li>
                                        <li><strong>Or use committed status as proxy:</strong> <code>GET /api/v1/projectVersions?committed=true&includeInactive=false</code> (assumes only production versions are committed)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>// Step 1: Identify production versions
attributeDefs = GET "/api/v1/attributeDefinitions"
envAttrId = attributeDefs.data.find(a => a.name.match(/environment|sdlc/i))?.id

if (!envAttrId) {
  console.warn("No environment attribute - falling back to committed=true")
  productionVersions = GET "/api/v1/projectVersions?committed=true&includeInactive=false"
} else {
  allVersions = GET "/api/v1/projectVersions?includeInactive=false"
  productionVersions = []
  
  for (version in allVersions.data) {
    attributes = GET "/api/v1/projectVersions/{version.id}/attributes"
    envAttr = attributes.data.find(a => a.attributeDefinitionId == envAttrId)
    
    if (envAttr && envAttr.value.match(/prod/i)) {
      productionVersions.push(version)
    }
  }
}

// Step 2: Aggregate issue counts from production versions only
severityCounts = {Critical: 0, High: 0, Medium: 0, Low: 0}

for (version in productionVersions) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?groupingtype=friority&q=analysis:exploitable+removed:false"
  
  for (group in issues.groupBySet) {
    severityCounts[group.id] += group.totalCount
  }
}</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>SSC has NO native production/environment field</strong>â€”completely dependent on custom attribute configuration (deployment-specific). Common attribute names: "Environment", "SDLC Status", "Deployment Stage", but these must be manually created and maintained. If no custom attributes exist, fallback to <code>committed=true</code> (unreliableâ€”not all production versions are committed, some non-prod versions are committed). Naming convention approach (<code>*-PROD</code>) is fragile and inconsistent across teams. <strong>CRITICAL RECOMMENDATION:</strong> For SSC deployments, this KPI may not be implementable without significant custom attribute infrastructure. Consider marking as "Not Available" or implement environment tagging as prerequisite.</p>
                                </li>
                                <li><strong>Environment Tagging Requirements:</strong> Accurate environment classification is CRITICAL for this metric. FoD: set <code>isProd=true</code> for production releases during onboarding. SSC: create mandatory "Environment" custom attribute and enforce via process. Validate tagging accuracy quarterlyâ€”spot check high-risk apps to ensure correct environment classification.</li>
                                <li><strong>Platform Gaps:</strong> FoD has clearer production designation (isProd flag + SDLC status). SSC relies on custom attributes which vary by deploymentâ€”standardization required. Some orgs have multiple "production" types (Prod, Prod-DR, Prod-Canary)â€”decide if all count as "production" for this metric.</li>
                                <li><strong>Data Quality:</strong> Environment misclassification is the biggest data quality riskâ€”dev releases tagged as "prod" inflate counts; prod releases tagged as "dev" create blind spots. Regular validation required. Emergency releases or hotfixes might bypass proper environment taggingâ€”audit workflows. For SSC: if no custom attribute exists, falls back to "all environments" (same as KPI #3)â€”clearly document this limitation.</li>
                                <li><strong>Security vs. Compliance Filtering:</strong> Some orgs further filter to "Internet-facing Production" for most critical subset. Consider adding business criticality dimension (High/Medium/Low Business Impact) for additional prioritization. Production issues in low-criticality apps may be lower priority than staging issues in high-criticality apps.</li>
                                <li><strong>Recommendation:</strong> <strong>Make this the primary vulnerability metric</strong> on executive dashboards. Hide or de-emphasize "All Environments" view. Add table drill-down showing production issues by app/version with remediation owners. Pair with "Production Issues by Age" to identify stale risks. For SSC: prioritize environment attribute standardizationâ€”this metric is meaningless without it.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Vulnerability Density - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate average: (Total Open Issues / Total Active Versions). Use production versions only or all environments depending on context. Exclude suppressed/false positive issues from numerator. Track trend over time (monthly or quarterly) to show improvement/regression.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Simple Calculation (Recommended):</strong> Use aggregated counts from releases endpoint:
                                            <ol>
                                                <li>Query releases: GET /api/v3/releases?filters=suspended:false&offset=0&limit=50</li>
                                                <li>Sum all issue counts: totalIssues = Î£(release.critical + release.high + release.medium + release.low)</li>
                                                <li>Count releases: totalReleases = response.totalCount</li>
                                                <li>Calculate density: totalIssues / totalReleases</li>
                                            </ol>
                                        </li>
                                        <li><strong>Severity-Weighted Density:</strong> Assign weights (Critical=10, High=5, Medium=2, Low=1), then calculate: Î£(criticalÃ—10 + highÃ—5 + mediumÃ—2 + lowÃ—1) / totalReleases. This provides better risk-weighted metric than simple count.</li>
                                        <li><strong>Production-Only Density:</strong> Add sdlcStatusType:Production filter to get production-specific density: GET /api/v3/releases?filters=sdlcStatusType:Production+suspended:false. Production density typically lower than all-environment density due to release gate filtering.</li>
                                        <li><strong>Response Fields:</strong> critical, high, medium, low (int32) â€” Issue counts per release. suspended (boolean) â€” Exclude suspended=true. totalCount (int32) â€” Total release count for denominator.</li>
                                        <li><strong>Query Parameters:</strong> filters (suspended:false for active releases, sdlcStatusType:Production for production-only), offset/limit (max 50), totalCount (essential for accurate denominator).</li>
                                        <li><strong>Trend Calculation:</strong> Store monthly density snapshots, compare current month to same month last year for YoY trend. Calculate: ((currentDensity - priorDensity) / priorDensity) Ã— 100 for percentage change.</li>
                                        <li><strong>Performance:</strong> Aggregating from release objects is highly efficientâ€”single API call per 50 releases. Cache daily, recalculate monthly for trend tracking. For large portfolios (1000+ releases), paginate through all releases summing counts.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Get version count (denominator):</strong> <code>GET /api/v1/projectVersions?includeInactive=false&committed=true</code> â†’ use <code>count</code> field</li>
                                        <li><strong>Get total issue count (numerator):</strong> Aggregate across all versions using <code>GET /api/v1/projectVersions/{id}/issues?q=analysis:exploitable+removed:false&start=0&limit=1</code> (use <code>count</code> field per version)</li>
                                        <li><strong>Calculate density:</strong> <code>totalIssues / totalVersions</code></li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>versions = GET "/api/v1/projectVersions?includeInactive=false&committed=true"
totalVersions = versions.count
totalIssues = 0

for (version in versions.data) {
  // Get issue count for this version (limit=1 to just get count, not data)
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=analysis:exploitable+removed:false&start=0&limit=1"
  totalIssues += issues.count
}

vulnerabilityDensity = totalIssues / totalVersions

// Optional: Severity-weighted density
weightedScore = 0
for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?groupingtype=friority&q=analysis:exploitable+removed:false"
  
  for (group in issues.groupBySet) {
    if (group.id == "Critical") weightedScore += group.totalCount * 10
    if (group.id == "High") weightedScore += group.totalCount * 5
    if (group.id == "Medium") weightedScore += group.totalCount * 2
    if (group.id == "Low") weightedScore += group.totalCount * 1
  }
}

weightedDensity = weightedScore / totalVersions</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> No portfolio-level issue aggregation endpointâ€”must iterate all versions to sum issue counts (performance-intensive). For 1000 versions, expect 5-10 minute calculation time. <code>committed=true</code> filter recommended to exclude setup/abandoned versions from denominator. <code>analysis:exploitable</code> filter depends on manual issue review; if not populated, counts include unreviewed issues. <strong>Recommendation:</strong> Calculate density as scheduled batch job (daily/weekly), store result in database or custom attribute, and display cached value rather than real-time calculation.</p>
                                </li>
                                <li><strong>Severity-Weighted Density (Advanced):</strong> Simple count treats all issues equallyâ€”consider severity-weighted density for better risk representation. Formula: ((Critical Ã— 10) + (High Ã— 5) + (Medium Ã— 2) + (Low Ã— 1)) / Total Versions. Example: (100C + 200H + 300M + 400L) / 50 versions = (1000+1000+600+400) / 50 = 60 weighted density.</li>
                                <li><strong>Platform Gaps:</strong> Both FoD and SSC support this calculation but require separate queries for numerator/denominatorâ€”no native "density" metric. Calculation logic identical across platforms. Version definition consistency criticalâ€”FoD "releases" vs SSC "project versions" should map 1:1.</li>
                                <li><strong>Data Quality:</strong> Denominator sensitivity: abandoned versions with no recent scans inflate denominator and deflate density artificially. Filter denominator to versions with scans in last 90 days for accurate metric. Numerator can be inflated by tool noiseâ€”track false positive rate separately. Density can decrease simply by onboarding many new apps with zero issues (denominator increases).</li>
                                <li><strong>Trend Tracking:</strong> Calculate density monthly and store historical values. Plot trend line on dashboard (12-month rolling average recommended). YoY comparison shows program improvement. Identify inflection points (density spikes) and correlate with events (new tool deployment, onboarding surge, training initiative).</li>
                                <li><strong>Benchmarking:</strong> Compare portfolio density to industry benchmarks (if available from tool vendors or research). Compare individual app density to portfolio average to identify outliers needing remediation focus. Segment density by language/framework to identify systemic issues (e.g., Java apps have 2Ã— density of Python apps).</li>
                                <li><strong>Recommendation:</strong> Use severity-weighted density for executive reporting (better represents risk). Include trend indicator (improving/declining) on dashboard. Add drill-down showing "Top 10 Highest Density Apps" to focus remediation. Exclude newly onboarded apps (&lt;30 days) from calculation until first full scan cycle completes.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. New Issues by Severity (12 Month Trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count vulnerabilities first detected in each calendar month over last 12 months. Group by severity and month. "New" = first introduced date within that month (not re-opened or re-scanned existing issues). Ambiguity warning: "new" can mean new code vulnerability, new scan rule detection, or first scan of legacy codeâ€”clarify definition.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?offset=0&limit=50 returns VulnerabilityListResponse with items array containing vulnerability objects.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>introducedDate (date-time, ISO 8601) â€” Timestamp when vulnerability was first detected, persists across scans</li>
                                                <li>severityString (string) â€” "Critical", "High", "Medium", "Low"</li>
                                                <li>id (string/GUID) â€” Unique vulnerability identifier</li>
                                                <li>isSuppressed (boolean) â€” Exclude suppressed=true vulnerabilities</li>
                                                <li>primaryLocationRemoved (boolean) â€” Exclude true (code location no longer exists)</li>
                                            </ul>
                                        </li>
                                        <li><strong>Monthly Aggregation Workflow:</strong>
                                            <ol>
                                                <li>Calculate date range: startDate = 12 months ago, endDate = now</li>
                                                <li>Query vulnerabilities: For each release, GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false&offset=0&limit=50</li>
                                                <li>Filter by introducedDate within last 12 months on client side</li>
                                                <li>Group by month: Parse introducedDate, extract year-month (YYYY-MM)</li>
                                                <li>Group by severity within each month: Count vulnerabilities per severityString per month</li>
                                                <li>Result: Matrix of [month][severity] = count</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (isSuppressed:false, primaryLocationRemoved:false), offset (0-based), limit (max 50), orderBy (introducedDate for chronological sorting), totalCount (validate pagination).</li>
                                        <li><strong>Date Filtering:</strong> FoD vulnerabilities API doesn't support direct introducedDate filtering in query parametersâ€”must retrieve all vulnerabilities and filter client-side by introducedDate. For performance, consider querying only recent scans (scanId from scans completed in last 12 months).</li>
                                        <li><strong>Performance Optimization:</strong> For large portfolios, query scans first: GET /api/v3/scans?completedOnStartDate={12monthsAgo}&analysisStatusType=Completed, then query vulnerabilities only for those scans. This avoids retrieving all historical vulnerabilities. Cache monthly aggregates, recalculate only current month daily.</li>
                                        <li><strong>Data Quality Notes:</strong> introducedDate represents first detection, not when code was writtenâ€”new tool rules can cause "new" spikes for old code. Onboarding legacy applications creates large introducedDate spike in onboarding month. Exclude scan setup/configuration scans (first scan of newly onboarded app) for cleaner trend line.</li>
                                    </ul>
                                </li>                                        <li>Exclude reopened issues: filter where <code>closedDate</code> is null or <code>introducedDate</code> &gt; <code>closedDate</code></li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query all project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id</code></li>
                                        <li><strong>Get issues with date filtering:</strong> <code>GET /api/v1/projectVersions/{id}/issues?q=foundDate:[{12monthsAgo} TO *]+analysis:exploitable+removed:false</code></li>
                                        <li><strong>Group by month and severity:</strong> Parse <code>foundDate</code> field client-side, extract month (YYYY-MM), aggregate by <code>friority</code></li>
                                        <li><strong>Optional - use groupingtype:</strong> <code>GET /api/v1/projectVersions/{id}/issues?groupingtype=FOLDER&q=foundDate:[{12monthsAgo} TO *]</code> (may support date grouping)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>monthlyData = {} // {"2024-01": {Critical: 0, High: 0, Medium: 0, Low: 0}, ...}
twelveMonthsAgo = new Date().subtract(12, 'months').toISOString()

versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  // Get issues found in last 12 months
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=foundDate:[{twelveMonthsAgo} TO *]+analysis:exploitable+removed:false"
  
  for (issue in issues.data) {
    // Extract month from foundDate (e.g., "2024-03-15T10:30:00.000Z" -> "2024-03")
    month = issue.foundDate.substring(0, 7)
    
    // Initialize month if not exists
    if (!monthlyData[month]) {
      monthlyData[month] = {Critical: 0, High: 0, Medium: 0, Low: 0}
    }
    
    // Increment severity count for this month
    severity = issue.friority // "Critical", "High", "Medium", or "Low"
    monthlyData[month][severity]++
  }
}

// Render grouped bar chart from monthlyData
for (month in monthlyData) {
  renderMonthlyBar(month, monthlyData[month])
}</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> SSC <code>foundDate</code> represents first detection (persists across scans via issue correlation). No native monthly groupingâ€”must parse <code>foundDate</code> and aggregate client-side. <code>analysis:exploitable</code> filter depends on manual review. Querying all issues across all versions with date filtering can be slow (5-10 minutes for 1000+ versions). "New" definition ambiguity: new code vulnerabilities vs. new scan rule detections vs. first scan of legacy codeâ€”all have same <code>foundDate</code>. <strong>Recommendation:</strong> Implement as scheduled batch job (daily), cache monthly aggregates, and display from cache. Filter out first scan of newly onboarded apps to avoid misleading spikes.</p>
                                </li>
                                <li><strong>"New Issue" Definition Challenges:</strong> Both platforms: "New" is ambiguous. Scenarios: (1) New vulnerability in new code = truly new risk, (2) New scan rule detects old code pattern = technically new finding but old code, (3) First scan of legacy app = all issues "new" but code is old. Recommend clarifying: "Newly Detected Issues" is more accurate than "New Issues."</li>
                                <li><strong>Platform Gaps:</strong> FoD and SSC both track introduction dates accurately via issue correlation (same vuln across scans keeps original introducedDate). Tool rule changes can spike "new" countsâ€”both platforms lack "reason for new" metadata (new code vs new rule). App onboarding creates massive spikesâ€”filter out first scan of new apps for accurate trend.</li>
                                <li><strong>Data Quality:</strong> Onboarding legacy apps creates misleading spikesâ€”filter out first scans or annotate chart with onboarding events. Scan tool updates (new rules/signatures) create artificial spikesâ€”document tool changes on timeline. Seasonal patterns expected (holiday dips, quarter-end pushes). Pair with "Issues Closed" metric to calculate net backlog change.</li>
                                <li><strong>Grouped Bar Chart Implementation:</strong> Display monthly data with grouped bars by severity (Critical, High, Medium, Low). Y-axis: 0-400 issue count. X-axis: Month labels. Color-code by severity. Tooltips show exact counts. Consider adding trendline overlay showing total new issues per month.</li>
                                <li><strong>Recommendation:</strong> Pair with "Remediated Issues by Month" to show net backlog change (new - closed = Î”backlog). Add annotations for significant events (app onboarding, tool updates, training initiatives). Filter out first scans of newly onboarded apps to avoid misleading spikes. Consider splitting into "New Issues in Existing Apps" vs "New Issues in New Apps" for clearer insights.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Most Prevalent Vulnerabilities (Top 5 categories) - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Aggregate open issues by vulnerability category/type (e.g., SQL Injection, XSS, Command Injection). Show top 10 categories by total issue count. Include columns: Total Issues, Critical Count, High Count, Affected Versions. Optionally filter to production-only or all environments. Excellent for identifying systemic weaknesses requiring programmatic fixes.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?offset=0&limit=50 returns vulnerability objects with category field.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>category (string) â€” Vulnerability category name (e.g., "SQL Injection", "Cross-Site Scripting", "Command Injection")</li>
                                                <li>severityString (string) â€” "Critical", "High", "Medium", "Low"</li>
                                                <li>releaseId (int32) â€” Release containing the vulnerability (for affected version counting)</li>
                                                <li>isSuppressed (boolean) â€” Exclude suppressed=true</li>
                                            </ul>
                                        </li>
                                        <li><strong>Aggregation Workflow:</strong>
                                            <ol>
                                                <li>Query vulnerabilities across all releases (or production-only if filtered)</li>
                                                <li>Filter: isSuppressed=false, primaryLocationRemoved=false</li>
                                                <li>Group by category field, count total vulnerabilities per category</li>
                                                <li>Within each category group: Count by severityString (critical/high/medium/low), Count distinct releaseId values (affected versions)</li>
                                                <li>Sort categories by total count descending, take top 10</li>
                                                <li>Result: [category: {total, critical, high, medium, low, affectedVersions}]</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (isSuppressed:false, primaryLocationRemoved:false, severityString for severity filtering), offset/limit (max 50), orderBy (category for grouping), totalCount.</li>
                                        <li><strong>Performance:</strong> Category aggregation requires retrieving all vulnerabilitiesâ€”expensive for large portfolios. Cache results hourly, recalculate when new scans complete. For 10,000+ vulnerabilities, consider server-side aggregation or pre-computed reports. Paginate through all releases and accumulate category counts client-side.</li>
                                        <li><strong>Category Taxonomy:</strong> FoD uses Fortify Taxonomy with 400+ categories aligned to CWE. Common categories: "SQL Injection", "Cross-Site Scripting (XSS)", "Command Injection", "Path Manipulation", "Insecure Randomness", "Hardcoded Password". Categories are hierarchical but API returns leaf category names.</li>
                                        <li><strong>Best Practices:</strong> Filter to production releases for business-relevant categories. Calculate percentage: (categoryCount / totalIssues) Ã— 100 to show category concentration. Add trend tracking (compare current top 10 to previous month). Map to OWASP Top 10 for executive reporting (group granular categories into OWASP classes).</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query all project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id</code></li>
                                        <li><strong>Get issues with category grouping:</strong> <code>GET /api/v1/projectVersions/{id}/issues?groupingtype=FOLDER&q=analysis:exploitable+removed:false</code></li>
                                        <li><strong>Parse issueName or category:</strong> SSC uses <code>issueName</code> field (e.g., "SQL Injection", "Cross-Site Scripting") for categorization</li>
                                        <li><strong>Aggregate across all versions:</strong> Group by <code>issueName</code>, count total issues, count by <code>friority</code>, count distinct <code>projectVersionId</code></li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>categoryStats = {} // {categoryName: {total: 0, critical: 0, high: 0, affectedVersions: Set()}}
versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=analysis:exploitable+removed:false"
  
  for (issue in issues.data) {
    category = issue.issueName // e.g., "SQL Injection"
    
    // Initialize category if not exists
    if (!categoryStats[category]) {
      categoryStats[category] = {
        total: 0,
        critical: 0,
        high: 0,
        medium: 0,
        low: 0,
        affectedVersions: new Set()
      }
    }
    
    // Increment counts
    categoryStats[category].total++
    categoryStats[category][issue.friority.toLowerCase()]++
    categoryStats[category].affectedVersions.add(version.id)
  }
}

// Sort by total count, take top 10
topCategories = Object.entries(categoryStats)
  .sort((a, b) => b[1].total - a[1].total)
  .slice(0, 10)
  .map(([name, stats]) => ({
    name,
    total: stats.total,
    critical: stats.critical,
    high: stats.high,
    affectedVersions: stats.affectedVersions.size
  }))</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> SSC uses <code>issueName</code> field which may have inconsistent values across scan types (SAST vs DAST). Custom scan rules can create non-standard category names. No native category aggregation endpointâ€”must iterate all versions and issues (very slow for 1000+ versions). <code>groupingtype=FOLDER</code> provides hierarchical grouping but structure varies by SSC version. <strong>Recommendation:</strong> Implement as scheduled batch job (daily), cache category statistics, and display from cache. For production-only metrics, filter versions by custom environment attribute first.</p>
                                </li>
                                <li><strong>Category Taxonomy Differences:</strong> FoD and SSC both use Fortify Taxonomy (derived from CWE) but presentation differs. FoD API returns structured category hierarchy; SSC uses flat issueName strings. Mapping required if integrating data from both platforms. Third-party tools (Snyk, Veracode) have different taxonomiesâ€”standardize to CWE or OWASP Top 10 for cross-tool reporting.</li>
                                <li><strong>Platform Gaps:</strong> FoD category grouping is straightforward via API. SSC requires parsing issueName or custom categorization logic. SSC custom rules/checks may create non-standard category names requiring manual mapping. Both platforms: category granularity varies (SQL Injection vs. SQL Injection: Blind vs. SQL Injection: Stored Procedure)â€”decide on aggregation level.</li>
                                <li><strong>Data Quality:</strong> Category consistency depends on tool configurationâ€”custom rules can create outlier categories. High prevalence may indicate systemic coding patterns (input validation issues) or tool bias (some tools over-detect certain categories). "Affected Versions" metric can be misleading if same issue appears in multiple scans of same version. Compare category prevalence to industry benchmarks (OWASP Top 10 distribution).</li>
                                <li><strong>Actionable Insights:</strong> Top categories = training focus areas. High-prevalence injection issues (SQL, Command, LDAP) suggest need for parameterized queries/ORM adoption. XSS prevalence = output encoding framework needed. Category concentration (top 3 = 60% of issues) enables focused remediation vs. scattered issues requiring broad approach.</li>
                                <li><strong>Recommendation:</strong> Pair with "Category Trend Over Time" to show if top categories are improving or worsening. Add "Remediation Difficulty" column if available (time to fix). Map categories to OWASP Top 10 for executive reporting (everyone knows OWASP). Use this data to drive targeted training (e.g., "SQL Injection Remediation Workshop" for top category). Consider adding "Category by Language" drill-down (Java SQL injection vs. Python SQL injectionâ€”different root causes).</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Open Issues by Age & Severity Matrix - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Create matrix with age buckets (rows: <30d, 30-60d, 60-90d, >90d) and severity (columns: Critical, High, Medium, Low). Age = days since <code>introducedDate</code> or <code>foundDate</code> (not last scan date). Count open issues in each cell. Optionally filter to production-only. Essential for SLA monitoring and remediation velocity tracking.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false&offset=0&limit=50 returns vulnerability objects with introducedDate field.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>introducedDate (date-time) â€” ISO 8601 timestamp of first detection (e.g., "2024-01-15T14:23:11.000Z")</li>
                                                <li>severityString (string) â€” "Critical", "High", "Medium", "Low"</li>
                                                <li>id (string GUID) â€” Unique vulnerability identifier</li>
                                                <li>isSuppressed (boolean) â€” Exclude suppressed=true</li>
                                                <li>primaryLocationRemoved (boolean) â€” Exclude removed=true</li>
                                            </ul>
                                        </li>
                                        <li><strong>Age Bucket Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query all vulnerabilities across releases (or production-only if filtered)</li>
                                                <li>For each vulnerability: Calculate age = (currentDate - Date.parse(introducedDate)) / 86400000 milliseconds â†’ days</li>
                                                <li>Assign to age bucket: age < 30 â†’ "&lt;30d", 30 â‰¤ age < 60 â†’ "30-60d", 60 â‰¤ age < 90 â†’ "60-90d", age â‰¥ 90 â†’ "&gt;90d"</li>
                                                <li>Group by [ageBucket][severityString], count vulnerabilities per cell</li>
                                                <li>Result: 4Ã—4 matrix with counts in each [age][severity] cell</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (isSuppressed:false+primaryLocationRemoved:false, sdlcStatusType:Production for production-only), offset/limit (max 50), totalCount. Note: introducedDate filtering not supportedâ€”must retrieve all and filter client-side by date math.</li>
                                        <li><strong>Performance:</strong> Age matrix requires retrieving all open vulnerabilitiesâ€”expensive for large portfolios. Cache matrix results hourly, recalculate when new scans complete. For 10,000+ vulnerabilities, consider paginating in batches of 1,000 and accumulating counts incrementally. Pre-compute daily to avoid real-time calculation delays.</li>
                                        <li><strong>SLA Customization:</strong> Default buckets (<30d, 30-60d, 60-90d, >90d) align with common SLA thresholds: Critical=30d, High=60d, Medium=90d. Customize buckets to match organizational SLA policies. Example custom buckets: <7d, 7-14d, 14-30d, 30-60d, >60d for tighter SLAs. Color-code matrix cells: Green (within SLA), Yellow (approaching deadline), Red (SLA violation).</li>
                                        <li><strong>Date Accuracy:</strong> introducedDate persists across scans via vulnerability correlationâ€”accurate age tracking. Rescanning does NOT reset age counter. introducedDate = first detection timestamp (not code write date, not triage date). For reopened vulnerabilities, introducedDate remains original discovery date, not reopen dateâ€”tracks total lifetime age.</li>
                                        <li><strong>Best Practices:</strong> Filter to production releases (sdlcStatusType:Production) for business-critical age trackingâ€”aging dev issues less urgent. Add drill-down showing specific aging critical vulnerabilities with owners and remediation plans. Track >90d column trend (growing = velocity problem, shrinking = remediation progress). Pair with "Average Age by Severity" metric for single-number summary. Implement automated alerts for SLA violations (e.g., Critical vulnerability entering 30-60d bucket).</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><strong>Query all project versions:</strong> <code>GET /api/v1/projectVersions?includeInactive=false&fields=id</code></li>
                                        <li><strong>Get issues with foundDate:</strong> <code>GET /api/v1/projectVersions/{id}/issues?q=analysis:exploitable+removed:false&fields=id,foundDate,friority</code></li>
                                        <li><strong>Calculate age client-side:</strong> <code>ageInDays = (Date.now() - Date.parse(issue.foundDate)) / (1000 * 60 * 60 * 24)</code></li>
                                        <li><strong>Assign to age buckets:</strong> <30d, 30-60d, 60-90d, >90d based on calculated age</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre>// Initialize 4x4 matrix (4 age buckets Ã— 4 severity levels)
ageMatrix = {
  "<30d": {Critical: 0, High: 0, Medium: 0, Low: 0},
  "30-60d": {Critical: 0, High: 0, Medium: 0, Low: 0},
  "60-90d": {Critical: 0, High: 0, Medium: 0, Low: 0},
  ">90d": {Critical: 0, High: 0, Medium: 0, Low: 0}
}

currentDate = Date.now()
versions = GET "/api/v1/projectVersions?includeInactive=false"

for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=analysis:exploitable+removed:false"
  
  for (issue in issues.data) {
    // Calculate age in days
    foundDate = Date.parse(issue.foundDate)
    ageInDays = (currentDate - foundDate) / (1000 * 60 * 60 * 24)
    
    // Determine age bucket
    if (ageInDays < 30) {
      bucket = "<30d"
    } else if (ageInDays < 60) {
      bucket = "30-60d"
    } else if (ageInDays < 90) {
      bucket = "60-90d"
    } else {
      bucket = ">90d"
    }
    
    // Increment matrix cell
    severity = issue.friority // "Critical", "High", "Medium", "Low"
    ageMatrix[bucket][severity]++
  }
}

// Render matrix table from ageMatrix data</pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> SSC <code>foundDate</code> persists across scans via issue correlation (accurate age tracking). Must calculate age client-sideâ€”no native age bucketing. <code>analysis:exploitable</code> filter depends on manual review. Querying all issues across all versions is slow (5-10 minutes for 1000+ versions). For production-only matrix, must filter versions by custom environment attribute first. <strong>Recommendation:</strong> Implement as scheduled batch job (daily), cache matrix results, and display from cache. Add SLA threshold visualization (color-code cells: green=within SLA, red=violation).</p>
                                </li>
                                <li><strong>Age Calculation Consistency:</strong> <strong>CRITICAL:</strong> Use <code>introducedDate</code>/<code>foundDate</code> NOT last scan date. Rescanning should not reset age counter. Both FoD and SSC track introduction dates via issue correlation (same vuln in new scan maintains original date). Age clock starts at first detection, not at triage/assignment (unless implementing custom workflow states).</li>
                                <li><strong>SLA Alignment:</strong> Age buckets should align with organizational SLA policies. Common SLAs: Critical = 30 days, High = 60 days, Medium = 90 days, Low = 180 days. Customize buckets to match SLA thresholds for compliance reporting. Color-code matrix cells: green (within SLA), yellow (approaching SLA), red (SLA violation).</li>
                                <li><strong>Platform Gaps:</strong> Both platforms track issue age accurately. FoD has cleaner API response structure. SSC requires date arithmetic on foundDate. Neither platform tracks "time in specific workflow states" nativelyâ€”custom implementation needed for detailed SLA tracking (time to triage, time to assign, time to fix).</li>
                                <li><strong>Data Quality:</strong> Aging issues may include false positives awaiting closureâ€”track false positive rate by age bucket. Production environment filtering criticalâ€”aging dev issues less urgent than aging prod issues. Issue reopening can complicate age calculation (use original foundDate, not reopen date). Suppressed issues should be excluded from age tracking unless specifically monitoring "suppression age."</li>
                                <li><strong>Actionable Insights:</strong> >90d Critical issues = immediate remediation focus ("war room" scenarios). High concentration in >90d column = remediation velocity problem (resource constraints, architectural blockers, unclear ownership). Even distribution across age buckets = consistent discovery and remediation. Skew toward <30d = healthy velocity OR recent tool deployment/app onboarding.</li>
                                <li><strong>Recommendation:</strong> Add drill-down showing specific aging critical issues with owners and remediation plans. Pair with "Average Age by Severity" metric (single number per severity tier). Implement automated alerts for SLA violations. Consider adding "Production-Only" toggle to filter matrix. Track trend over timeâ€”is >90d column growing or shrinking?</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Open Source Security Risk - Value: 10/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate percentage: (Components with Known CVEs / Total Components) Ã— 100. Component = unique library/dependency (groupId:artifactId:version). Include direct and transitive dependencies. "Known CVE" = any CVE in NVD/vendor databases regardless of severity (optionally filter to Critical/High only for executive view). Essential supply chain risk metric.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/applications/{applicationId}/open-source-components?offset=0&limit=50 returns component objects with vulnerability data.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>componentName (string) â€” Library name (e.g., "log4j-core")</li>
                                                <li>componentVersion (string) â€” Version identifier (e.g., "2.14.1")</li>
                                                <li>componentHash (string) â€” Unique identifier for deduplication</li>
                                                <li>vulnerabilityCounts (object) â€” Severity breakdown: {critical: int, high: int, medium: int, low: int}</li>
                                                <li>totalVulnerabilities (int32) â€” Sum of all vulnerability counts</li>
                                                <li>licenses (array) â€” SPDX license identifiers (used for KPI 10)</li>
                                            </ul>
                                        </li>
                                        <li><strong>Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query components across all applications in portfolio</li>
                                                <li>Deduplicate by componentHash (same component in multiple apps counts once globally)</li>
                                                <li>Count total unique components after deduplication</li>
                                                <li>Count components where totalVulnerabilities > 0 OR (vulnerabilityCounts.critical + high + medium + low) > 0</li>
                                                <li>Calculate percentage: (vulnerableComponents / totalComponents) Ã— 100</li>
                                                <li>Optional severity breakdown: Count components with critical > 0, high > 0, etc.</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> offset/limit (max 50), applicationId (query per app, aggregate across portfolio), filters (for severity filtering). Response includes totalCount for pagination validation.</li>
                                        <li><strong>Deduplication Strategy:</strong> Global deduplication (componentHash) recommended for executive reporting ("38% of unique components vulnerable"). Alternative per-app aggregation ("22,204 vulnerable component instances") useful for remediation planning. Store both metricsâ€”percentage uses global dedupe, raw count uses per-app instances.</li>
                                        <li><strong>CVE Enrichment:</strong> FoD provides rich vulnerability metadata: CVSS v3 scores (baseScore, temporalScore), EPSS probability (exploit prediction), exploitMaturity (Unproven, Proof of Concept, Functional, High), fixAvailable (boolean, patchable vs. workaround-only). Use EPSS and exploitMaturity for advanced prioritization beyond raw CVE counts.</li>
                                        <li><strong>Performance:</strong> Components API is lightweightâ€”50 components per call. For portfolios with 100+ apps and 50K+ components, expect 1,000+ API calls for full dataset. Cache component list daily, refresh vulnerability counts hourly (CVE database updates frequently). Parallelize app queries for faster aggregation. Pre-compute global dedupe hash table to avoid NÂ² deduplication cost.</li>
                                        <li><strong>Data Quality:</strong> Transitive dependencies inflate countsâ€”consider filtering to direct dependencies only for cleaner metric (directDependency field if available). Version detection accuracy criticalâ€”version range vulnerabilities may misidentify affected versions. Components with no fix available tracked via fixAvailable=falseâ€”report separately ("% Unfixable Vulnerabilities"). False positives rare in SCA (CVE matching is deterministic), but library aliasing (different package names, same code) can create duplicates.</li>
                                        <li><strong>Advanced Prioritization:</strong> Calculate "% Components with Critical CVEs" (subset of 38%, higher urgency). Use EPSS score to show "% Components with Exploited CVEs" (EPSS > 0.5 threshold). Filter to reachable vulnerabilities if code path analysis available (many CVEs in unused code). Track "% Components Outdated by >2 Years" (compare componentVersion to latest release date) to measure dependency debt.</li>
                                        <li><strong>Best Practices:</strong> Target <10% vulnerable components for mature programs. Pair with "Average Time to Patch" metric for remediation velocity. Implement automated dependency updates (Dependabot, Renovate Bot) to continuously reduce percentage. For SBOM compliance (EO 14028), export component list in SPDX/CycloneDX format using FoD export APIs. Track trend over timeâ€”percentage should decrease as updates are applied, may spike when onboarding legacy apps.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/project?fields=id,name â†’ enumerate all projects in portfolio</li>
                                        <li>GET /api/v1/projects/{projectId}/versions?fields=id,name â†’ get all versions per project</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/dependencies â†’ retrieve component list with vulnerability data</li>
                                        <li>For each dependency object: Extract <code>library</code> (component name), <code>version</code>, <code>vulnerabilities</code> array (CVE list)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize aggregation variables</span>
totalComponents = 0
vulnerableComponents = <span style="color: #569cd6;">new</span> Set() <span style="color: #6a9955;">// Use Set for deduplication by library:version</span>
allComponents = <span style="color: #569cd6;">new</span> Set()

<span style="color: #6a9955;">// Step 2: Iterate all project versions in portfolio</span>
projects = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/project?fields=id,name'</span>)
<span style="color: #c586c0;">for</span> each project <span style="color: #c586c0;">in</span> projects:
    versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projects/${project.id}/versions?fields=id,name`</span>)
    
    <span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions:
        <span style="color: #6a9955;">// Step 3: Fetch dependencies for this version</span>
        dependencies = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/dependencies`</span>)
        
        <span style="color: #c586c0;">for</span> each dep <span style="color: #c586c0;">in</span> dependencies:
            componentKey = <span style="color: #ce9178;">`${dep.library}:${dep.version}`</span>
            allComponents.<span style="color: #dcdcaa;">add</span>(componentKey)
            
            <span style="color: #6a9955;">// Step 4: Check for vulnerabilities</span>
            <span style="color: #c586c0;">if</span> (dep.vulnerabilities && dep.vulnerabilities.length > 0):
                vulnerableComponents.<span style="color: #dcdcaa;">add</span>(componentKey)

<span style="color: #6a9955;">// Step 5: Calculate percentage</span>
totalComponents = allComponents.size
vulnerableCount = vulnerableComponents.size
percentageVulnerable = (vulnerableCount / totalComponents) * 100

<span style="color: #6a9955;">// Display: "38% of Components Vulnerable (22,241 of 58,530)"</span>
<span style="color: #dcdcaa;">displayMetric</span>(percentageVulnerable, vulnerableCount, totalComponents)
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> SSC dependency data availability is <strong>CRITICAL DEPENDENCY</strong> on active SCA tool integrationâ€”native Fortify SCA provides dependency/CVE data, but third-party SCA imports (Sonatype, Snyk) may not populate the <code>dependencies</code> endpoint. Verify integration status first via GET /api/v1/projectVersions/{id}/dependencies (empty response = no SCA data). Alternative fallback: Query SSC issues with <code>q=scanType:SCA</code> and extract component info from issue details, though this is less reliable and lacks deduplication. Component deduplication by library:version string is essentialâ€”same library in multiple apps should count once for portfolio-level percentage. Query performance scales poorly (5-10 minutes for 1000+ versions)â€”recommend daily batch job with cached results. Transitive dependencies may inflate counts; consider filtering to direct dependencies only if SCA tool provides that distinction.</p>
                                </li>
                                <li><strong>Component Deduplication:</strong> Same component in multiple apps: count once globally (portfolio-level metric) or count per instance (app-level aggregation). Global dedupe recommended for executive reporting (38% of unique components are vulnerable). Per-app count useful for remediation planning (22K vulnerable instances across portfolio).</li>
                                <li><strong>Platform Gaps:</strong> FoD has superior SCA integration with automatic CVE enrichment (EPSS, exploit maturity). SSC SCA data quality varies by integration methodâ€”native Fortify SCA provides good data; imported third-party SCA results may lack CVE details. FoD updates CVE data automatically; SSC requires CVE database sync maintenance.</li>
                                <li><strong>Data Quality:</strong> Transitive dependencies can inflate countsâ€”consider showing "Direct Dependencies with CVEs" separately. CVE severity distribution mattersâ€”38% vulnerable with all Low severity is very different from 38% with Critical. False positives rare in SCA (CVE matching is objective) but version detection can be inaccurate (version range vulnerabilities). Components with no fix available should be tracked separately (cannot be remediated, only accepted or replaced).</li>
                                <li><strong>Advanced Prioritization:</strong> Enhance with EPSS (Exploit Prediction Scoring System) to show "% Components with Actively Exploited CVEs" (much lower number, higher urgency). Add reachability analysis (% Vulnerable Components with Reachable Code Paths)â€”many CVEs in unused code paths pose minimal risk. Show "% Components Outdated by >2 Years" to highlight dependency debt.</li>
                                <li><strong>Regulatory/Compliance Context:</strong> SBOM requirements (EO 14028) make component tracking mandatory for government contracts. Cyber insurance questionnaires ask about dependency vulnerability rates. NIST SSDF framework requires supply chain risk management. This metric demonstrates due diligence.</li>
                                <li><strong>Recommendation:</strong> Pair with "% Components with Critical CVEs" (subset of 38%) for severity context. Add "Avg Time to Patch Components" metric for remediation velocity. Implement automated dependency updates (Dependabot, Renovate) to drive percentage down. For SSC: ensure SCA integration is configured and CVE database is current. Target: <10% vulnerable components for mature programs.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Open Source License Risk - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Classify all open source components by license type. Categories: Strong Copyleft (GPL, AGPL), Weak Copyleft (LGPL, MPL), Permissive (MIT, Apache, BSD), Proprietary, Unknown. Show distribution as percentages and counts. Prioritize investigating "Unknown" licensesâ€”potential compliance risk. This is a legal/compliance metric, not a security metric.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/applications/{applicationId}/open-source-components?offset=0&limit=50 returns component objects with licenses array (same endpoint as KPI 9).</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>licenses (array of strings) â€” SPDX license identifiers (e.g., ["MIT"], ["Apache-2.0"], ["GPL-3.0"])</li>
                                                <li>licenseRisk (string) â€” FoD risk classification: "High" (strong copyleft), "Medium" (weak copyleft), "Low" (permissive), "Unknown"</li>
                                                <li>componentName (string) â€” Library name</li>
                                                <li>componentVersion (string) â€” Version identifier</li>
                                                <li>componentHash (string) â€” Unique identifier for deduplication</li>
                                            </ul>
                                        </li>
                                        <li><strong>Classification Workflow:</strong>
                                            <ol>
                                                <li>Query components across all applications in portfolio</li>
                                                <li>Deduplicate by componentHash (same component in multiple apps counts once globally)</li>
                                                <li>For each unique component: Parse licenses array, classify by SPDX identifier</li>
                                                <li>License categories: Strong Copyleft (GPL-2.0, GPL-3.0, AGPL-3.0), Weak Copyleft (LGPL-2.1, LGPL-3.0, MPL-2.0), Permissive (MIT, Apache-2.0, BSD-2-Clause, BSD-3-Clause), Proprietary (commercial licenses), Unknown (empty array or unrecognized identifiers)</li>
                                                <li>Handle multi-licensed components: If licenses array has multiple values (e.g., ["MIT", "GPL-2.0"]), classify by most permissive license for dual-license scenario OR flag as "requires legal review"</li>
                                                <li>Count components per category, calculate percentages: (categoryCount / totalComponents) Ã— 100</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> offset/limit (max 50), applicationId (query per app, aggregate across portfolio), filters if available. Response includes totalCount for pagination.</li>
                                        <li><strong>SPDX License Reference:</strong> FoD uses SPDX standardized identifiers. Common examples: Strong Copyleft (GPL-2.0-only, GPL-3.0-only, AGPL-3.0-only â€” viral licenses requiring source disclosure), Weak Copyleft (LGPL-2.1-only, LGPL-3.0-only, MPL-2.0 â€” library-level copyleft), Permissive (MIT, Apache-2.0, BSD-3-Clause, ISC â€” minimal restrictions). Full SPDX list: https://spdx.org/licenses/</li>
                                        <li><strong>Risk Classification Logic:</strong> Use FoD's licenseRisk field if available (pre-classified as High/Medium/Low/Unknown). Alternative: Implement custom classification based on licenses array SPDX identifiers. Strong Copyleft = licenseRisk:High OR licenses contains GPL/AGPL. Weak Copyleft = licenseRisk:Medium OR licenses contains LGPL/MPL. Permissive = licenseRisk:Low OR licenses contains MIT/Apache/BSD. Unknown = licenseRisk:Unknown OR licenses array empty.</li>
                                        <li><strong>Multi-Licensed Components:</strong> Components with multiple licenses (e.g., ["MIT", "GPL-2.0"]) allow user to choose license terms. Convention: Report least restrictive license (MIT in this example) since users can opt for permissive terms. Alternative: Flag as "requires legal review" if one license is copyleft and another is permissiveâ€”indicates potential ambiguity. Dual-licensed components common in OSS to balance commercial use and community contribution.</li>
                                        <li><strong>Performance:</strong> License data retrieved via same components API as KPI 9â€”single query provides both vulnerability and license info. Cache component data daily (licenses rarely change), recalculate classification monthly. Deduplication hash table built once, reused for both KPIs. Expect 1,000+ API calls for large portfolios (100+ apps, 50K+ components)â€”parallelize app queries.</li>
                                        <li><strong>Data Quality:</strong> Unknown licenses (6% in example) require investigation: Could be custom proprietary licenses not in SPDX database, components with missing/incorrect manifest files (package.json, pom.xml), internal libraries without formal license declarations. License detection accuracy depends on manifest file qualityâ€”incorrect metadata = wrong license classification. Some libraries incorrectly declare licenses (e.g., MIT in README but GPL in source headers)â€”FoD uses manifest files, not source scanning.</li>
                                        <li><strong>Policy Enforcement:</strong> Define organizational license policy based on product type: Commercial SaaS = prohibit AGPL (network copyleft), allow MIT/Apache. Commercial on-prem = review all GPL (source disclosure risk), allow LGPL/MIT. Internal tools = all licenses acceptable. Open source projects = copyleft compatible. Implement policy as filters: Count components violating policy (licenseRisk:High for commercial software) â†’ actionable "License Violations" metric. Enforce via CI/CD gates (block builds with prohibited licenses).</li>
                                        <li><strong>Best Practices:</strong> Investigate Unknown (6%) immediatelyâ€”generate CSV with componentName, componentVersion, licenses array, source repository for legal review. Partner with legal team to define risk thresholds and prohibited licenses. Track "License Violations" count (components violating policy) as actionable metric alongside distribution percentages. For M&A due diligence, export SBOM with license attribution in SPDX/CycloneDX format. Target metrics: <1% Unknown (clean metadata), <5% Strong Copyleft for commercial software, 0 Policy Violations. Add trend tracking to show license compliance improving over time.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/project?fields=id,name â†’ enumerate all projects in portfolio</li>
                                        <li>GET /api/v1/projects/{projectId}/versions?fields=id,name â†’ get all versions per project</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/dependencies â†’ retrieve component list with license data</li>
                                        <li>For each dependency object: Extract <code>library</code>, <code>version</code>, <code>licenses</code> array (SPDX identifiers or license names)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize license category counters and deduplication</span>
allComponents = <span style="color: #569cd6;">new</span> Set()
licenseCategories = {
    strongCopyleft: <span style="color: #569cd6;">new</span> Set(),  <span style="color: #6a9955;">// GPL-2.0, GPL-3.0, AGPL-3.0</span>
    weakCopyleft: <span style="color: #569cd6;">new</span> Set(),    <span style="color: #6a9955;">// LGPL-2.1, LGPL-3.0, MPL-2.0</span>
    permissive: <span style="color: #569cd6;">new</span> Set(),      <span style="color: #6a9955;">// MIT, Apache-2.0, BSD-*</span>
    proprietary: <span style="color: #569cd6;">new</span> Set(),     <span style="color: #6a9955;">// Commercial licenses</span>
    unknown: <span style="color: #569cd6;">new</span> Set()          <span style="color: #6a9955;">// Empty or unrecognized</span>
}

<span style="color: #6a9955;">// Step 2: Define SPDX license classification mappings</span>
strongCopyleftLicenses = [<span style="color: #ce9178;">'GPL-2.0'</span>, <span style="color: #ce9178;">'GPL-3.0'</span>, <span style="color: #ce9178;">'AGPL-3.0'</span>, <span style="color: #ce9178;">'GPL-2.0-only'</span>, <span style="color: #ce9178;">'GPL-3.0-only'</span>]
weakCopyleftLicenses = [<span style="color: #ce9178;">'LGPL-2.1'</span>, <span style="color: #ce9178;">'LGPL-3.0'</span>, <span style="color: #ce9178;">'MPL-2.0'</span>, <span style="color: #ce9178;">'LGPL-2.1-only'</span>, <span style="color: #ce9178;">'LGPL-3.0-only'</span>]
permissiveLicenses = [<span style="color: #ce9178;">'MIT'</span>, <span style="color: #ce9178;">'Apache-2.0'</span>, <span style="color: #ce9178;">'BSD-2-Clause'</span>, <span style="color: #ce9178;">'BSD-3-Clause'</span>, <span style="color: #ce9178;">'ISC'</span>]

<span style="color: #6a9955;">// Step 3: Iterate all project versions</span>
projects = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/project?fields=id,name'</span>)
<span style="color: #c586c0;">for</span> each project <span style="color: #c586c0;">in</span> projects:
    versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projects/${project.id}/versions?fields=id,name`</span>)
    
    <span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions:
        dependencies = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/dependencies`</span>)
        
        <span style="color: #c586c0;">for</span> each dep <span style="color: #c586c0;">in</span> dependencies:
            componentKey = <span style="color: #ce9178;">`${dep.library}:${dep.version}`</span>
            allComponents.<span style="color: #dcdcaa;">add</span>(componentKey)
            
            <span style="color: #6a9955;">// Step 4: Classify license (use least restrictive for multi-licensed)</span>
            <span style="color: #c586c0;">if</span> (!dep.licenses || dep.licenses.length === 0):
                licenseCategories.unknown.<span style="color: #dcdcaa;">add</span>(componentKey)
            <span style="color: #c586c0;">else</span>:
                <span style="color: #6a9955;">// Check if any license in array is permissive (least restrictive)</span>
                isPermissive = dep.licenses.<span style="color: #dcdcaa;">some</span>(lic => permissiveLicenses.<span style="color: #dcdcaa;">includes</span>(lic))
                isWeakCopyleft = dep.licenses.<span style="color: #dcdcaa;">some</span>(lic => weakCopyleftLicenses.<span style="color: #dcdcaa;">includes</span>(lic))
                isStrongCopyleft = dep.licenses.<span style="color: #dcdcaa;">some</span>(lic => strongCopyleftLicenses.<span style="color: #dcdcaa;">includes</span>(lic))
                
                <span style="color: #6a9955;">// Multi-license prioritization: prefer permissive over copyleft</span>
                <span style="color: #c586c0;">if</span> (isPermissive):
                    licenseCategories.permissive.<span style="color: #dcdcaa;">add</span>(componentKey)
                <span style="color: #c586c0;">else if</span> (isWeakCopyleft):
                    licenseCategories.weakCopyleft.<span style="color: #dcdcaa;">add</span>(componentKey)
                <span style="color: #c586c0;">else if</span> (isStrongCopyleft):
                    licenseCategories.strongCopyleft.<span style="color: #dcdcaa;">add</span>(componentKey)
                <span style="color: #c586c0;">else</span>:
                    licenseCategories.unknown.<span style="color: #dcdcaa;">add</span>(componentKey)  <span style="color: #6a9955;">// Unrecognized SPDX</span>

<span style="color: #6a9955;">// Step 5: Calculate percentages</span>
totalComponents = allComponents.size
strongCopyleftPct = (licenseCategories.strongCopyleft.size / totalComponents) * 100
weakCopyleftPct = (licenseCategories.weakCopyleft.size / totalComponents) * 100
permissivePct = (licenseCategories.permissive.size / totalComponents) * 100
unknownPct = (licenseCategories.unknown.size / totalComponents) * 100

<span style="color: #6a9955;">// Display: "8% Strong Copyleft (4,674) | 14% Weak (8,194) | 72% Permissive (42,155) | 6% Unknown (3,507)"</span>
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> SSC license data availability is <strong>CRITICAL DEPENDENCY</strong> on SCA tool integration qualityâ€”native Fortify SCA provides license metadata from component manifests (package.json, pom.xml), but third-party SCA imports may not include license information. Verify data availability first via GET /api/v1/projectVersions/{id}/dependencies (check if <code>licenses</code> field exists and is populated). License classification must be implemented client-side since SSC does not provide risk ratingsâ€”you must maintain SPDX identifier mappings for strong copyleft, weak copyleft, permissive categories. Multi-licensed components (e.g., ["MIT", "GPL-2.0"]) are common in dual-license scenarios; recommendation is to classify by least restrictive license since users can choose permissive terms, though legal review may be needed for accuracy. Unknown licenses (6% in example) require investigationâ€”could indicate missing component metadata, custom proprietary licenses, or internal libraries without formal declarations. Same performance limitations as KPI 9 (5-10 minute query time for 1000+ versions)â€”recommend daily batch job with cached results. License data accuracy depends on component manifest file quality; incorrect metadata in pom.xml/package.json results in wrong license classification.</p>
                                </li>
                                <li><strong>License Classification:</strong> Use SPDX license identifiers for standardization. Categories: (1) Strong Copyleft = GPL-2.0, GPL-3.0, AGPL-3.0 (viral licenses requiring source disclosure), (2) Weak Copyleft = LGPL-2.1, LGPL-3.0, MPL-2.0 (library-level copyleft), (3) Permissive = MIT, Apache-2.0, BSD-3-Clause (minimal restrictions), (4) Proprietary = commercial licenses, (5) Unknown = no license metadata or custom licenses.</li>
                                <li><strong>Platform Gaps:</strong> FoD has better license detection and classification (built-in risk ratings). SSC license data quality variesâ€”depends on SCA tool quality and component metadata completeness. Neither platform provides legal interpretationâ€”just license identification. Complex licensing (dual-licensed components, license exceptions) may not be handled correctlyâ€”manual review required.</li>
                                <li><strong>Data Quality:</strong> Unknown licenses (6%) require investigationâ€”could be: (1) custom/proprietary licenses not in SPDX database, (2) components with missing metadata, (3) internal libraries without formal licenses. License detection accuracy depends on manifest files (package.json, pom.xml, go.mod)â€”missing/incorrect manifests = wrong license data. Multi-licensed components (e.g., "MIT OR Apache-2.0") typically show least restrictive license.</li>
                                <li><strong>Policy Development:</strong> Define organizational license policy: Which licenses are prohibited? (e.g., AGPL for SaaS products). Which require legal review? (e.g., any copyleft for commercial software). Which are auto-approved? (e.g., MIT, Apache-2.0 for all uses). Enforce policy via CI/CD license scanning gates. Document rationale for legal team alignment.</li>
                                <li><strong>Use Case Context:</strong> License risk is context-dependent: Internal tools = copyleft acceptable. Customer-facing products = copyleft problematic. SaaS = AGPL especially problematic (network copyleft). Open source projects = copyleft compatible. Add "Product Type" filter to dashboard for context-specific views.</li>
                                <li><strong>M&A Due Diligence:</strong> This metric is critical during acquisitionsâ€”license violations can derail deals. Generate SBOM with license attribution for due diligence requests. Identify high-risk components requiring replacement before acquisition closes. 8% strong copyleft may be acceptable or deal-breaker depending on business model.</li>
                                <li><strong>Recommendation:</strong> Investigate 6% unknown licenses immediatelyâ€”generate reports with component names, versions, file paths. Partner with legal team to define license policy and risk thresholds. Implement CI/CD license gates to prevent new high-risk licenses. For SSC: ensure SCA integration provides license metadata or use third-party license scanning tools. Add "License Violations" count (components violating policy) for actionable metric. Target: <1% unknown, <5% strong copyleft for commercial software.</li>
                            </ul>
                        </div>
                    </details>

                    
                    </details>
                </div>
            </div>

            <!-- FoD API Enhancement Recommendations for Risk Exposure Dashboard -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ’¡ Recommended FoD API Enhancements for Risk Exposure Dashboard Implementation</summary>
                        <div>
                            <p style="margin-bottom: 16px;">The current FoD API requires extensive client-side aggregation, N+1 query patterns, and pagination overhead to implement the Risk Exposure Dashboard. The following endpoints would dramatically improve performance, reduce API call volume, and simplify implementation.</p>
                            
                            <div style="margin-bottom: 16px; padding: 12px; background: rgba(59, 130, 246, 0.1); border-left: 4px solid #3b82f6; border-radius: 4px;">
                                <p style="margin: 0; font-weight: 600; color: #3b82f6;">âœ… What Already Works Well:</p>
                                <p style="margin: 8px 0 0 0;"><strong>GET /api/v3/releases</strong> endpoint already provides efficient bulk queries for policy compliance and star ratings. It supports:</p>
                                <ul style="margin: 8px 0 0 0;">
                                    <li><strong>Bulk retrieval:</strong> 50 releases per call (vs. 1 per call for individual endpoints)</li>
                                    <li><strong>Built-in fields:</strong> <code>isPassed</code> (policy compliance), <code>rating</code> (star rating), vulnerability counts</li>
                                    <li><strong>Comprehensive filtering:</strong> <code>filters</code> and <code>releaseFilters</code> parameters for application/release attributes</li>
                                    <li><strong>Efficiency:</strong> For 300 releases: 6 API calls (50 per page) vs. 300 individual calls</li>
                                </ul>
                                <p style="margin: 8px 0 0 0; font-style: italic; opacity: 0.9;">This means the "Versions by Policy Compliance" and "Versions by Star Rating" KPIs can be efficiently implemented using existing endpointsâ€”no new endpoints needed for these metrics.</p>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: rgba(220, 38, 38, 0.1); border: 2px solid var(--sev-critical); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--sev-critical);">ðŸ”´ Critical Missing Endpoints (Must-Have)</h4>
                                
                                <details style="margin-top: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600;">1. GET /api/v3/releases/vulnerability-analytics - Category Analysis & Technical Debt</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> "Most Prevalent Vulnerability Categories", "AgeÃ—Severity Matrix", and "Security Technical Debt" KPIs require querying all vulnerabilities across the portfolio to extract category names, aging data, and calculate technical debt. This requires paginating through 10,000+ individual vulnerability records to group by category and calculate age distributions.</p>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #28a745; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âœ… What GET /api/v3/tenant-summary Already Provides:</p>
                                            <p style="margin: 0 0 8px 0; font-size: 13px;">The existing <code>tenant-summary</code> endpoint efficiently provides the data needed for several Risk Exposure Dashboard KPIs:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Open Issues by Severity (All):</strong> <code>criticalCount</code>, <code>highCount</code>, <code>mediumCount</code>, <code>lowCount</code> fields</li>
                                                <li>âœ… <strong>Open Issues by Severity (Production):</strong> Add <code>?sdlcStatus=Production</code> parameter</li>
                                                <li>âœ… <strong>Vulnerability Density:</strong> Calculate from <code>(criticalCount + highCount + mediumCount + lowCount) / totalReleaseCount</code></li>
                                                <li>âœ… <strong>Efficient single-call query:</strong> Returns all severity counts in one response</li>
                                                <li>âœ… <strong>SDLC filtering:</strong> Supports Production, QA, Development, Retired filters</li>
                                            </ul>
                                            <p style="margin: 8px 0 0 0; font-size: 13px; font-weight: 600; color: var(--text);">Example: <code>GET /api/v3/tenant-summary?sdlcStatus=Production</code> gives all production vulnerability counts instantly.</p>
                                        </div>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš ï¸ Gap: tenant-summary Doesn't Provide Category/Aging Analytics</p>
                                            <p style="margin: 0; font-size: 13px;">While <code>tenant-summary</code> gives <strong>total counts by severity</strong>, it lacks:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âŒ <strong>Category breakdown:</strong> Which vulnerability types are most prevalent? (e.g., "SQL Injection: 342 issues across 28 releases")</li>
                                                <li>âŒ <strong>Age distribution:</strong> How many issues are aging beyond 30/60/90 days? (AgeÃ—Severity matrix)</li>
                                                <li>âŒ <strong>Technical debt calculation:</strong> Estimated remediation hours based on severity and count</li>
                                                <li>âŒ <strong>Cross-release analytics:</strong> Which categories affect the most applications?</li>
                                                <li>âŒ <strong>Advanced filtering:</strong> Only supports single <code>sdlcStatus</code> value, no custom attributes</li>
                                            </ul>
                                            <p style="margin: 8px 0 0 0; font-size: 13px; font-weight: 600; color: var(--text);">These analytics require querying individual vulnerability records (300+ releases Ã— 50+ vulnerabilities = 15,000+ records), which is extremely expensive via existing endpoints.</p>
                                        </div>
                                        
                                        <p><strong>Proposed New Endpoint (Focused on Analytics Gaps):</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>GET /api/v3/releases/vulnerability-analytics?filters={applicationFilters}&releaseFilters={releaseFilters}</code></pre>
                                        
                                        <p><strong>Purpose:</strong> Provides <strong>category-level analytics and aging data</strong> that <code>tenant-summary</code> doesn't cover. For basic severity counts, continue using <code>tenant-summary</code>.</p>
                                        
                                        <p><strong>Query Parameters:</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string) - Application-level attribute filters:
                                                <ul>
                                                    <li><code>businessCriticalityType:High</code> - High-criticality apps</li>
                                                    <li><code>attributes.BusinessUnit:Finance</code> - Custom application attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>releaseFilters</code> (string) - Release-level attribute filters:
                                                <ul>
                                                    <li><code>sdlcStatusType:Production</code> - Production releases (optional filter, NOT default)</li>
                                                    <li><code>sdlcStatusType:Production|QA</code> - Production or QA</li>
                                                    <li><code>attributes.Region:US-East</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>topN</code> (integer, default: 10) - Number of top categories to return</li>
                                            <li><code>ageBuckets</code> (string, default: "30,60,90") - Age grouping in days</li>
                                            <li><code>includeTechnicalDebt</code> (boolean, default: true) - Include remediation effort estimates</li>
                                            <li><code>excludeSuppressed</code> (boolean, default: true) - Exclude suppressed vulnerabilities</li>
                                        </ul>
                                        
                                        <p><strong>âš ï¸ CRITICAL:</strong> Must support custom attribute filtering on both applications and releases. Default: include all active releases (Dev/QA/Production), exclude only retired releases. Do NOT default to production-only.</p>
                                        
                                        <p><strong>Response Schema (Analytics-Focused):</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  // ========== CATEGORY ANALYTICS (Gap in tenant-summary) ==========
  "topCategories": [
    {
      "category": "Command Injection",
      "categoryId": 123,
      "totalIssues": 342,
      "bySeverity": {
        "critical": 58,
        "high": 104,
        "medium": 98,
        "low": 82
      },
      "affectedReleases": 28,
      "affectedApplications": 19,
      "percentOfPortfolio": 2.5  // 342 / 13,429 total issues
    },
    {
      "category": "SQL Injection",
      "categoryId": 87,
      "totalIssues": 298,
      "bySeverity": {
        "critical": 42,
        "high": 87,
        "medium": 94,
        "low": 75
      },
      "affectedReleases": 24,
      "affectedApplications": 16,
      "percentOfPortfolio": 2.2
    }
    // ... up to topN categories
  ],
  
  // ========== AGEÃ—SEVERITY MATRIX (Gap in tenant-summary) ==========
  "agingSummary": {
    "lessThan30Days": {
      "critical": 1427, "high": 2749, "medium": 3122, "low": 4404
    },
    "days30To60": {
      "critical": 898, "high": 1621, "medium": 2284, "low": 3206
    },
    "days60To90": {
      "critical": 587, "high": 1034, "medium": 1647, "low": 2304
    },
    "moreThan90Days": {
      "critical": 453, "high": 832, "medium": 1268, "low": 2199
    }
  },
  
  // ========== TECHNICAL DEBT (Gap in tenant-summary) ==========
  "technicalDebt": {
    "totalEstimatedHours": 4284,
    "bySeverity": {
      "critical": {
        "count": 3365,
        "hoursPerIssue": 8,
        "totalHours": 26920
      },
      "high": {
        "count": 6287,
        "hoursPerIssue": 4,
        "totalHours": 25148
      },
      "medium": {
        "count": 8321,
        "hoursPerIssue": 2,
        "totalHours": 16642
      },
      "low": {
        "count": 12113,
        "hoursPerIssue": 0.5,
        "totalHours": 6056
      }
    },
    "estimatedCost": 535650  // totalHours Ã— $125/hour
  },
  
  // ========== CONTEXT (for reference) ==========
  "portfolioContext": {
    "totalVulnerabilities": 30086,  // For reference (also in tenant-summary)
    "totalReleases": 367,
    "totalApplications": 123
  },
  
  "appliedFilters": {
    "applicationFilters": null,
    "releaseFilters": "sdlcStatusType:Production",
    "effectiveApplicationCount": 123,
    "effectiveReleaseCount": 142
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>Example Queries:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default: All active releases (Dev/QA/Prod), top 10 categories, aging analysis
GET /api/v3/releases/vulnerability-analytics

// Production-only analytics (user-applied filter when explicitly needed)
GET /api/v3/releases/vulnerability-analytics?releaseFilters=sdlcStatusType:Production

// High-criticality applications, all environments, top 20 categories
GET /api/v3/releases/vulnerability-analytics?filters=businessCriticalityType:High&topN=20

// Finance business unit, production releases, custom age buckets
GET /api/v3/releases/vulnerability-analytics?filters=attributes.BusinessUnit:Finance&releaseFilters=sdlcStatusType:Production&ageBuckets=15,45,90

// Minimal payload (skip technical debt calculation)
GET /api/v3/releases/vulnerability-analytics?includeTechnicalDebt=false</code></pre>
                                        
                                        <p><strong>Recommended Implementation Pattern:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// ========== EFFICIENT DASHBOARD LOADING ==========

// Call 1: Get basic severity counts (fast, existing endpoint)
const severityCounts = await fetch('/api/v3/tenant-summary?sdlcStatus=Production');
// Returns: { criticalCount: 3365, highCount: 6287, mediumCount: 8321, lowCount: 12113 }
// Use for: "Open Issues by Severity (Production)" KPI, Vulnerability Density calculation

// Call 2: Get analytics (categories, aging, technical debt) - NEW endpoint
const analytics = await fetch('/api/v3/releases/vulnerability-analytics?releaseFilters=sdlcStatusType:Production');
// Returns: { topCategories: [...], agingSummary: {...}, technicalDebt: {...} }
// Use for: "Most Prevalent Categories", "AgeÃ—Severity Matrix", "Security Technical Debt" KPIs

// ========== RESULT: 2 API calls instead of 500+ ==========</code></pre>
                                        
                                        <p><strong>Performance Benefits:</strong></p>
                                        <ul>
                                            <li><strong>API Calls:</strong> 500+ calls (300 releases + 200+ pagination) â†’ 2 calls (tenant-summary + analytics) = 99.6% reduction</li>
                                            <li><strong>Data Transfer:</strong> ~50MB paginated vulnerability data â†’ ~100KB combined (99.8% reduction)</li>
                                            <li><strong>Load Time:</strong> 45-60 seconds â†’ 2-3 seconds (95% improvement)</li>
                                            <li><strong>Leverages Existing:</strong> Reuses <code>tenant-summary</code> for basic counts, new endpoint only for advanced analytics</li>
                                            <li><strong>Clear Separation:</strong> <code>tenant-summary</code> = severity counts, <code>vulnerability-analytics</code> = category/aging/debt analysis</li>
                                        </ul>
                                        
                                        <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: #4a9eff;">ðŸŽ¯ Why This Approach is Better:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Reuses existing endpoint:</strong> <code>tenant-summary</code> already efficiently provides severity counts and SDLC filtering</li>
                                                <li>âœ… <strong>Focuses new endpoint on gaps:</strong> Category analytics, aging data, technical debt calculations that aren't available anywhere</li>
                                                <li>âœ… <strong>Backward compatible:</strong> Existing <code>tenant-summary</code> consumers unaffected</li>
                                                <li>âœ… <strong>Simpler implementation:</strong> New endpoint doesn't duplicate severity aggregation (that's already done)</li>
                                                <li>âœ… <strong>Better caching:</strong> Can cache <code>tenant-summary</code> and <code>vulnerability-analytics</code> independently with different TTLs</li>
                                                <li>âœ… <strong>Efficient filtering:</strong> <code>tenant-summary</code> handles basic SDLC filtering, new endpoint adds advanced custom attribute filtering</li>
                                            </ul>
                                        </div>
                                        
                                        <p><strong>Implementation Notes:</strong></p>
                                        <ul>
                                            <li><strong>Category Grouping:</strong> Group vulnerabilities by <code>category</code> field (e.g., "SQL Injection", "XSS", "Command Injection"). Return top N by total issue count.</li>
                                            <li><strong>Aging Calculation:</strong> Calculate age as <code>(now - introducedDate)</code> in days. Bucket into configurable ranges (default: 0-30, 30-60, 60-90, 90+ days). Cross with severity for matrix.</li>
                                            <li><strong>Technical Debt Formula:</strong> Use industry-standard remediation hours: Critical=8h, High=4h, Medium=2h, Low=0.5h. Multiply by developer hourly rate ($125 default). This is an <strong>estimate</strong> for prioritization, not precise billing.</li>
                                            <li><strong>Deduplication:</strong> Count each unique vulnerability instance (not vulnerability ID). Same vulnerability in 3 releases = 3 issues.</li>
                                            <li><strong>Performance Optimization:</strong> Use database aggregation queries (GROUP BY category, CASE for age buckets). Index on <code>introducedDate</code>, <code>category</code>, <code>severity</code>. Cache for 1 hour (analytics change slowly).</li>
                                            <li><strong>Filter Consistency:</strong> When <code>releaseFilters=sdlcStatusType:Production</code> is used, analytics should match <code>tenant-summary?sdlcStatus=Production</code> totals.</li>
                                        </ul>
                                    </div>
                                </details>

                                <details style="margin-top: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600;">2. Enhance GET /api/v3/applications/open-source-components - Add Summary Metrics & Filtering</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> "Open Source Security Risk" and "License Risk" KPIs need portfolio-wide component metrics with filtering by business criticality and SDLC status. Current endpoint provides detailed component list but requires 77+ paginated API calls and client-side aggregation for a typical portfolio.</p>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #28a745; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âœ… What GET /api/v3/applications/open-source-components Already Does Well:</p>
                                            <p style="margin: 0 0 8px 0; font-size: 13px;">This endpoint is <strong>already designed for portfolio-wide data</strong>:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Tenant-wide scope:</strong> Returns deduplicated components across ALL releases and applications</li>
                                                <li>âœ… <strong>Deduplication built-in:</strong> Each component appears only once (uses componentHash)</li>
                                                <li>âœ… <strong>Rich metadata:</strong> <code>componentName</code>, <code>vulnerabilityCounts[]</code>, <code>licenses[]</code>, <code>releases[]</code></li>
                                                <li>âœ… <strong>Pagination support:</strong> 50 components per page with totalCount</li>
                                            </ul>
                                        </div>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš ï¸ Gap 1: No Summary Metrics Mode</p>
                                            <p style="margin: 0; font-size: 13px;">Dashboard needs aggregated counts, not full component list:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Current behavior:</strong> Returns paginated list (50 components/page)</li>
                                                <li>âŒ <strong>Dashboard needs:</strong> Just the totals - "38% vulnerable", "8% Strong Copyleft"</li>
                                                <li>âŒ <strong>Current cost:</strong> 3,847 components Ã· 50/page = 77 API calls + client-side aggregation</li>
                                                <li>âŒ <strong>Impact:</strong> Slow dashboard load, complex client code, unnecessary data transfer</li>
                                            </ul>
                                        </div>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš ï¸ Gap 2: No Application/Release Attribute Filtering</p>
                                            <p style="margin: 0; font-size: 13px;">Current <code>filters</code> parameter only supports component-level fields:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Supported now:</strong> <code>filters=componentName:log4j</code>, <code>scanTool:Sonatype</code></li>
                                                <li>âŒ <strong>Not supported:</strong> Filter by application businessCriticalityType, custom attributes</li>
                                                <li>âŒ <strong>Not supported:</strong> Filter by release sdlcStatusType (Production only), custom release attributes</li>
                                                <li>âŒ <strong>Impact:</strong> Can't answer "Components in production only" or "High-criticality apps only"</li>
                                            </ul>
                                        </div>
                                        
                                        <div style="margin: 12px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš ï¸ Gap 3: No License Risk Categorization</p>
                                            <p style="margin: 0; font-size: 13px;">Current <code>licenses[]</code> array only contains license names:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Current data:</strong> <code>{ "name": "GPL-3.0" }</code></li>
                                                <li>âŒ <strong>Missing context:</strong> Is GPL-3.0 high risk (Strong Copyleft) or low risk (Permissive)?</li>
                                                <li>âŒ <strong>Client burden:</strong> Must maintain license database client-side to categorize</li>
                                                <li>âŒ <strong>Impact:</strong> Can't aggregate "8% Strong Copyleft" without complex client logic</li>
                                            </ul>
                                        </div>
                                        
                                        <p><strong>Recommended Enhancement: Add Summary Metrics Mode</strong></p>
                                        <p style="font-size: 13px; margin: 8px 0;">Add <code>summary=true</code> parameter to return aggregated metrics instead of paginated component list:</p>
                                        
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>// Default behavior (unchanged): Paginated component list
GET /api/v3/applications/open-source-components

// NEW: Summary metrics only (dashboard mode)
GET /api/v3/applications/open-source-components?summary=true&releaseFilters=sdlcStatusType:Production</code></pre>
                                        
                                        <p><strong>New Query Parameters:</strong></p>
                                        <ul>
                                            <li><code>summary</code> (boolean, default: <code>false</code>) - Return aggregated metrics instead of component list
                                                <ul>
                                                    <li><code>summary=false</code> (default): Existing behavior - paginated component list</li>
                                                    <li><code>summary=true</code>: NEW behavior - portfolio-wide aggregated metrics (see response below)</li>
                                                </ul>
                                            </li>
                                            <li><code>applicationFilters</code> (string, NEW) - Filter by application-level attributes:
                                                <ul>
                                                    <li><code>applicationFilters=businessCriticalityType:High</code> - Only components used by high-criticality apps</li>
                                                    <li><code>applicationFilters=attributes.BusinessUnit:Finance</code> - Custom application attributes</li>
                                                    <li><code>applicationFilters=businessCriticalityType:High|Medium</code> - OR condition</li>
                                                </ul>
                                            </li>
                                            <li><code>releaseFilters</code> (string, NEW) - Filter by release-level attributes:
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code> - Only components in production releases</li>
                                                    <li><code>releaseFilters=sdlcStatusType:Production|QA</code> - Production OR QA</li>
                                                    <li><code>releaseFilters=suspended:false</code> - Exclude suspended/retired releases (should be DEFAULT)</li>
                                                    <li><code>releaseFilters=attributes.ComplianceScope:PCI</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                            <li><code>topN</code> (integer, default: 10) - When <code>summary=true</code>, number of top vulnerable components to include</li>
                                        </ul>
                                        
                                        <p><strong>Enhanced Response Schema (when summary=true):</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "summary": {
    // ========== COMPONENT COUNTS (Uses existing componentHash deduplication) ==========
    "totalComponents": 3847,
    "totalUniqueComponents": 3847,  // Already deduplicated by endpoint
    
    // ========== SECURITY RISK (Uses existing vulnerabilityCounts data) ==========
    "securityRisk": {
      "vulnerableComponents": {
        "count": 1462,
        "percentage": 38.0
      },
      "secureComponents": {
        "count": 2385,
        "percentage": 62.0
      },
      "bySeverity": {
        "critical": 127,    // Components with criticalCount > 0
        "high": 389,        // Components with highCount > 0
        "medium": 643,      // Components with mediumCount > 0
        "low": 303          // Components with lowCount > 0
      }
    },
    
    // ========== LICENSE RISK (NEW: Categorize existing licenses[] data) ==========
    "licenseRisk": {
      "strongCopyleft": {
        "count": 308,
        "percentage": 8.0,
        "examples": ["GPL-3.0", "AGPL-3.0", "OSL-3.0"]
      },
      "weakCopyleft": {
        "count": 539,
        "percentage": 14.0,
        "examples": ["LGPL-2.1", "MPL-2.0", "EPL-1.0"]
      },
      "permissive": {
        "count": 2770,
        "percentage": 72.0,
        "examples": ["MIT", "Apache-2.0", "BSD-3-Clause"]
      },
      "unknown": {
        "count": 230,
        "percentage": 6.0,
        "examples": ["UNKNOWN", "NOASSERTION", "Proprietary"]
      }
    },
    
    // ========== TOP VULNERABLE COMPONENTS (NEW: Server-side sorting) ==========
    "topVulnerableComponents": [
      {
        "componentName": "log4j-core",
        "componentVersionName": "2.14.1",
        "packageUrl": "pkg:maven/org.apache.logging.log4j/log4j-core@2.14.1",
        "licenses": ["Apache-2.0"],
        "licenseRiskCategory": "Permissive",  // NEW: Highest risk category across all licenses
        "vulnerabilityCounts": [
          { "severity": "Critical", "severityId": 4, "count": 1 },
          { "severity": "High", "severityId": 3, "count": 0 }
        ],
        "vulnerabilityScore": 40,  // NEW: Weighted score (CriticalÃ—10 + HighÃ—5 + MediumÃ—2 + Low)
        "affectedReleases": 47,
        "affectedApplications": 32
      },
      {
        "componentName": "mysql-connector-java",
        "componentVersionName": "8.0.28",
        "licenses": ["GPL-2.0"],
        "licenseRiskCategory": "StrongCopyleft",  // High-risk license
        "vulnerabilityCounts": [
          { "severity": "High", "severityId": 3, "count": 2 }
        ],
        "vulnerabilityScore": 10,
        "affectedReleases": 23,
        "affectedApplications": 18
      }
      // ... up to topN components (default: 10)
    ]
  },
  
  "appliedFilters": {
    "applicationFilters": null,
    "releaseFilters": "sdlcStatusType:Production",
    "effectiveApplicationCount": 123,
    "effectiveReleaseCount": 142
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>License Risk Categories (for reference):</strong></p>
                                        <ul>
                                            <li><code>"StrongCopyleft"</code> - High risk: GPL-2.0, GPL-3.0, AGPL-3.0, OSL-3.0 (requires derivative works to use same license)</li>
                                            <li><code>"WeakCopyleft"</code> - Medium risk: LGPL-2.1, LGPL-3.0, MPL-2.0, EPL-1.0 (allows proprietary linking)</li>
                                            <li><code>"Permissive"</code> - Low risk: MIT, Apache-2.0, BSD-3-Clause, ISC (minimal restrictions)</li>
                                            <li><code>"Unknown"</code> - Unknown risk: UNKNOWN, NOASSERTION, null, custom/proprietary licenses</li>
                                        </ul>
                                        
                                        <p><strong>Example Enhanced Queries:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default: Paginated component list (unchanged, backward compatible)
GET /api/v3/applications/open-source-components

// NEW: Portfolio-wide summary metrics, all active releases
GET /api/v3/applications/open-source-components?summary=true

// NEW: Summary for production releases only
GET /api/v3/applications/open-source-components?summary=true&releaseFilters=sdlcStatusType:Production

// NEW: High-criticality apps, production releases, top 20 vulnerable components
GET /api/v3/applications/open-source-components?summary=true&applicationFilters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production&topN=20

// NEW: Finance BU apps, PCI-compliant releases
GET /api/v3/applications/open-source-components?summary=true&applicationFilters=attributes.BusinessUnit:Finance&releaseFilters=attributes.ComplianceScope:PCI

// Existing filters still work with summary mode
GET /api/v3/applications/open-source-components?summary=true&filters=scanTool:Sonatype</code></pre>
                                        
                                        <p><strong>Dashboard Implementation Pattern (SIMPLIFIED!):</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// BEFORE (current approach): 77 API calls + client-side aggregation
const components = await fetchAllPages('/api/v3/applications/open-source-components?releaseFilters=sdlcStatusType:Production');
const totalComponents = components.length;
const vulnerableComponents = components.filter(c => c.vulnerabilityCounts.some(vc => vc.count > 0));
// ... complex client-side license categorization ...

// AFTER (with summary=true): 1 API call, server-side aggregation
const response = await fetch('/api/v3/applications/open-source-components?summary=true&releaseFilters=sdlcStatusType:Production');
const data = await response.json();

// All KPIs ready to display - no client-side processing needed!
const securityRiskKPI = {
  vulnerable: data.summary.securityRisk.vulnerableComponents.percentage,  // 38%
  secure: data.summary.securityRisk.secureComponents.percentage            // 62%
};

const licenseRiskKPI = {
  strongCopyleft: data.summary.licenseRisk.strongCopyleft.percentage,      // 8%
  weakCopyleft: data.summary.licenseRisk.weakCopyleft.percentage,          // 14%
  permissive: data.summary.licenseRisk.permissive.percentage,              // 72%
  unknown: data.summary.licenseRisk.unknown.percentage                     // 6%
};

const topVulnerableComponents = data.summary.topVulnerableComponents;  // Top 10 ready to display</code></pre>
                                        
                                        <p><strong>Performance Benefits:</strong></p>
                                        <ul>
                                            <li><strong>API Calls:</strong> 77 paginated calls â†’ 1 summary call (99% reduction)</li>
                                            <li><strong>Data Transfer:</strong> ~15MB paginated components â†’ ~10KB summary (99.9% reduction)</li>
                                            <li><strong>Load Time:</strong> 15-30 seconds â†’ <2 seconds (90%+ improvement)</li>
                                            <li><strong>No client-side aggregation:</strong> Server calculates all metrics - dashboard just displays them</li>
                                            <li><strong>No license database needed:</strong> Server categorizes licenses using SPDX database</li>
                                            <li><strong>Backward compatible:</strong> <code>summary=false</code> (default) preserves existing paginated behavior</li>
                                            <li><strong>Cacheable:</strong> Summary results can be cached for 1 hour (component data changes slowly)</li>
                                        </ul>
                                        
                                        <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: #4a9eff;">ðŸŽ¯ Why Summary Mode is the Right Approach:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li>âœ… <strong>Dashboard needs metrics, not lists:</strong> Risk Exposure Dashboard shows "38% vulnerable", not 3,847 component rows</li>
                                                <li>âœ… <strong>Server-side aggregation is faster:</strong> Database can count/group in milliseconds vs paginating 77 API calls</li>
                                                <li>âœ… <strong>Minimal data transfer:</strong> 10KB summary vs 15MB of detailed component data</li>
                                                <li>âœ… <strong>Simpler client code:</strong> No pagination loops, no client-side license categorization, no complex filtering</li>
                                                <li>âœ… <strong>Consistent with FoD patterns:</strong> Similar to tenant-summary endpoint (aggregated metrics with filtering)</li>
                                                <li>âœ… <strong>Optional enhancement:</strong> Existing paginated list mode still available for detailed component investigation</li>
                                                <li>âœ… <strong>Future-proof:</strong> Top N components list helps users drill down from summary to details</li>
                                            </ul>
                                        </div>
                                        
                                        <p><strong>Implementation Notes:</strong></p>
                                        <ul>
                                            <li><strong>Default Behavior:</strong> Should exclude retired/suspended releases by default. Add <code>includeRetired=true</code> parameter if customer wants to include them.</li>
                                            <li><strong>Filter Join Logic:</strong> When <code>releaseFilters=sdlcStatusType:Production</code> is set, only count components used by at least one production release. If a component is in both Dev and Production, it should still appear in Production filter (because it's in Production).</li>
                                            <li><strong>License Risk Mapping:</strong> Use SPDX license database for categorization. Map common license names (case-insensitive):
                                                <ul>
                                                    <li><strong>StrongCopyleft:</strong> GPL-2.0, GPL-3.0, AGPL-3.0, AGPL-1.0, OSL-3.0, EUPL-1.2</li>
                                                    <li><strong>WeakCopyleft:</strong> LGPL-2.1, LGPL-3.0, MPL-2.0, EPL-1.0, EPL-2.0, CDDL-1.0, CPL-1.0</li>
                                                    <li><strong>Permissive:</strong> MIT, Apache-2.0, BSD-2-Clause, BSD-3-Clause, ISC, 0BSD, Unlicense</li>
                                                    <li><strong>Unknown:</strong> UNKNOWN, NOASSERTION, null, any unrecognized license name</li>
                                                </ul>
                                            </li>
                                            <li><strong>License Risk Category for Component:</strong> If component has multiple licenses, use the <strong>highest risk category</strong> (StrongCopyleft > WeakCopyleft > Unknown > Permissive). Example: Component with [MIT, GPL-3.0] = "StrongCopyleft"</li>
                                            <li><strong>Vulnerability Score Calculation:</strong> <code>CriticalÃ—10 + HighÃ—5 + MediumÃ—2 + LowÃ—1</code>. Sort top N components by this score (descending), then by affectedReleases for tie-breaking.</li>
                                            <li><strong>Component Security Status:</strong> Component is "vulnerable" if ANY severity count > 0 (even if just 1 low-severity issue)</li>
                                            <li><strong>Performance Optimization:</strong> Cache summary results for 1 hour (component data changes slowly). Use database aggregation queries (COUNT DISTINCT, GROUP BY, SUM). Index on release.sdlcStatusType, application.businessCriticalityType, component.componentHash for fast filtering.</li>
                                            <li><strong>Backward Compatibility:</strong> When <code>summary</code> parameter is omitted or <code>false</code>, return existing paginated response format. No breaking changes to existing consumers.</li>
                                        </ul>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--bg-surface); border: 2px solid var(--accent); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--accent);">ðŸŸ¢ Nice-to-Have Enhancements (Quality of Life)</h4>
                                
                                <details style="margin-top: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                    <summary style="cursor: pointer; font-weight: 600;">3. GET /api/v3/releases/new-issues-trend - Monthly New Issue Detection Trend</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> "New Issues by Severity (12 Month Trend)" requires querying all vulnerabilities, filtering by introducedDate in each month bucket, grouping by severity. Computationally expensive and slow for large portfolios.</p>
                                        
                                        <p><strong>Proposed Endpoint:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>GET /api/v3/releases/new-issues-trend?filters={filters}&releaseFilters={releaseFilters}&months=12</code></pre>
                                        
                                        <p><strong>Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "trend": [
    {
      "month": "2024-01",
      "newIssues": {
        "total": 600,
        "critical": 70,
        "high": 140,
        "medium": 280,
        "low": 110
      }
    },
    {
      "month": "2024-02",
      "newIssues": {
        "total": 720,
        "critical": 130,
        "high": 250,
        "medium": 160,
        "low": 180
      }
    }
    // ... 12 months
  ]
}</code></pre>
                                        
                                        <p><strong>Note:</strong> This endpoint can be derived from endpoint #1 (vulnerability-summary) if it includes monthly grouping capability. Consider as convenience endpoint only if specific month-over-month trending is frequently needed.</p>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--bg-surface); border: 3px solid #ffc107; border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--text);">âš ï¸ CRITICAL CROSS-CUTTING REQUIREMENT: Application & Release Attribute Filtering</h4>
                                
                                <p style="color: var(--text); margin-bottom: 12px;"><strong>Issue:</strong> All recommended endpoints MUST support comprehensive filtering by application-level and release-level attributes, including custom attributes. This is non-negotiable for real-world dashboard implementations.</p>
                                
                                <div style="background: white; padding: 12px; border-radius: 4px; margin-top: 12px; color: #333;">
                                    <p style="margin-top: 0;"><strong>Why This Matters:</strong></p>
                                    <ul style="margin-bottom: 12px;">
                                        <li><strong>Flexible Filtering:</strong> Dashboards must support filtering to production-only views when needed, but should default to all SDLC statuses (Dev/QA/Production), excluding only retired releases and suppressed vulnerabilities</li>
                                        <li><strong>Business Criticality:</strong> Risk exposure metrics are meaningless without ability to filter by business criticality, compliance scope, or customer-defined importance</li>
                                        <li><strong>Custom Attributes:</strong> Every customer defines unique attributes like "Business Unit", "Compliance Scope", "Geography", "Data Classification"</li>
                                        <li><strong>User-Driven Filtering:</strong> Dashboard users must apply filters via UI controls that translate to API filters</li>
                                        <li><strong>Segmentation:</strong> "Show me risk exposure for Finance BU production apps only" is a common use case</li>
                                    </ul>
                                    
                                    <p><strong>Recommended Filter Architecture (Consistent Across All Endpoints):</strong></p>
                                    <ul style="margin-bottom: 12px;">
                                        <li><code>filters</code> parameter - Application-level attribute filtering (businessCriticalityType, applicationTypeId, custom attributes)</li>
                                        <li><code>releaseFilters</code> parameter - Release-level attribute filtering (sdlcStatusType, suspended, custom release attributes)</li>
                                        <li>Use existing FoD filter syntax: <code>field:value</code>, <code>+</code> for AND, <code>|</code> for OR</li>
                                        <li>Support custom attributes: <code>attributes.{attributeName}:{value}</code> pattern</li>
                                        <li>Return <code>appliedFilters</code> object in response showing what filters were applied and effective counts</li>
                                    </ul>
                                    
                                    <p><strong>Default Behavior (All Endpoints):</strong></p>
                                    <ul style="margin-bottom: 12px;">
                                        <li>Include all active applications and releases across all environments (Dev/QA/Production)</li>
                                        <li>Exclude retired releases (<code>sdlcStatusType:Retired</code>)</li>
                                        <li>Exclude suppressed vulnerabilities where applicable</li>
                                        <li>Do NOT default to production-only - that's an optional filter users apply when needed</li>
                                    </ul>
                                    
                                    <p><strong>Common Filter Patterns:</strong></p>
                                    <pre style="background: #f5f5f5; padding: 8px; border-radius: 4px; overflow-x: auto; font-size: 12px; margin: 8px 0;"><code>// Production releases only (optional filter for production-specific views)
?releaseFilters=sdlcStatusType:Production

// High-criticality apps, production releases
?filters=businessCriticalityType:High&releaseFilters=sdlcStatusType:Production

// Finance business unit, PCI-compliant apps, production or QA
?filters=attributes.BusinessUnit:Finance+attributes.ComplianceScope:PCI&releaseFilters=sdlcStatusType:Production|QA

// All apps except low criticality, all environments
?filters=businessCriticalityType:High|Medium</code></pre>
                                    
                                    <p style="margin-bottom: 0;"><strong>Implementation Note:</strong> This filtering architecture must be designed in from the start, not added as an afterthought. Server-side filtering is critical for performanceâ€”returning all data and filtering client-side defeats the purpose of these aggregation endpoints.</p>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸ“Š Performance Impact Comparison</h4>
                                <table style="width: 100%; border-collapse: collapse; margin-top: 12px;">
                                    <thead>
                                        <tr style="background: rgba(255,255,255,0.05);">
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: left;">Metric</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: center;">Current State</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: center;">With Enhancements</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: center;">Improvement</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border);">API Calls to Render Dashboard</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">600-800+</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">8-12</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center; color: #51cf66; font-weight: 600;">98%+ reduction</td>
                                        </tr>
                                        <tr style="background: rgba(255,255,255,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border);">Data Transfer Size</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">80-120 MB</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">300-600 KB</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center; color: #51cf66; font-weight: 600;">99%+ reduction</td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Dashboard Load Time</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">60-90 seconds</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">3-5 seconds</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center; color: #51cf66; font-weight: 600;">94%+ improvement</td>
                                        </tr>
                                        <tr style="background: rgba(255,255,255,0.02);">
                                            <td style="padding: 8px; border: 1px solid var(--border);">Client-Side Processing</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Heavy (pagination, aggregation, filtering)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Minimal (render only)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center; color: #51cf66; font-weight: 600;">90%+ reduction</td>
                                        </tr>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Server Load per Dashboard Render</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Very High (800+ DB queries)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Low (8-12 optimized queries)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center; color: #51cf66; font-weight: 600;">98%+ reduction</td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <div style="margin-top: 12px; padding: 12px; background: var(--bg-surface); border-left: 3px solid #51cf66; border-radius: 4px;">
                                    <p style="margin: 0;"><strong>Note:</strong> Current state already benefits from efficient <code>GET /api/v3/releases</code> bulk queries for policy compliance and star ratings (6 calls for 300 releases). The major performance gains come from the vulnerability aggregation and SCA component summary endpoints.</p>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸŽ¯ Implementation Priority Ranking</h4>
                                <table style="width: 100%; border-collapse: collapse; margin-top: 12px;">
                                    <thead>
                                        <tr style="background: rgba(255,255,255,0.05);">
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: left;">Priority</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: left;">Endpoint</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: left;">Impact</th>
                                            <th style="padding: 8px; border: 1px solid var(--border); text-align: left;">KPIs Enabled / Use Case</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr style="background: rgba(220, 38, 38, 0.1);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600;">ðŸ”´ P0</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);"><code>/api/v3/releases/vulnerability-summary</code></td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Massive - enables 6 of 10 KPIs, 500+ API calls â†’ 1</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Open Issues (All/Prod), Density, AgeÃ—Severity Matrix, Top Categories, New Issues Trend</td>
                                        </tr>
                                        <tr style="background: rgba(220, 38, 38, 0.05);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600;">ðŸ”´ P0</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);"><code>/api/v3/scans/components-summary</code></td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Critical - enables supply chain risk KPIs (both 10/10 value)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Open Source Security Risk, License Risk - requires aggregating 50K+ components across 300+ SCA scans</td>
                                        </tr>
                                        <tr style="background: var(--bg-surface);">
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600;">ï¿½ P2</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);"><code>/api/v3/releases/new-issues-trend</code></td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">Low - convenience endpoint (can derive from vulnerability-summary)</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">New Issues by Severity - only if month-over-month trending is frequently needed</td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <div style="margin-top: 16px; padding: 12px; background: rgba(59, 130, 246, 0.1); border-left: 3px solid #3b82f6; border-radius: 4px;">
                                    <p style="margin: 0;"><strong>Note on Existing GET /api/v3/releases Endpoint:</strong></p>
                                    <p style="margin: 8px 0 0 0;">The existing <code>/api/v3/releases</code> endpoint already efficiently handles Policy Compliance and Star Rating KPIs with bulk queries (50 releases per call) and comprehensive filtering. No new endpoints needed for these metricsâ€”they're already well-designed.</p>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: rgba(81, 207, 102, 0.1); border: 2px solid #51cf66; border-radius: 8px;">
                                <h4 style="margin-top: 0; color: #51cf66;">âœ… Additional Benefits Beyond Performance</h4>
                                <ul style="margin-bottom: 0;">
                                    <li><strong>Consistency:</strong> Server-side aggregation ensures all dashboards show identical metrics (no client-side logic divergence)</li>
                                    <li><strong>Caching:</strong> Server can cache expensive aggregations and refresh hourly, improving performance for all users</li>
                                    <li><strong>Reduced Bandwidth:</strong> 99%+ reduction in data transfer benefits mobile/remote users and reduces infrastructure costs</li>
                                    <li><strong>Simplified Client Code:</strong> Eliminates complex pagination, aggregation, and filtering logic from dashboard implementations</li>
                                    <li><strong>Real-time Feasibility:</strong> Sub-3-second load times enable real-time dashboards (auto-refresh every 5-10 minutes)</li>
                                    <li><strong>Scalability:</strong> Server-side aggregation scales better than thousands of clients doing the same calculations</li>
                                    <li><strong>API Rate Limit Friendliness:</strong> 99% fewer API calls prevents rate limit issues for large portfolios</li>
                                    <li><strong>Audit & Compliance:</strong> Server-side policy evaluation provides audit trail of compliance assessments</li>
                                    <li><strong>Mobile Support:</strong> Dramatically reduced data transfer makes mobile dashboards practical</li>
                                    <li><strong>Multi-Tenant Efficiency:</strong> SaaS customers benefit from shared caching infrastructure</li>
                                    <li><strong>Filtering Flexibility:</strong> Full support for application and release attribute filtering from day one enables customer-specific views</li>
                                    <li><strong>Executive Adoption:</strong> Fast-loading, responsive dashboards drive executive engagement vs. slow, frustrating UX</li>
                                </ul>
                            </div>

                            <div style="margin-top: 24px; padding: 12px; background: rgba(59, 130, 246, 0.1); border-left: 4px solid #3b82f6; border-radius: 4px;">
                                <p style="margin: 0; font-weight: 600; color: #3b82f6;">ðŸ’¡ Recommendation:</p>
                                <p style="margin: 8px 0 0 0;">Implement the two P0 endpoints (<code>vulnerability-summary</code> and <code>components-summary</code>) as the highest priority. These two endpoints would enable 8 of 10 KPIs on the Risk Exposure Dashboard and reduce dashboard load time from 60-90 seconds to under 5 secondsâ€”a transformative improvement. The remaining 2 KPIs (Policy Compliance and Star Rating) are already well-served by the existing <code>GET /api/v3/releases</code> endpoint's bulk query capabilities. Add comprehensive filtering support (application attributes, release attributes, custom attributes) from the start to ensure real-world usability.</p>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
        </section>

        <!-- ===================== REMEDIATION DASHBOARD ===================== -->
        <section id="page-remediation" class="page" aria-label="Remediation Dashboard">
            <!-- ROW 1: % Issues Fixed by Severity + % Issues Fixed by Scan Type -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Remediated Issues by Severity</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Issues fixed by severity">
                        <div class="hlabel">Critical</div>
                        <div class="hbar" style="--pct:78; --fill-color:var(--sev-critical)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">78% (2,418)</div>
                        <div class="hlabel">High</div>
                        <div class="hbar" style="--pct:82; --fill-color:var(--sev-high)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">82% (4,264)</div>
                        <div class="hlabel">Medium</div>
                        <div class="hbar" style="--pct:71; --fill-color:var(--sev-medium)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">71% (5,893)</div>
                        <div class="hlabel">Low</div>
                        <div class="hbar" style="--pct:64; --fill-color:var(--sev-low)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">64% (8,832)</div>
                        <div class="hlabel"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            Total</div>
                        <div style="padding-top:8px; border-top:1px solid var(--border); margin-top:8px;"></div>
                        <div class="hmeta"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            73% (21,407)</div>
                    </div>
                </div>

                <div class="card col-6">
                    <h3>Remediated Issues by Scan Type</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--c-sast)"></i>SAST</span>
                        <span class="sw"><i style="background:var(--c-dast)"></i>DAST</span>
                        <span class="sw"><i style="background:var(--c-sca)"></i>SCA</span>
                        <span class="sw"><i style="background:var(--c-other)"></i>Other</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Issues fixed by scan type">
                        <div class="hlabel">SAST</div>
                        <div class="hbar" style="--pct:74; --fill-color:var(--c-sast)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">74% (8,436)</div>
                        <div class="hlabel">DAST</div>
                        <div class="hbar" style="--pct:68; --fill-color:var(--c-dast)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">68% (4,692)</div>
                        <div class="hlabel">SCA</div>
                        <div class="hbar" style="--pct:81; --fill-color:var(--c-sca)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">81% (7,128)</div>
                        <div class="hlabel">Other</div>
                        <div class="hbar" style="--pct:59; --fill-color:var(--c-other)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">59% (1,151)</div>
                        <div class="hlabel"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            Total</div>
                        <div style="padding-top:8px; border-top:1px solid var(--border); margin-top:8px;"></div>
                        <div class="hmeta"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            73% (21,407)</div>
                    </div>
                </div>
            </div>

            <!-- ROW 2: Mean Time to Remediate (Overall) + MTTR by Severity -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>Mean Time to Remediate</h3>
                    <div class="kpi">14.2 <span class="muted">days</span></div>
                    <span class="delta negative" aria-label="Down 18 percent year over year" title="-18% vs. last year">
                        <i>â–¼</i> 18% YoY
                    </span>
                </div>
                <div class="card col-8">
                    <h3>Mean Time to Remediate by Severity</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Mean time to remediate by severity">
                        <div class="hlabel">Critical</div>
                        <div class="hbar" style="--pct:23; --fill-color:var(--sev-critical)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">5.8d <span class="delta negative"
                                style="font-size:10px; padding:0 4px;">â–¼12%</span></div>
                        <div class="hlabel">High</div>
                        <div class="hbar" style="--pct:38; --fill-color:var(--sev-high)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">9.7d <span class="delta negative"
                                style="font-size:10px; padding:0 4px;">â–¼15%</span></div>
                        <div class="hlabel">Medium</div>
                        <div class="hbar" style="--pct:68; --fill-color:var(--sev-medium)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">17.2d <span class="delta negative"
                                style="font-size:10px; padding:0 4px;">â–¼22%</span></div>
                        <div class="hlabel">Low</div>
                        <div class="hbar" style="--pct:100; --fill-color:var(--sev-low)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">25.3d <span class="delta negative"
                                style="font-size:10px; padding:0 4px;">â–¼8%</span></div>
                    </div>
                </div>
            </div>

            <!-- ROW 3: Remediated Issues (Monthly, grouped bar chart with axes) -->
            <div class="grid row">
                <div class="card col-12">
                    <h3>Issue Remediation Trend</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="trend-grouped">
                        <div class="y-axis-label">Number of Issues</div>
                        <div class="y-axis">
                            <div>500</div>
                            <div>400</div>
                            <div>300</div>
                            <div>200</div>
                            <div>100</div>
                            <div>0</div>
                        </div>
                        <div class="chart-area">
                            <!-- Jan -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:40px; background:var(--sev-critical)" title="Critical: 80"></div>
                                <div class="grouped-bar" style="height:60px; background:var(--sev-high)" title="High: 120"></div>
                                <div class="grouped-bar" style="height:90px; background:var(--sev-medium)" title="Medium: 180"></div>
                                <div class="grouped-bar" style="height:110px; background:var(--sev-low)" title="Low: 220"></div>
                            </div>
                            <!-- Feb -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:55px; background:var(--sev-critical)" title="Critical: 110"></div>
                                <div class="grouped-bar" style="height:70px; background:var(--sev-high)" title="High: 140"></div>
                                <div class="grouped-bar" style="height:105px; background:var(--sev-medium)" title="Medium: 210"></div>
                                <div class="grouped-bar" style="height:130px; background:var(--sev-low)" title="Low: 260"></div>
                            </div>
                            <!-- Mar -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:45px; background:var(--sev-critical)" title="Critical: 90"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--sev-high)" title="High: 160"></div>
                                <div class="grouped-bar" style="height:120px; background:var(--sev-medium)" title="Medium: 240"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-low)" title="Low: 190"></div>
                            </div>
                            <!-- Apr -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:65px; background:var(--sev-critical)" title="Critical: 130"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-high)" title="High: 190"></div>
                                <div class="grouped-bar" style="height:85px; background:var(--sev-medium)" title="Medium: 170"></div>
                                <div class="grouped-bar" style="height:140px; background:var(--sev-low)" title="Low: 280"></div>
                            </div>
                            <!-- May -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:60px; background:var(--sev-critical)" title="Critical: 120"></div>
                                <div class="grouped-bar" style="height:75px; background:var(--sev-high)" title="High: 150"></div>
                                <div class="grouped-bar" style="height:100px; background:var(--sev-medium)" title="Medium: 200"></div>
                                <div class="grouped-bar" style="height:120px; background:var(--sev-low)" title="Low: 240"></div>
                            </div>
                            <!-- Jun -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:50px; background:var(--sev-critical)" title="Critical: 100"></div>
                                <div class="grouped-bar" style="height:65px; background:var(--sev-high)" title="High: 130"></div>
                                <div class="grouped-bar" style="height:115px; background:var(--sev-medium)" title="Medium: 230"></div>
                                <div class="grouped-bar" style="height:105px; background:var(--sev-low)" title="Low: 210"></div>
                            </div>
                            <!-- Jul -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:70px; background:var(--sev-critical)" title="Critical: 140"></div>
                                <div class="grouped-bar" style="height:85px; background:var(--sev-high)" title="High: 170"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-medium)" title="Medium: 190"></div>
                                <div class="grouped-bar" style="height:145px; background:var(--sev-low)" title="Low: 290"></div>
                            </div>
                            <!-- Aug -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:55px; background:var(--sev-critical)" title="Critical: 110"></div>
                                <div class="grouped-bar" style="height:90px; background:var(--sev-high)" title="High: 180"></div>
                                <div class="grouped-bar" style="height:110px; background:var(--sev-medium)" title="Medium: 220"></div>
                                <div class="grouped-bar" style="height:125px; background:var(--sev-low)" title="Low: 250"></div>
                            </div>
                            <!-- Sep -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:75px; background:var(--sev-critical)" title="Critical: 150"></div>
                                <div class="grouped-bar" style="height:80px; background:var(--sev-high)" title="High: 160"></div>
                                <div class="grouped-bar" style="height:100px; background:var(--sev-medium)" title="Medium: 200"></div>
                                <div class="grouped-bar" style="height:135px; background:var(--sev-low)" title="Low: 270"></div>
                            </div>
                            <!-- Oct -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:60px; background:var(--sev-critical)" title="Critical: 120"></div>
                                <div class="grouped-bar" style="height:70px; background:var(--sev-high)" title="High: 140"></div>
                                <div class="grouped-bar" style="height:125px; background:var(--sev-medium)" title="Medium: 250"></div>
                                <div class="grouped-bar" style="height:115px; background:var(--sev-low)" title="Low: 230"></div>
                            </div>
                            <!-- Nov -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:65px; background:var(--sev-critical)" title="Critical: 130"></div>
                                <div class="grouped-bar" style="height:95px; background:var(--sev-high)" title="High: 190"></div>
                                <div class="grouped-bar" style="height:105px; background:var(--sev-medium)" title="Medium: 210"></div>
                                <div class="grouped-bar" style="height:150px; background:var(--sev-low)" title="Low: 300"></div>
                            </div>
                            <!-- Dec -->
                            <div class="month-group">
                                <div class="grouped-bar" style="height:80px; background:var(--sev-critical)" title="Critical: 160"></div>
                                <div class="grouped-bar" style="height:85px; background:var(--sev-high)" title="High: 170"></div>
                                <div class="grouped-bar" style="height:120px; background:var(--sev-medium)" title="Medium: 240"></div>
                                <div class="grouped-bar" style="height:130px; background:var(--sev-low)" title="Low: 260"></div>
                            </div>
                        </div>
                        <div class="x-axis">
                            <div class="month-label">Jan</div>
                            <div class="month-label">Feb</div>
                            <div class="month-label">Mar</div>
                            <div class="month-label">Apr</div>
                            <div class="month-label">May</div>
                            <div class="month-label">Jun</div>
                            <div class="month-label">Jul</div>
                            <div class="month-label">Aug</div>
                            <div class="month-label">Sep</div>
                            <div class="month-label">Oct</div>
                            <div class="month-label">Nov</div>
                            <div class="month-label">Dec</div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- ROW 4: % Issues Reviewed by Severity + % Issues Reviewed by Scan Type -->
            <div class="grid row">
                <div class="card col-6">
                    <h3>Reviewed Issues by Severity</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--sev-critical)"></i>Critical</span>
                        <span class="sw"><i style="background:var(--sev-high)"></i>High</span>
                        <span class="sw"><i style="background:var(--sev-medium)"></i>Medium</span>
                        <span class="sw"><i style="background:var(--sev-low)"></i>Low</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Issues reviewed by severity">
                        <div class="hlabel">Critical</div>
                        <div class="hbar" style="--pct:94; --fill-color:var(--sev-critical)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">94% (2,914)</div>
                        <div class="hlabel">High</div>
                        <div class="hbar" style="--pct:89; --fill-color:var(--sev-high)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">89% (4,628)</div>
                        <div class="hlabel">Medium</div>
                        <div class="hbar" style="--pct:76; --fill-color:var(--sev-medium)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">76% (6,308)</div>
                        <div class="hlabel">Low</div>
                        <div class="hbar" style="--pct:58; --fill-color:var(--sev-low)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">58% (8,004)</div>
                        <div class="hlabel"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            Total</div>
                        <div style="padding-top:8px; border-top:1px solid var(--border); margin-top:8px;"></div>
                        <div class="hmeta"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            74% (21,854)</div>
                    </div>
                </div>

                <div class="card col-6">
                    <h3>Reviewed Issues by Scan Type</h3>
                    <div class="legend">
                        <span class="sw"><i style="background:var(--c-sast)"></i>SAST</span>
                        <span class="sw"><i style="background:var(--c-dast)"></i>DAST</span>
                        <span class="sw"><i style="background:var(--c-sca)"></i>SCA</span>
                        <span class="sw"><i style="background:var(--c-other)"></i>Other</span>
                    </div>
                    <div class="hbars" role="group" aria-label="Issues reviewed by scan type">
                        <div class="hlabel">SAST</div>
                        <div class="hbar" style="--pct:86; --fill-color:var(--c-sast)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">86% (9,804)</div>
                        <div class="hlabel">DAST</div>
                        <div class="hbar" style="--pct:79; --fill-color:var(--c-dast)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">79% (5,451)</div>
                        <div class="hlabel">SCA</div>
                        <div class="hbar" style="--pct:71; --fill-color:var(--c-sca)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">71% (6,247)</div>
                        <div class="hlabel">Other</div>
                        <div class="hbar" style="--pct:64; --fill-color:var(--c-other)">
                            <div class="fill"></div>
                        </div>
                        <div class="hmeta">64% (352)</div>
                        <div class="hlabel"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            Total</div>
                        <div style="padding-top:8px; border-top:1px solid var(--border); margin-top:8px;"></div>
                        <div class="hmeta"
                            style="font-weight:600; padding-top:8px; border-top:1px solid var(--border); margin-top:8px;">
                            74% (21,854)</div>
                    </div>
                </div>
            </div>

            <!-- ROW 5: Mean Time to Review + % Issues Reopened + Top 5 Reopened Categories -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>Mean Time to Review</h3>
                    <div class="kpi">2.8 <span class="muted">days</span></div>
                    <span class="delta negative" aria-label="Down 24 percent year over year" title="-24% vs. last year">
                        <i>â–¼</i> 24% YoY
                    </span>
                </div>
                <div class="card col-4">
                    <h3>Vulnerability Recurrence Rate</h3>
                    <div class="donut-wrap">
                        <div class="donut" style="--pct:8; --ring-color:var(--status-error)"></div>
                        <div>
                            <div class="kpi">8% <span class="muted">reopened</span></div>
                            <div class="muted">92% remediated permanently</div>
                            <div class="legend" aria-hidden="true">
                                <span class="sw"><i style="background:var(--status-error)"></i>Reopened</span>
                                <span class="sw"><i style="background:var(--track)"></i>Remediated</span>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card col-4">
                    <h3>Most Reopened Vulnerabilities</h3>
                    <table>
                        <thead>
                            <tr>
                                <th>Category</th>
                                <th style="text-align:right;">Count</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SQL Injection</td>
                                <td style="text-align:right">142</td>
                            </tr>
                            <tr>
                                <td>Cross-Site Scripting (XSS)</td>
                                <td style="text-align:right">118</td>
                            </tr>
                            <tr>
                                <td>Command Injection</td>
                                <td style="text-align:right">97</td>
                            </tr>
                            <tr>
                                <td>Insecure Deserialization</td>
                                <td style="text-align:right">84</td>
                            </tr>
                            <tr>
                                <td>XML External Entity (XXE)</td>
                                <td style="text-align:right">76</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- ROW 6: SAST Aviator Metrics -->
            <div class="grid row">
                <div class="card col-4">
                    <h3>SAST Aviator Adoption Rate</h3>
                    <div class="donut-wrap">
                        <div class="donut" style="--pct:67; --ring-color:#1aa364"></div>
                        <div>
                            <div class="kpi">67% <span class="muted">enabled</span></div>
                            <div class="muted">83 of 124 applications</div>
                            <div class="legend" aria-hidden="true">
                                <span class="sw"><i style="background:#1aa364"></i>Enabled</span>
                                <span class="sw"><i style="background:var(--track)"></i>Not enabled</span>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="card col-8">
                    <h3>SAST Aviator Return on Investment (ROI)</h3>
                    <div class="kpi-row">
                        <div>
                            <div class="kpi">$891,025 <span class="muted">savings</span></div>
                            <div class="muted" style="margin-top:8px;">Based on development rate of $50/hr
                            </div>
                        </div>
                        <div style="display:flex; gap:24px; margin-top:16px;">
                            <div>
                                <div style="font-size:14px; color:var(--text-muted);">Review time saved</div>
                                <div style="font-size:20px; font-weight:600; color:var(--text); margin-top:4px;">15 min/issue
                                </div>
                            </div>
                            <div>
                                <div style="font-size:14px; color:var(--text-muted);">Issues auto-reviewed</div>
                                <div style="font-size:20px; font-weight:600; color:var(--text); margin-top:4px;">42,630
                                </div>
                            </div>
                            <div>
                                <div style="font-size:14px; color:var(--text-muted);">Remediation time saved</div>
                                <div style="font-size:20px; font-weight:600; color:var(--text); margin-top:4px;">30 min/issue
                                </div>
                            </div>
                            <div>
                                <div style="font-size:14px; color:var(--text-muted);">Issues fixed</div>
                                <div style="font-size:20px; font-weight:600; color:var(--text); margin-top:4px;">14,326
                                </div>
                            </div>
                            <div>
                                <div style="font-size:14px; color:var(--text-muted);">Developer hours saved</div>
                                <div style="font-size:20px; font-weight:600; color:var(--text); margin-top:4px;">17,821 hrs
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- KPI Analysis (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ“Š Remediation Dashboard KPI Analysis</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>Target Personas:</strong> Director of Application Security, Application Security Engineers, Development Team Leads, Engineering Managers</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Remediated Issues by Severity - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of discovered issues successfully fixed, broken down by severity. Shows priority discipline (82% high > 71% medium > 64% low). Total: 21,407 issues remediated.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Program effectiveness indicatorâ€”78% critical fixed demonstrates strong prioritization. Validates security investment ROI for executives.</li>
                                <li><strong>Engineers:</strong> Workload completion tracking. 73% overall fix rate is solid but 27% backlog (open issues) needs attention. Priority discipline validation.</li>
                                <li><strong>Dev Managers:</strong> Team performance metric showing security hygiene. High fix rates = security culture adoption.</li>
                                <li><strong>Insights gained:</strong> Priority discipline effectiveness, remediation capacity, security culture health, SLA compliance.</li>
                                <li><strong>Actions driven:</strong> Backlog reduction campaigns, resource allocation decisions, celebrating high performers, investigating low fix-rate teams.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Excellent outcome metric showing actual risk reduction. The severity-based breakdown proves priority discipline (high-severity issues get fixed faster/more often). 21,407 remediations is impressive scale. Demonstrates program effectiveness better than activity metrics. Loses 1 point for lacking time dimension (trend) and because fix rates include all severities equally (fixing 100 low = fixing 1 critical in this metric).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>False positive inflation:</strong> "Fixed" includes marking issues as false positivesâ€”might inflate fix rates without reducing real risk.</li>
                                <li><strong>Denominator ambiguity:</strong> 78% of what? All-time discovered criticals or current backlog? Time window unclear.</li>
                                <li><strong>No trend data:</strong> Is 78% improving or declining? Historical context missing.</li>
                                <li><strong>Effort equivalence:</strong> Fixing 8,832 low issues looks impressive but may represent less security value than 2,418 critical.</li>
                                <li><strong>Gaming potential:</strong> Teams might close issues as "won't fix" or "false positive" to boost percentages.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Remediated Issues by Scan Type - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Fix rate by tool type. SCA leads at 81% (easiestâ€”dependency upgrades). DAST lowest at 68% (hardestâ€”architectural changes). Total: 21,407 remediations.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Tool-specific effectiveness indicator. High SCA fix rate justifies investment. Low DAST fix rate = resource need or training gap.</li>
                                <li><strong>Engineers:</strong> Identifies which tool findings are actionable vs. noisy. 68% DAST might indicate false positives, complexity, or lack of expertise.</li>
                                <li><strong>Insights gained:</strong> Tool noise levels, remediation difficulty by finding type, training needs, tool ROI comparison.</li>
                                <li><strong>Actions driven:</strong> Tool tuning to reduce noise, specialized training (e.g., DAST remediation workshops), resource allocation by tool type.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Useful for identifying tool-specific challenges and ROI. The variance (81% SCA vs. 59% Other) reveals actionability differences. Helps prioritize tool investments and training. However, tool categorization can be misleading (SCA might just be easier, not better). Lacks context on why fix rates differ. More diagnostic than strategic.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Difficulty conflation:</strong> SCA high fix rate might just mean "easier" (update dependency) vs. SAST (refactor code)â€”not tool quality.</li>
                                <li><strong>Volume imbalance:</strong> 8,436 SAST remediations vs. 1,151 Otherâ€”percentages hide effort concentration.</li>
                                <li><strong>Tool overlap:</strong> Some issues found by multiple toolsâ€”how are they categorized? Double-counting risk.</li>
                                <li><strong>Noise signal unclear:</strong> Low fix rates might indicate tool noise OR genuinely hard-to-fix issues. Needs investigation.</li>
                                <li><strong>No severity breakdown:</strong> Does DAST's 68% mean fixing low issues or high issues? Impact unclear.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. Mean Time to Remediate - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Average time from issue discovery to fix deployment. 14.2 days overall, improving 18% year-over-year. Core velocity metric.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> Executive-level program effectiveness metric. 14.2 days shows mature velocity. â–¼18% YoY proves continuous improvementâ€”perfect for board reports.</li>
                                <li><strong>Engineers:</strong> Benchmarking metric for process efficiency. 14.2 days indicates good automation and developer responsiveness.</li>
                                <li><strong>Dev Managers:</strong> Team velocity indicator. Can compare teams against 14.2-day baseline to identify high/low performers.</li>
                                <li><strong>Insights gained:</strong> Process efficiency, automation effectiveness, developer engagement, program maturity, competitive positioning.</li>
                                <li><strong>Actions driven:</strong> Process optimization, automation investments, celebrating improvements, benchmarking against industry standards (MTTR typically 30-60 days).</li>
                            </ul>
                            <p><strong>Value Rating: 10/10</strong><br><em>Reasoning:</em> Perfect KPIâ€”universally understood velocity metric with clear business value. Faster MTTR = lower risk exposure window. The â–¼18% YoY trend shows continuous improvement. 14.2 days is excellent (industry average 30-60 days). Actionable, trendable, benchmarkable. Directly correlates to breach risk reduction. Single number that executives, engineers, and developers all understand and value. This is THE north-star metric for remediation effectiveness.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Clock start ambiguity:</strong> MTTR from discovery, triage, or assignment? Different clocks = different meanings.</li>
                                <li><strong>Clock stop gaming:</strong> Teams might mark issues "fixed" in dev but delay production deploymentâ€”gaming the metric.</li>
                                <li><strong>Severity blending:</strong> Average includes all severitiesâ€”critical MTTR matters more than low. Needs breakdown (see next KPI).</li>
                                <li><strong>Outlier sensitivity:</strong> Mean vulnerable to outliersâ€”one 365-day fix skews average. Median might be more robust.</li>
                                <li><strong>False positive masking:</strong> Issues closed as "false positive" improve MTTR without fixing real vulnerabilities.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Mean Time to Remediate by Severity - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> MTTR broken down by severity with YoY improvements. Critical: 5.8d (â–¼12%), High: 9.7d (â–¼15%), Medium: 17.2d (â–¼22%), Low: 25.3d (â–¼8%). Shows perfect priority discipline.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> SLA compliance proof. 5.8 days for critical is excellent (typical SLAs: 7-15 days). Shows priority discipline working. All trends improving = program excellence.</li>
                                <li><strong>Engineers:</strong> SLA target tracking. Validates prioritization framework effectiveness. Can identify which severity levels need attention.</li>
                                <li><strong>Compliance:</strong> Demonstrates SLA compliance for audits, cyber insurance, regulatory requirements. Evidence of risk management maturity.</li>
                                <li><strong>Insights gained:</strong> SLA compliance, priority discipline, process maturity, continuous improvement culture, resource allocation effectiveness.</li>
                                <li><strong>Actions driven:</strong> SLA policy refinement, resource reallocation, celebrating team performance, industry benchmarking, audit reporting.</li>
                            </ul>
                            <p><strong>Value Rating: 10/10</strong><br><em>Reasoning:</em> Perfect metricâ€”multi-dimensional MTTR showing priority discipline. The gradient (5.8d â†’ 25.3d by severity) proves teams prioritize correctly. All four severities improving YoY demonstrates continuous improvement. 5.8 days for critical is world-class. Directly supports SLA compliance and audit requirements. Essential for cyber insurance. This breakdown transforms MTTR from good to exceptional by adding priority context. Absolute best-in-class remediation metric.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Clock definition consistency:</strong> Must use same start/stop clock across all severities for valid comparison.</li>
                                <li><strong>Threshold gaming:</strong> Teams might downgrade severity to improve MTTR metrics (critical â†’ high to hit 5.8d target).</li>
                                <li><strong>Volume imbalance:</strong> More low-severity issues might skew averagesâ€”median or P50/P90 percentiles better for outlier resistance.</li>
                                <li><strong>SLA mismatch:</strong> Visual display doesn't show SLA targetsâ€”users can't tell if 5.8d meets policy without external context.</li>
                                <li><strong>Environment ambiguity:</strong> MTTR to dev/test or production? Production deployment matters more for risk.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Remediated Issues (12 Month trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Monthly volume of fixed issues by severity over 12 months, showing remediation throughput patterns and consistency.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Capacity planning metric showing team throughput consistency. Stable bars = predictable velocity. Dips indicate resource constraints or holidays.</li>
                                <li><strong>Engineers:</strong> Workload trends and seasonal patterns. Helps forecast capacity for upcoming sprints or releases.</li>
                                <li><strong>Insights gained:</strong> Team capacity, seasonal patterns, resource adequacy, remediation consistency, process stability.</li>
                                <li><strong>Actions driven:</strong> Capacity planning, resource augmentation during peak periods, investigating throughput dips, celebrating high-output months.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Solid operational health metric showing remediation consistency. Steady throughput indicates mature processes. Useful for capacity planning and spotting anomalies. However, volume doesn't equal valueâ€”1,000 low fixes â‰  100 critical fixes. Lacks context on backlog growth (fixes might be outpaced by new issues). Better as operational diagnostic than strategic metric.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Volume â‰  value:</strong> High fix counts might be low-severity issuesâ€”doesn't show risk reduction impact.</li>
                                <li><strong>No backlog context:</strong> Fixing 1,000 issues/month is meaningless if discovering 2,000/monthâ€”net backlog grows.</li>
                                <li><strong>Scale ambiguity:</strong> No Y-axis labelsâ€”is this 50 fixes/month or 5,000? Relative heights insufficient for planning.</li>
                                <li><strong>Seasonality unexplained:</strong> Dips might be holidays (expected) or process breakdowns (concerning)â€”needs annotation.</li>
                                <li><strong>False positive inclusion:</strong> "Remediated" might include closing as false positiveâ€”inflates throughput without risk reduction.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. Reviewed Issues by Severity - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of discovered issues that have been triaged/reviewed by security team. 94% critical reviewed = excellent coverage. Total: 21,854 issues reviewed.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Triage capacity indicator. 94% critical coverage shows team keeping pace with findings. 26% unreviewed backlog needs attention.</li>
                                <li><strong>Engineers:</strong> Workload visibilityâ€”review queue status. High review rates = low false positive noise reaching developers.</li>
                                <li><strong>Insights gained:</strong> Triage capacity, backlog health, automation effectiveness (Aviator impact), team bandwidth.</li>
                                <li><strong>Actions driven:</strong> Triage capacity expansion, automation investments (AI triage), backlog reduction sprints, tooling tuning to reduce noise.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> Important process metric showing triage effectiveness. 94% critical review coverage is excellentâ€”ensures high-severity issues get expert attention. Priority gradient (94% â†’ 58%) proves sensible prioritization. 21,854 reviews is significant effort. However, "reviewed" doesn't mean "fixed"â€”just triaged. Gap between reviewed (74%) and remediated (73%) is interesting but unexplained. Loses 2 points for being process- rather than outcome-focused.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Review â‰  action:</strong> 94% reviewed doesn't mean 94% fixedâ€”just means someone looked at it. Outcome unclear.</li>
                                <li><strong>Review depth unknown:</strong> Does "review" mean 5-minute triage or 30-minute investigation? Quality varies.</li>
                                <li><strong>Backlog ambiguity:</strong> 26% unreviewedâ€”is this new findings awaiting review or old backlog being ignored?</li>
                                <li><strong>Comparison confusion:</strong> Review rates (74%) vs. fix rates (73%) are nearly identicalâ€”why? Needs explanation.</li>
                                <li><strong>Automation impact:</strong> With Aviator auto-triaging, what counts as "reviewed"â€”human review only or AI+human?</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Reviewed Issues by Scan Type - Value: 6/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Review completion rates by tool type. SAST highest at 86%, Other lowest at 64%. Shows triage prioritization by tool.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Identifies which tool findings are getting attention vs. being ignored. 71% SCA might indicate review capacity issues despite high fix rates.</li>
                                <li><strong>Engineers:</strong> Tool-specific triage workload visibility. Can identify tooling noise issues (low review rate = too noisy?).</li>
                                <li><strong>Insights gained:</strong> Tool prioritization, triage capacity by tool type, potential noise problems.</li>
                                <li><strong>Actions driven:</strong> Tool tuning, triage capacity reallocation, investigating why certain tools have lower review rates.</li>
                            </ul>
                            <p><strong>Value Rating: 6/10</strong><br><em>Reasoning:</em> Moderately useful diagnostic metric showing tool-specific triage patterns. Variance (86% SAST vs. 64% Other) reveals prioritization or capacity issues. However, review rates by tool are less actionable than by severity. Tool categorization can be arbitrary. This metric feels like it's duplicating insights from other KPIs without adding much new value. Better as operational diagnostic than dashboard KPI.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Limited actionability:</strong> What should users do with this data? Tool-specific review rates are less clear-cut than severity-based.</li>
                                <li><strong>Overlap with fix rates:</strong> Very similar to "Remediated by Scan Type"â€”redundant insights.</li>
                                <li><strong>Volume imbalance hidden:</strong> 86% SAST (9,804 issues) vs. 64% Other (352 issues)â€”percentages hide massive effort difference.</li>
                                <li><strong>Tool noise conflation:</strong> Low review rate might mean "too noisy to review" OR "low priority"â€”ambiguous signal.</li>
                                <li><strong>Dashboard clutter:</strong> This KPI might be better in a detailed operational view rather than executive dashboard.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Mean Time to Review - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Average time from issue discovery to security team triage completion. 2.8 days with 24% year-over-year improvement.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Triage velocity metric. 2.8 days is excellentâ€”shows responsive security team. â–¼24% improvement likely from Aviator automation (67% adoption).</li>
                                <li><strong>Engineers:</strong> Developer experience metric. Fast triage (2.8d) means developers get feedback quickly, enabling rapid iteration.</li>
                                <li><strong>Insights gained:</strong> Triage efficiency, automation ROI (Aviator impact), team responsiveness, developer experience quality.</li>
                                <li><strong>Actions driven:</strong> Automation expansion, process optimization, celebrating triage team performance, capacity planning.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Excellent velocity metric showing triage efficiency. 2.8 days is outstanding (industry typical: 7-14 days). The â–¼24% YoY improvement is dramatic and likely correlates with Aviator adoption (67%). Fast triage enables faster remediation (2.8d triage + 14.2d fix = competitive cycle time). Directly impacts developer experience. Only loses 1 point because triage speed alone doesn't guarantee qualityâ€”fast but wrong triage is counterproductive.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Speed vs. quality tradeoff:</strong> Fast triage might sacrifice accuracyâ€”needs paired with false positive rates.</li>
                                <li><strong>Automation masking:</strong> With 67% Aviator adoption, is 2.8d reflecting AI speed or human speed? Need to separate.</li>
                                <li><strong>Severity blending:</strong> Average across all severitiesâ€”critical should be triaged faster than low. Needs breakdown.</li>
                                <li><strong>Clock definition:</strong> When does "review" clock start? Issue creation or assignment to security team?</li>
                                <li><strong>Gaming potential:</strong> Quick "looks good" review without deep analysis improves metric but degrades quality.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--status-error);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Reopen Rate - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of "fixed" issues that reappear after closure. 8% reopen rate means 92% of fixes are durable/permanent.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Fix quality indicator. 8% reopen is acceptable (industry typical: 10-15%). Shows fixes address root causes, not just symptoms.</li>
                                <li><strong>Engineers:</strong> Identifies fix quality issues. High reopen rates indicate incomplete fixes, incorrect remediation, or process problems.</li>
                                <li><strong>Dev Managers:</strong> Code quality and testing effectiveness metric. Reopens often indicate insufficient testing or rushed fixes.</li>
                                <li><strong>Insights gained:</strong> Fix durability, root cause analysis effectiveness, testing adequacy, developer skill level, rushing behavior.</li>
                                <li><strong>Actions driven:</strong> Root cause analysis training, testing improvements, investigating high-reopen categories (see next KPI), code review process enhancement.</li>
                            </ul>
                            <p><strong>Value Rating: 9/10</strong><br><em>Reasoning:</em> Exceptional quality metric measuring fix durability. 8% reopen is healthyâ€”proves fixes address root causes. This metric prevents gaming "remediated issues" by tracking if fixes actually stick. Directly measures engineering quality and process effectiveness. Essential for understanding true risk reduction vs. superficial fixes. Loses 1 point for lacking trend data and category breakdown (which issues reopen most?).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Time window ambiguity:</strong> Reopen within what timeframe? 30 days? 90 days? One year? Window matters.</li>
                                <li><strong>Root cause ambiguity:</strong> Why do issues reopen? Incomplete fix? New code reintroducing vulnerability? Regression? Needs investigation.</li>
                                <li><strong>No trend data:</strong> Is 8% improving or worsening? Historical context missing.</li>
                                <li><strong>Threshold uncertainty:</strong> Is 8% good or bad? Industry benchmarks needed for context.</li>
                                <li><strong>Reopened vs. recurrent:</strong> Is this same instance reopening or new instance of same vulnerability class? Very different implications.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Most Reopened Categories - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Top 5 vulnerability categories that reappear after being "fixed." SQL Injection leads with 142 reopens, followed by XSS (118) and Command Injection (97).</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Identifies systemic remediation challenges. SQL Injection topping reopens = developers need better training on parameterized queries.</li>
                                <li><strong>Engineers:</strong> Targeted improvement areasâ€”focus training, tooling, or architectural solutions on these categories.</li>
                                <li><strong>Training Teams:</strong> Data-driven curriculum developmentâ€”emphasize proper remediation techniques for SQL Injection, XSS, Command Injection.</li>
                                <li><strong>Insights gained:</strong> Remediation skill gaps, training needs, architectural weaknesses, category-specific fix difficulty.</li>
                                <li><strong>Actions driven:</strong> Targeted training programs, secure coding standard updates, architectural pattern adoption (e.g., ORMs), remediation guidebooks.</li>
                            </ul>
                            <p><strong>Value Rating: 8/10</strong><br><em>Reasoning:</em> High-value diagnostic metric identifying specific remediation challenges. Transforms reopen rate from abstract percentage to actionable intelligence. SQL Injection leading reopens suggests training or tooling gaps. Enables targeted interventions rather than generic "improve quality" goals. Directly informs training investments. Loses 2 points for lacking context (is 142 reopens from 200 fixes or 2,000 fixes? Rate matters).</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>No rate context:</strong> 142 SQL Injection reopens from how many total SQL Injection fixes? Absolute numbers without denominators are misleading.</li>
                                <li><strong>Volume bias:</strong> Categories with more issues naturally have more reopensâ€”needs normalization (reopen rate %).</li>
                                <li><strong>Root cause unknown:</strong> Why do these categories reopen? Incomplete fixes? Hard to fix properly? Architectural issues?</li>
                                <li><strong>No trend data:</strong> Are these improving or worsening? Historical context missing.</li>
                                <li><strong>Limited scope:</strong> Top 5 onlyâ€”might miss emerging problems in positions 6-10.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid #1aa364;">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">11. SAST Aviator Adoption Rate - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Percentage of applications with AI-powered auto-triage enabled. 67% adoption means 83 of 124 applications use Aviator for automated issue review.</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director:</strong> Innovation adoption metric. 67% is solid but 33% (41 apps) still need onboarding. Shows AI tool rollout progress.</li>
                                <li><strong>Engineers:</strong> Identifies which teams/apps aren't using automationâ€”onboarding opportunity. Can correlate adoption with MTTR improvements.</li>
                                <li><strong>Insights gained:</strong> Automation adoption, change resistance pockets, onboarding effectiveness, innovation culture.</li>
                                <li><strong>Actions driven:</strong> Onboarding campaigns for remaining 41 apps, showcasing ROI to drive adoption, investigating adoption barriers.</li>
                            </ul>
                            <p><strong>Value Rating: 7/10</strong><br><em>Reasoning:</em> Important adoption metric for modern AI-powered tools. 67% is respectable but shows room for growth. Directly correlates with efficiency gains (see next KPI). However, adoption alone doesn't prove valueâ€”need paired with ROI metrics. Loses 3 points because: (1) adoption â‰  effectiveness, (2) lacks trend (is adoption growing?), (3) doesn't show which 33% aren't adopting and why.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Adoption â‰  value:</strong> 67% enabled doesn't mean 67% actively using or benefitingâ€”might be enabled but ignored.</li>
                                <li><strong>No trend data:</strong> Is adoption growing or stagnant? Velocity of rollout matters.</li>
                                <li><strong>Resistance unexplored:</strong> Why haven't 33% adopted? Technical barriers? Trust issues? Lack of awareness? Needs investigation.</li>
                                <li><strong>Accuracy unknown:</strong> High adoption is meaningless if Aviator makes poor triage decisionsâ€”needs accuracy metrics.</li>
                                <li><strong>Comparison limitation:</strong> Can't compare Aviator vs. non-Aviator outcomes from this metric aloneâ€”needs paired data.</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid #1aa364;">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">12. SAST Aviator ROI - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>What it displays:</strong> Financial ROI from AI-powered auto-triage and auto-remediation: $891,025 annual time savings, 42,630 issues auto-reviewed, 14,326 issues auto-fixed, 17,821 developer hours saved. Breaks down savings into review time (15 min/issue) and remediation time (30 min/issue).</p>
                            <p><strong>How personas use it:</strong></p>
                            <ul>
                                <li><strong>Director/CISO:</strong> Budget justification gold mine. $891K annual savings proves AppSec program ROI. Essential for executive/board presentations and tool renewal justifications. Demonstrates both triage automation (42,630 issues) and remediation automation (14,326 fixes).</li>
                                <li><strong>Engineers:</strong> Demonstrates automation value across full security lifecycleâ€”15 min saved on review + 30 min saved on fix = 45 min total per auto-fixed issue. Validates AI tool investment with tangible productivity gains.</li>
                                <li><strong>Finance/CFO:</strong> Hard dollar savings for cost-benefit analysis. $891K savings vs. tool cost = clear ROI calculation. Multi-dimensional view (review + remediation) shows comprehensive automation impact.</li>
                                <li><strong>Insights gained:</strong> Dual automation ROI (triage + fix), productivity gains at scale (42K+ issues processed), tool investment justification, time-to-value measurement, capacity creation for high-value security work.</li>
                                <li><strong>Actions driven:</strong> Budget approvals for AI tools, expanding Aviator adoption (67%â†’100%), tool renewal justifications, showcasing AppSec program value, justifying headcount reallocation to strategic initiatives.</li>
                            </ul>
                            <p><strong>Value Rating: 10/10</strong><br><em>Reasoning:</em> Perfect executive KPIâ€”translates technical automation into business value ($$$). $891K annual savings is compelling for any CFO/board. Multi-dimensional view (dollars, hours, issues reviewed, issues fixed, time per issue) supports various stakeholder perspectives. Shows end-to-end automation value: 42,630 issues auto-reviewed + 14,326 auto-fixed = comprehensive security automation. Directly proves AppSec program ROI and tool investment wisdom. Essential for budget justification and program advocacy. This metric alone justifies entire dashboard. Transforms AppSec from "cost center" to "value creator." Higher savings than typical triage-only metrics because it captures both review and remediation automation.</p>
                            <p><strong>Potential Issues:</strong></p>
                            <ul>
                                <li><strong>Calculation methodology transparency:</strong> $891K based on $50/hr development rate. Review savings: 42,630 issues Ã— 15 min Ã— $50/hr = $532K. Remediation savings: 14,326 issues Ã— 30 min Ã— $50/hr = $358K. Total = $891K. Hourly rate assumptions must be clearly stated for credibilityâ€”some orgs use $100+/hr (inflates savings), others use $35/hr (deflates). Recommend showing formula in dashboard tooltip.</li>
                                <li><strong>Auto-review vs auto-fix confusion:</strong> 42,630 issues auto-reviewed but only 14,326 auto-fixedâ€”where did 28,304 issues go? Needs clarity: Not all auto-reviewed issues are fixable by AI (e.g., architectural flaws, business logic issues require human judgment). The 33.6% auto-fix rate (14,326/42,630) should be called out as a benchmark.</li>
                                <li><strong>Accuracy assumptions:</strong> ROI assumes Aviator triage/fix recommendations are correct. If 30% are false positives or incorrect fixes, savings evaporate and may create rework costs. Dashboard should show Aviator accuracy rate (e.g., "95% triage accuracy, 88% fix accuracy") alongside ROI to validate savings claims.</li>
                                <li><strong>Time savings variance:</strong> "15 min review, 30 min fix" are averages with high variance. Simple config issues: 2 min review + 5 min fix. Complex SQL injection: 45 min review + 4 hours fix. Averages mask this distributionâ€”consider showing median instead of mean, or breaking down by vulnerability severity/category.</li>
                                <li><strong>Opportunity cost validation:</strong> Hours "saved" (17,821) must be redeployed productivelyâ€”if developers waste freed time on non-security tasks or it simply reduces overtime, business value doesn't materialize. Consider tracking: "Hours reallocated to [strategic security initiatives]" to prove value capture.</li>
                                <li><strong>Recurring vs one-time clarity:</strong> Is $891K annual recurring savings? If yes, multi-year ROI is $891K Ã— years. If one-time (e.g., backlog cleanup), clarify duration. Likely recurring based on "issues auto-reviewed" continuous flow, but needs explicit labeling.</li>
                                <li><strong>Baseline comparison missing:</strong> What would costs be WITHOUT Aviator? Dashboard shows savings but not baseline spend. Example: "Manual triage would cost $1.2M/year â†’ Aviator reduces to $309K â†’ $891K savings." This strengthens ROI narrative.</li>
                                <li><strong>Developer rate sensitivity:</strong> $50/hr is moderate (industry range: $35-150/hr depending on region/seniority). At $35/hr, savings drop to $624K. At $100/hr, savings jump to $1.78M. Consider making rate configurable or showing sensitivity analysis: "Savings range: $624K-$1.78M depending on labor costs."</li>
                            </ul>
                            
                            
                        </div>
                    </details>

                    <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid #1aa364; border-radius: 8px;">
                        <h4 style="margin-top: 0;">ðŸ“ˆ Summary Dashboard Value Rankings</h4>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px;">
                            <div>
                                <h5 style="color: #1aa364;">Highest Value (10/10) ðŸ†</h5>
                                <ol style="margin: 8px 0;">
                                    <li>Mean Time to Remediate (10/10)</li>
                                    <li>MTTR by Severity (10/10)</li>
                                    <li>SAST Aviator ROI (10/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--accent); margin-top: 16px;">Exceptional Value (9/10) â­</h5>
                                <ol start="4" style="margin: 8px 0;">
                                    <li>Remediated Issues by Severity (9/10)</li>
                                    <li>Mean Time to Review (9/10)</li>
                                    <li>Reopen Rate (9/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--accent); margin-top: 16px;">High Value (8/10)</h5>
                                <ol start="7" style="margin: 8px 0;">
                                    <li>Reviewed Issues by Severity (8/10)</li>
                                    <li>Most Reopened Categories (8/10)</li>
                                </ol>
                            </div>
                            <div>
                                <h5 style="color: var(--text-muted);">Good Value (7/10)</h5>
                                <ol start="9" style="margin: 8px 0;">
                                    <li>Remediated Issues by Scan Type (7/10)</li>
                                    <li>Remediated Issues (Monthly) (7/10)</li>
                                    <li>Aviator Adoption Rate (7/10)</li>
                                </ol>
                                
                                <h5 style="color: var(--text-muted); margin-top: 16px;">Medium Value (6/10)</h5>
                                <ol start="12" style="margin: 8px 0;">
                                    <li>Reviewed Issues by Scan Type (6/10)</li>
                                </ol>
                            </div>
                        </div>
                        <div style="margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border);">
                            <h5>ðŸŽ¯ Key Insights</h5>
                            <ul style="margin: 8px 0;">
                                <li><strong>MTTR is the north-star:</strong> 14.2 days overall with perfect priority discipline (5.8d critical â†’ 25.3d low) demonstrates world-class remediation velocity.</li>
                                <li><strong>ROI metrics transform perception:</strong> $487K Aviator savings translates technical excellence into business valueâ€”essential for budget justification.</li>
                                <li><strong>Quality metrics prevent gaming:</strong> Reopen rate (8%) ensures "remediated" means truly fixed, not just closed.</li>
                                <li><strong>Multi-dimensional velocity:</strong> MTTR + MTT-Review + Fix Rate + Reopen Rate provide comprehensive remediation health picture.</li>
                                <li><strong>AI automation impact:</strong> 67% Aviator adoption correlates with 24% YoY MTT-Review improvementâ€”clear automation ROI.</li>
                                <li><strong>Severity discipline everywhere:</strong> Every metric shows proper prioritization (critical > high > medium > low)â€”mature program.</li>
                            </ul>
                        </div>
                        <p style="margin-top: 16px; padding-top: 16px; border-top: 1px solid var(--border);"><strong>Overall Assessment:</strong> The Remediation Dashboard is exceptionalâ€”perfectly focused on velocity, quality, and ROI. MTTR metrics (overall and by severity) are world-class north-star KPIs. The ROI calculation ($487K savings) transforms AppSec from cost center to value creator, essential for executive buy-in. Quality metrics (reopen rate, most reopened categories) prevent gaming and ensure genuine risk reduction. This dashboard proves program effectiveness better than any otherâ€”it shows actual security outcomes, not just activity. Only minor improvement: add more trend data to show continuous improvement trajectories.</p>
                        </div>
                    </details>
                </div>
            </div>

            <!-- Design/Implementation Notes (Collapsible) -->
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">Remediation Dashboard Design/Implementation Notes</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>AI Generated Suggestions:</strong> Based on analysis of OpenAPI Specs for FoD/SSC and the Program Dashboard requirements. Consider this a starting point, not definitive. There will be errors.</p>
                            
                            <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                                <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. Remediated Issues by Severity - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate fix rate: (Closed/Remediated Issues / Total Discovered Issues) Ã— 100 for trailing 12 months. Group by severity. "Remediated" includes: Fixed (code changed), Mitigated (compensating controls), Risk Accepted (documented decision). Exclude: Reopened issues (count in reopen rate metric). Essential outcome metric showing actual risk reduction.</li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoints:</strong>
                                            <ul>
                                                <li>Numerator (Remediated): GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false+closedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                                <li>Denominator (Total Discovered): GET /api/v3/releases/{releaseId}/vulnerabilities?filters=introducedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                            </ul>
                                        </li>
                                        <li><strong>Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query closed vulnerabilities for trailing 12 months, group by severityString</li>
                                                <li>Count remediations per severity: Critical=2,418, High=4,264, Medium=5,893, Low=8,832</li>
                                                <li>Query all discovered vulnerabilities (introducedDate within 12 months), group by severity</li>
                                                <li>Count total discovered per severity (example: Critical discovered=3,100)</li>
                                                <li>Calculate fix rate per severity: (remediated / discovered) Ã— 100</li>
                                                <li>Example: Critical fix rate = (2,418 / 3,100) Ã— 100 = 78%</li>
                                            </ol>
                                        </li>
                                        <li><strong>Key Response Fields:</strong> closedDate (date-time), introducedDate (date-time), severityString (Critical/High/Medium/Low), isSuppressed, primaryLocationRemoved. Filter suppressions to exclude false positives and risk acceptances unless explicitly counting them.</li>
                                        <li><strong>Status Classification:</strong> Remediated = closedDate not null AND primaryLocationRemoved=false (actual fix, not code deletion). Include: Fixed (code changed), Mitigated (compensating control), Risk Accepted (if counting as resolved). Exclude: Suppressed issues (isSuppressed=true unless risk accepted), code deletions (primaryLocationRemoved=true), reopened issues (count separately in reopen rate).</li>
                                        <li><strong>Priority Gradient Validation:</strong> Fix rates should follow severity priority: Critical â‰¥ High â‰¥ Medium â‰¥ Low. Example gradient (78% â†’ 82% â†’ 71% â†’ 64%) shows generally good prioritization with slight High outlier (82% > 78% Critical may indicate easier High issues or Critical complexity). Inverted gradients flag priority discipline failures or severity misclassification.</li>
                                        <li><strong>Time Window Consistency:</strong> Use trailing 12-month window for both numerator and denominator. Date filters: closedDate:&gt;{12monthsAgo} for remediations, introducedDate:&gt;{12monthsAgo} for discoveries. Consistent window ensures fair fix rate calculationâ€”avoid comparing 12-month remediations to all-time discoveries (inflates rates).</li>
                                        <li><strong>Performance:</strong> Two separate queries (numerator/denominator) each requiring pagination. Cache results daily. For large portfolios (100+ releases), query in parallel per application and aggregate. Pre-compute monthly to track fix rate trends over time.</li>
                                        <li><strong>False Positive Handling:</strong> False positives inflate fix rates without risk reduction. Options: (1) Exclude entirely (isSuppressed:false filter), (2) Track separately as "FP Rate" metric, (3) Include but annotate. Best practice: Exclude from fix rate, report FP rate separately (FPs / Total) to show noise levels. High FP rates (>20%) indicate tool tuning needed.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/projectVersions?includeInactive=false&fields=id,name â†’ enumerate active project versions</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/issues?q=foundDate:[{12monthsAgo} TO *]&groupingtype=friority â†’ total discovered issues by severity</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/issues?q=foundDate:[{12monthsAgo} TO *]+removed:true&groupingtype=friority â†’ remediated issues by severity</li>
                                        <li>Calculate fix rate per severity: (remediatedCount / discoveredCount) Ã— 100</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize counters for discovered and remediated by severity</span>
discovered = {Critical: 0, High: 0, Medium: 0, Low: 0}
remediated = {Critical: 0, High: 0, Medium: 0, Low: 0}

<span style="color: #6a9955;">// Step 2: Calculate 12-month date range</span>
twelveMonthsAgo = <span style="color: #569cd6;">new</span> Date()
twelveMonthsAgo.<span style="color: #dcdcaa;">setMonth</span>(twelveMonthsAgo.<span style="color: #dcdcaa;">getMonth</span>() - 12)
dateFilter = twelveMonthsAgo.<span style="color: #dcdcaa;">toISOString</span>().<span style="color: #dcdcaa;">substring</span>(0, 10)  <span style="color: #6a9955;">// "2024-11-07"</span>

<span style="color: #6a9955;">// Step 3: Iterate all active project versions</span>
versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/projectVersions?includeInactive=false&fields=id,name'</span>)

<span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions.data:
    <span style="color: #6a9955;">// Step 4: Get total discovered issues in 12-month window, grouped by severity</span>
    discoveredIssues = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/issues?q=foundDate:[${dateFilter} TO *]&groupingtype=friority`</span>)
    
    <span style="color: #c586c0;">for</span> each group <span style="color: #c586c0;">in</span> discoveredIssues.groupBySet:
        severity = group.id  <span style="color: #6a9955;">// "Critical", "High", "Medium", "Low"</span>
        discovered[severity] += group.totalCount
    
    <span style="color: #6a9955;">// Step 5: Get remediated issues (removed:true status) in 12-month window</span>
    remediatedIssues = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/issues?q=foundDate:[${dateFilter} TO *]+removed:true&groupingtype=friority`</span>)
    
    <span style="color: #c586c0;">for</span> each group <span style="color: #c586c0;">in</span> remediatedIssues.groupBySet:
        severity = group.id
        remediated[severity] += group.totalCount

<span style="color: #6a9955;">// Step 6: Calculate fix rates per severity</span>
fixRates = {}
<span style="color: #c586c0;">for</span> each severity <span style="color: #c586c0;">in</span> [<span style="color: #ce9178;">"Critical"</span>, <span style="color: #ce9178;">"High"</span>, <span style="color: #ce9178;">"Medium"</span>, <span style="color: #ce9178;">"Low"</span>]:
    <span style="color: #c586c0;">if</span> (discovered[severity] > 0):
        fixRates[severity] = (remediated[severity] / discovered[severity]) * 100
    <span style="color: #c586c0;">else</span>:
        fixRates[severity] = 0

<span style="color: #6a9955;">// Display: "Critical: 78% (2,418), High: 82% (4,264), Medium: 71% (5,893), Low: 64% (8,832)"</span>
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> SSC <code>removed:true</code> status indicates code fix verified by rescanâ€”this is the gold standard for remediation but may undercount mitigations/risk acceptances handled outside code fixes. To include risk acceptances and suppressions in remediation count, query with <code>q=analysis:suppressed</code> separately and add to numerator, though this may inflate fix rates with non-code-change "fixes." SSC issue lifecycle is complexâ€”issues can be reopened after remediation; use <code>foundDate</code> range filtering consistently for both numerator and denominator to avoid double-counting reopened issues. For false positive exclusion, add <code>+analysis:!NotAnIssue</code> to filters. Performance: Requires two API calls per project version (discovered + remediated) with <code>groupingtype=friority</code> aggregation, expect 5-10 minute calculation time for 1000+ versions. <strong>Recommendation:</strong> Implement as daily batch job, cache results as custom attributes or Performance Indicators, validate that Critical fix rate â‰¥ High â‰¥ Medium â‰¥ Low to confirm priority discipline (inverted gradient indicates process failure).</p>
                                </li>
                                <li><strong>Closure Classification:</strong> Remediated should include: (1) Fixed = code changed and verified clean on rescan, (2) Mitigated = compensating control added (WAF rule, network isolation), (3) Risk Accepted = formally accepted with business owner signoff. Exclude from remediated count: False Positives (noise, not real fixes), Won't Fix without risk acceptance, Not Reproducible. Track exclusions separately to monitor false positive rates.</li>
                                <li><strong>Platform Gaps:</strong> FoD has clearer status taxonomy (Fixed vs Mitigated vs Remediation Accepted). SSC "removed" status is straightforward for code fixes but mitigations/risk acceptances require custom analysis types. Both platforms: issue reopening complicates calculationsâ€”use closure date range filtering to avoid double-counting. Time window consistency criticalâ€”12-month trailing window recommended.</li>
                                <li><strong>Data Quality:</strong> False positive markings inflate fix ratesâ€”track FP rate separately (FPs / Total Issues). "Fixed" without rescan verification questionableâ€”ensure only scan-verified fixes count. Priority discipline evident in data (78% Critical > 64% Low)â€”if inverted, indicates gaming or poor prioritization. High fix rates with growing backlog possible if discovery outpaces remediation.</li>
                                <li><strong>Severity Prioritization Validation:</strong> Fix rate gradient (Critical > High > Medium > Low) validates priority discipline. Inverted gradient (Low > Critical) indicates: (1) low-severity issues easier to fix (not necessarily bad), or (2) priority discipline failure (bad). Compare to MTTR by severity for full pictureâ€”fast MTTR + high fix rate = excellent.</li>
                                <li><strong>Recommendation:</strong> Pair with "Issue Discovery Rate" to show net backlog change (discovered - remediated = Î”backlog). Add trend line (12-month rolling fix rates) to show improvement. Break out fix rates by environment (Dev vs Prod)â€”prod fixes matter more. Track "Verified Fixes" (rescan-confirmed) separately from "Claimed Fixes" (marked fixed but not rescanned). Target: >90% Critical/High, >70% Medium, >50% Low for mature programs.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">2. Remediated Issues by Scan Type - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate fix rates by scan type: (Remediated Issues by Tool / Total Issues by Tool) Ã— 100. Group by SAST, DAST, SCA, Other. Same 12-month trailing window. Useful for tool ROI comparison and identifying tool-specific challenges. Lower priority than severity-based fix rates.</li>
                                <li><strong>Fortify on Demand (Detailed Implementation):</strong>
                                    <ul>
                                        <li><strong>Endpoints:</strong> Same as KPI #1 but with additional grouping by assessmentTypeId field:
                                            <ul>
                                                <li>GET /api/v3/releases/{releaseId}/vulnerabilities?filters=closedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                                <li>GET /api/v3/releases/{releaseId}/vulnerabilities?filters=introducedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                            </ul>
                                        </li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>assessmentTypeId (int32) â€” Scan type: 1=Static (SAST), 2=Dynamic (DAST), 3=Mobile, 4=Open Source (SCA)</li>
                                                <li>closedDate (date-time), introducedDate (date-time), isSuppressed, primaryLocationRemoved</li>
                                            </ul>
                                        </li>
                                        <li><strong>Grouping Workflow:</strong>
                                            <ol>
                                                <li>Query vulnerabilities with both closedDate and introducedDate filters</li>
                                                <li>Group by assessmentTypeId: Map to tool categories (1â†’SAST, 2â†’DAST, 4â†’SCA, 3+othersâ†’Other)</li>
                                                <li>Calculate fix rate per tool type: (closed by tool / discovered by tool) Ã— 100</li>
                                                <li>Example: SCA = (7,128 remediated / 8,800 discovered) Ã— 100 = 81%</li>
                                            </ol>
                                        </li>
                                        <li><strong>Tool Type Expectations:</strong> SCA typically highest fix rates (81% in example)â€”dependency updates via package managers are straightforward (npm update, maven dependency version bump). SAST medium fix rates (74%)â€”code refactoring more complex than dependency updates but well-understood. DAST lowest fix rates (68%)â€”often require architectural changes (session management, authentication frameworks) or infrastructure changes (TLS configuration). "Other" includes mobile, container, IaC scansâ€”variability depends on finding types.</li>
                                        <li><strong>Deduplication:</strong> Same vulnerability detected by multiple tools (e.g., SQL injection found by both SAST and DAST) may appear as separate findings. FoD deduplicates within release but cross-tool deduplication varies. For fix rate calculation, count unique vulnerabilities per tool categoryâ€”if same issue found by 2 tools, counts in both tool fix rates (acceptable for tool-specific metrics).</li>
                                        <li><strong>Volume Context:</strong> Report absolute counts alongside percentages: "SAST: 74% (8,436 of 11,400)", "SCA: 81% (7,128 of 8,800)". Raw counts show effort concentrationâ€”8,436 SAST fixes represents more work than 1,151 Other fixes despite lower percentage. Use for resource allocation and capacity planning.</li>
                                        <li><strong>ROI Analysis:</strong> Pair fix rates with discovery rates by tool: High discovery + high fix rate = high-value tool (SCA often fits). Low discovery + low fix rate = questionable tool ROI or need for tuning. High discovery + low fix rate = noisy tool OR genuinely complex issues requiring training/automation.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/projectVersions?includeInactive=false&fields=id,name â†’ enumerate active project versions</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/issues?q=foundDate:[{12monthsAgo} TO *]&fields=engineType,removed â†’ get all discovered issues with scan type</li>
                                        <li>Group by <code>engineType</code> field: "Fortify SCA", "Fortify WebInspect", "Fortify SCA (Third Party Libraries)", etc.</li>
                                        <li>Calculate fix rate per tool type: (removed:true count / total count) Ã— 100</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize counters for discovered and remediated by scan type</span>
scanTypes = {SAST: {discovered: 0, remediated: 0}, 
             DAST: {discovered: 0, remediated: 0}, 
             SCA: {discovered: 0, remediated: 0}, 
             Other: {discovered: 0, remediated: 0}}

<span style="color: #6a9955;">// Step 2: Calculate 12-month date range</span>
dateFilter = <span style="color: #dcdcaa;">getDateFilter</span>(12)  <span style="color: #6a9955;">// "2024-11-07"</span>

<span style="color: #6a9955;">// Step 3: Iterate all active project versions</span>
versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/projectVersions?includeInactive=false&fields=id,name'</span>)

<span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions.data:
    <span style="color: #6a9955;">// Step 4: Get all issues with engineType field in 12-month window</span>
    issues = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/issues?q=foundDate:[${dateFilter} TO *]&fields=engineType,removed&start=0&limit=-1`</span>)
    
    <span style="color: #c586c0;">for</span> each issue <span style="color: #c586c0;">in</span> issues.data:
        <span style="color: #6a9955;">// Step 5: Map engineType to scan type category</span>
        scanType = <span style="color: #dcdcaa;">mapEngineType</span>(issue.engineType)
        <span style="color: #6a9955;">// "Fortify SCA" â†’ SAST</span>
        <span style="color: #6a9955;">// "Fortify WebInspect" â†’ DAST</span>
        <span style="color: #6a9955;">// "Fortify SCA (Third Party Libraries)" â†’ SCA</span>
        <span style="color: #6a9955;">// Others â†’ Other</span>
        
        scanTypes[scanType].discovered++
        
        <span style="color: #6a9955;">// Step 6: Check if issue is remediated</span>
        <span style="color: #c586c0;">if</span> (issue.removed === <span style="color: #569cd6;">true</span>):
            scanTypes[scanType].remediated++

<span style="color: #6a9955;">// Step 7: Calculate fix rates per scan type</span>
fixRates = {}
<span style="color: #c586c0;">for</span> each type <span style="color: #c586c0;">in</span> [<span style="color: #ce9178;">"SAST"</span>, <span style="color: #ce9178;">"DAST"</span>, <span style="color: #ce9178;">"SCA"</span>, <span style="color: #ce9178;">"Other"</span>]:
    <span style="color: #c586c0;">if</span> (scanTypes[type].discovered > 0):
        fixRates[type] = (scanTypes[type].remediated / scanTypes[type].discovered) * 100
    <span style="color: #c586c0;">else</span>:
        fixRates[type] = 0

<span style="color: #6a9955;">// Display: "SAST: 74% (8,436), DAST: 68% (4,692), SCA: 81% (7,128), Other: 59% (1,151)"</span>
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> SSC <code>engineType</code> field contains scan engine name strings (not standardized IDs like FoD)â€”requires client-side mapping to SAST/DAST/SCA/Other categories. Common <code>engineType</code> values include "Fortify SCA" (SAST), "Fortify WebInspect" (DAST), "Fortify SCA (Third Party Libraries)" (SCA), but third-party integrations may have custom engine names requiring deployment-specific mapping logic. Multi-tool environments with non-Fortify integrations (Checkmarx, Snyk, etc.) will have varied <code>engineType</code> stringsâ€”maintain mapping dictionary. Same deduplication challenge as KPI 1â€”use consistent <code>foundDate</code> filtering for both numerator/denominator. Performance: Must retrieve all issues (not grouped) to extract <code>engineType</code> field, then aggregate client-side; slower than <code>groupingtype</code> approach (10-15 minutes for 1000+ versions with 100K+ issues). <strong>Recommendation:</strong> Implement as batch job, expect SCA highest fix rate (~81%, dependency updates), DAST lowest (~68%, architectural changes), SAST mid-range (~74%, code refactoring). Report absolute counts alongside percentages for resource allocation context.</p>
                                </li>
                                <li><strong>Fortify on Demand &amp; SSC (Combined):</strong> Both platforms: Group issues by scan type/source tool. FoD: <code>assessmentTypeId</code> (1=SAST, 2=DAST, 3=Mobile, 4=SCA). SSC: <code>engineType</code> or artifact source ("Fortify SCA", "Fortify WebInspect", etc.). Calculate numerator/denominator same as KPI #1 but grouped by tool. SCA typically highest fix rate (dependency updates simple). DAST typically lowest (architectural changes complex).</li>
                                <li><strong>Tool Categorization:</strong> SAST = static code analysis (Fortify SCA, Checkmarx, etc.). DAST = dynamic web scanning (Fortify WebInspect, Burp, etc.). SCA = dependency/component scanning (Fortify SCA OSS, Snyk, etc.). Other = container scanning, IaC scanning, manual pentests. Hybrid findings (same issue detected by multiple tools) should count onceâ€”deduplicate by issue instance ID.</li>
                                <li><strong>Fix Rate Interpretation:</strong> SCA high rates (81%) expectedâ€”dependency updates via package managers are straightforward. SAST medium rates (74%)â€”code refactoring more complex than dependency updates. DAST low rates (68%)â€”often require architectural changes (session management, authentication). "Other" lowest (59%)â€”may include manual findings or emerging tool types with less automated remediation.</li>
                                <li><strong>Platform Gaps:</strong> Both FoD and SSC support tool-based grouping. FoD assessmentTypeId is clean; SSC engineType strings vary. Multi-tool environments (third-party integrations) may have inconsistent tool categorization. Issue deduplication across tools challengingâ€”same SQL injection found by SAST and DAST counted once or twice?</li>
                                <li><strong>Data Quality:</strong> Tool-based fix rates less actionable than severity-based. Volume imbalances hidden by percentages (8K SAST vs 1K Other). Low fix rates ambiguousâ€”tool noise OR genuinely hard issues? Requires drill-down. Pair with false positive rate by tool to separate noise from difficulty.</li>
                                <li><strong>Recommendation:</strong> Subordinate metric to severity-based fix rates. Use for operational diagnostics (tool tuning, training needs) not executive reporting. Add "Average Remediation Difficulty" score by tool type if available. Investigate low fix-rate tools: improve configuration, provide specialized training, or consider replacement. Target: >75% for all tool types in mature programs.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">3. Mean Time to Remediate - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate average: Î£(closedDate - introducedDate) / Count(closed issues) for trailing 12 months. Include all severities (weighted average optional). Exclude: False positives (unless tracking separately), issues closed then reopened (use final closure date). <strong>THE most important remediation metric.</strong></li>
                                <li><strong>Fortify on Demand:</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false&offset=0&limit=50 returns vulnerability objects with introducedDate and closedDate fields.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>introducedDate (date-time) â€” ISO 8601 timestamp of first detection (e.g., "2024-01-15T14:23:11.000Z")</li>
                                                <li>closedDate (date-time, nullable) â€” ISO 8601 timestamp when vulnerability was remediated and verified by rescan</li>
                                                <li>severityString (string) â€” "Critical", "High", "Medium", "Low" (for severity-based MTTR calculation in KPI 4)</li>
                                                <li>isSuppressed (boolean) â€” Exclude suppressed=true issues</li>
                                                <li>primaryLocationRemoved (boolean) â€” true if code was deleted (not actually fixed), false if remediated</li>
                                            </ul>
                                        </li>
                                        <li><strong>MTTR Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query all closed/remediated vulnerabilities for trailing 12 months: filters=isSuppressed:false+closedDate:&gt;{12monthsAgo}</li>
                                                <li>Exclude code deletions: filter primaryLocationRemoved:false (only count actual fixes, not code removal)</li>
                                                <li>For each vulnerability: Calculate duration = (Date.parse(closedDate) - Date.parse(introducedDate)) / 86400000 milliseconds â†’ days</li>
                                                <li>Exclude outliers: Optionally filter out durations <1 hour (likely false positives) or >365 days (abandoned issues)</li>
                                                <li>Calculate Mean: Î£(durations) / count(vulnerabilities) â†’ average days</li>
                                                <li>Calculate Median (P50): Sort durations, take middle value â†’ more robust to outliers than mean</li>
                                                <li>Optional: Calculate P90 (90th percentile) to show tail performance</li>
                                            </ol>
                                        </li>
                                        <li><strong>Query Parameters:</strong> filters (isSuppressed:false, primaryLocationRemoved:false, closedDate:&gt;{startDate}), offset/limit (max 50), totalCount for pagination. Filter to production releases only (sdlcStatusType:Production) for business-critical MTTR tracking if needed.</li>
                                        <li><strong>Clock Definition (CRITICAL):</strong> Start clock = introducedDate (first detection timestamp, persists across scans via vulnerability correlation). Stop clock = closedDate (fix verified by successful rescan showing vulnerability no longer present). Do NOT use developer "mark as fixed" timestampsâ€”requires rescan verification. Do NOT use scan date or triage date as start. Both introducedDate and closedDate are system-set, reducing gaming potential.</li>
                                        <li><strong>Mean vs. Median:</strong> Mean MTTR (average) is sensitive to outliersâ€”one 365-day fix skews result. Median MTTR (P50, middle value) is more robust and represents typical remediation time. Best practice: Report both, default to median for stability. Industry benchmarks typically use median. Also consider P90 (90th percentile) showing "90% of issues fixed within X days"â€”useful for SLA compliance reporting.</li>
                                        <li><strong>Performance:</strong> Querying 12 months of closed vulnerabilities can retrieve 10,000+ recordsâ€”paginate with limit=50, offset incrementing by 50. Pre-filter by date (closedDate:&gt;{12monthsAgo}) to reduce dataset. Cache MTTR calculation daily (recalculate once per day), not real-time. For large portfolios, consider querying per application and aggregating client-side to parallelize API calls.</li>
                                        <li><strong>Data Quality:</strong> primaryLocationRemoved=true indicates code deletion (not remediation)â€”exclude from MTTR to avoid false improvements. False positive closures (isSuppressed=true) should be excluded or tracked separatelyâ€”they improve MTTR without fixing real vulnerabilities. Reopened issues complicate calculationâ€”use most recent closure cycle only, not original introducedDate. Minimum duration threshold (e.g., exclude <1 hour) filters trivial fixes and potential data errors.</li>
                                        <li><strong>Trend Tracking:</strong> Calculate MTTR monthly for trailing 12 months, plot trend to show continuous improvement. Year-over-year comparison: Compare current 12-month MTTR to prior 12-month period, calculate % change (e.g., "â–¼18% YoY" = (currentMTTR - priorMTTR) / priorMTTR Ã— 100). Track inflection points and correlate with process changes (automation, training, tool updates). Alert on degradation (e.g., MTTR increases >20% month-over-month).</li>
                                        <li><strong>Industry Benchmarks:</strong> World-class: <15 days median MTTR. Good: 15-30 days. Average: 30-60 days. Poor: >60 days. The example 14.2 days represents world-class performance. Benchmarks vary by industry (financial services typically faster due to regulatory pressure). Use for board reporting and competitive positioning: "Our MTTR is 2.5Ã— faster than industry average."</li>
                                        <li><strong>Best Practices:</strong> Make MTTR the #1 remediation dashboard metricâ€”universally understood velocity indicator. Add YoY trend indicator (improving/declining) for continuous improvement proof. Break down by severity (see KPI 4) for SLA compliance tracking. Pair with "Mean Time to Review/Triage" (introducedDate to firstTriageDate) to show full cycle time. For multi-environment orgs: Calculate separate MTTR-to-Dev and MTTR-to-Production metrics. Target: <15 days median overall, improving year-over-year, with all severity tiers meeting SLA thresholds.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/projectVersions?includeInactive=false&fields=id,name â†’ enumerate active project versions</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/issues?q=removed:true+removedDate:[{12monthsAgo} TO *]&fields=foundDate,removedDate â†’ get remediated issues</li>
                                        <li>For each issue: Calculate duration = (removedDate - foundDate) in days</li>
                                        <li>Calculate MTTR = Average(durations) or Median(durations)</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize duration collection</span>
durations = []

<span style="color: #6a9955;">// Step 2: Calculate 12-month date range</span>
twelveMonthsAgo = <span style="color: #569cd6;">new</span> Date()
twelveMonthsAgo.<span style="color: #dcdcaa;">setMonth</span>(twelveMonthsAgo.<span style="color: #dcdcaa;">getMonth</span>() - 12)
dateFilter = twelveMonthsAgo.<span style="color: #dcdcaa;">toISOString</span>().<span style="color: #dcdcaa;">substring</span>(0, 10)

<span style="color: #6a9955;">// Step 3: Iterate all active project versions</span>
versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/projectVersions?includeInactive=false&fields=id,name'</span>)

<span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions.data:
    <span style="color: #6a9955;">// Step 4: Get remediated issues in 12-month window</span>
    issues = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/issues?q=removed:true+removedDate:[${dateFilter} TO *]&fields=foundDate,removedDate&start=0&limit=-1`</span>)
    
    <span style="color: #c586c0;">for</span> each issue <span style="color: #c586c0;">in</span> issues.data:
        <span style="color: #c586c0;">if</span> (issue.foundDate && issue.removedDate):
            <span style="color: #6a9955;">// Step 5: Calculate duration in milliseconds then convert to days</span>
            foundDateMs = Date.<span style="color: #dcdcaa;">parse</span>(issue.foundDate)
            removedDateMs = Date.<span style="color: #dcdcaa;">parse</span>(issue.removedDate)
            durationMs = removedDateMs - foundDateMs
            durationDays = durationMs / (1000 * 60 * 60 * 24)
            
            <span style="color: #6a9955;">// Step 6: Filter outliers (optional but recommended)</span>
            <span style="color: #c586c0;">if</span> (durationDays >= 0.04 && durationDays <= 365):  <span style="color: #6a9955;">// Exclude <1hr and >1yr</span>
                durations.<span style="color: #dcdcaa;">push</span>(durationDays)

<span style="color: #6a9955;">// Step 7: Calculate MTTR metrics</span>
<span style="color: #c586c0;">if</span> (durations.length > 0):
    <span style="color: #6a9955;">// Mean (average) - sensitive to outliers</span>
    meanMTTR = durations.<span style="color: #dcdcaa;">reduce</span>((sum, d) => sum + d, 0) / durations.length
    
    <span style="color: #6a9955;">// Median (P50) - more robust to outliers (RECOMMENDED)</span>
    sortedDurations = durations.<span style="color: #dcdcaa;">sort</span>((a, b) => a - b)
    medianIndex = Math.<span style="color: #dcdcaa;">floor</span>(sortedDurations.length / 2)
    medianMTTR = sortedDurations[medianIndex]
    
    <span style="color: #6a9955;">// P90 (90th percentile) - shows tail performance</span>
    p90Index = Math.<span style="color: #dcdcaa;">floor</span>(sortedDurations.length * 0.9)
    p90MTTR = sortedDurations[p90Index]

<span style="color: #6a9955;">// Display: "14.2 days (mean), 11.5 days (median), 28.4 days (P90), based on 21,407 fixes"</span>
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> SSC <code>removedDate</code> field marks when issue status changed to "removed" (code fix verified by rescan)â€”this is the gold standard stop clock but requires rescan to populate. Issues marked "fixed" without rescan verification will have null <code>removedDate</code> and cannot be included in MTTR calculation. SSC has complex workflow states (New â†’ In Progress â†’ Reviewed â†’ Fixed â†’ Removed)â€”ensure using <code>removedDate</code> not intermediate state dates. Reopened issues complicate calculationâ€”if issue was removed then reopened, use most recent <code>removedDate</code> from final closure cycle, not original <code>foundDate</code>. Performance: Must retrieve all remediated issues individually (no aggregation) to calculate durations client-side; expect 10-15 minute calculation for 1000+ versions with 20K+ remediated issues. Outlier filtering criticalâ€”one 365-day issue can skew mean; recommend excluding <1 hour (data errors/trivial fixes) and >365 days (abandoned issues). <strong>Recommendation:</strong> Use median MTTR over mean for stability, implement as daily batch job with cached results, validate clock consistency across all severity tiers, target world-class <15 days median with continuous YoY improvement.</p>
                                </li>
                                <li><strong>Clock Start/Stop Definition:</strong> <strong>CRITICAL:</strong> Standardize clock definition: Start = <code>introducedDate</code>/<code>foundDate</code> (first detection), NOT scan date or triage date. Stop = <code>closedDate</code>/<code>removedDate</code> (fix verified by rescan), NOT developer "mark as fixed" date. Verification requiredâ€”manual "fixed" markings without rescan confirmation should not count. Both platforms track issue lifecycle via correlation across scans.</li>
                                <li><strong>Mean vs Median:</strong> Mean (average) MTTR sensitive to outliers (one 365-day fix skews average). Median (P50) more robustâ€”represents typical remediation time. Recommendation: Report both, default to median for stability. Also consider P90 (90th percentile) to show tail performance. Industry benchmarks typically use median.</li>
                                <li><strong>Platform Gaps:</strong> Both FoD and SSC track dates accurately. FoD simplerâ€”clear introducedDate + closedDate. SSC more complexâ€”multiple date fields (foundDate, scanDate, uploadDate, removedDate), workflow state transitions. Neither platform natively calculates MTTRâ€”client-side calculation required. Issue reopening complicates calcâ€”both platforms maintain reopen history.</li>
                                <li><strong>Data Quality:</strong> False positive closures artificially improve MTTRâ€”track FP closures separately. "Fixed in dev" vs "deployed to prod" gap can be significantâ€”clarify which environment counts. Outliers (>180 days) should be investigatedâ€”architectural blockers, abandoned apps, or process failures. Minimum time threshold recommended (exclude issues fixed <1 hour as likely false positives or trivial).</li>
                                <li><strong>Trend Tracking:</strong> Calculate MTTR monthly and plot 12-month trend. YoY comparison (â–¼18% in example) demonstrates continuous improvement. Identify inflection points and correlate with process changes (Aviator adoption, training programs, automation). Target degradation alerts (e.g., if MTTR increases >20% MoM).</li>
                                <li><strong>Industry Benchmarks:</strong> World-class: <15 days. Good: 15-30 days. Average: 30-60 days. Poor: >60 days. Example shows 14.2 days = world-class. Benchmarks vary by industry (finance stricter than retail). Use for board reporting and competitive positioning.</li>
                                <li><strong>Recommendation:</strong> <strong>Make this the #1 remediation KPI</strong> on all dashboards. Add trend indicator (improving/declining). Break down by severity (KPI #4). Pair with "Mean Time to Review" to show full cycle time (review + fix). For multi-environment orgs: calculate MTTR-to-Dev and MTTR-to-Prod separately. Target: <15 days overall, improving YoY.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">4. Mean Time to Remediate by Severity - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Same as KPI #3 but segmented by severity. Calculate MTTR separately for Critical, High, Medium, Low. Validates priority disciplineâ€”MTTR should decrease with increasing severity (Critical fastest, Low slowest). Add YoY trend indicators for each severity tier. <strong>Best-in-class remediation metric.</strong></li>
                                <li><strong>Fortify on Demand (Detailed Implementation):</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> Same as KPI 3: GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false+closedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                        <li><strong>Segmentation Workflow:</strong>
                                            <ol>
                                                <li>Query all closed vulnerabilities (same filters/pagination as KPI 3)</li>
                                                <li>Group by severityString: Create 4 buckets (Critical, High, Medium, Low)</li>
                                                <li>For each severity group: Calculate duration for each vulnerability = (closedDate - introducedDate) / 86400000 â†’ days</li>
                                                <li>Calculate Mean and Median MTTR per severity independently</li>
                                                <li>Calculate YoY comparison: Query same 12-month period from prior year, group by severity, compute % change per severity tier</li>
                                                <li>Validate priority gradient: Assert Critical_MTTR < High_MTTR < Medium_MTTR < Low_MTTR (inverted gradient = priority discipline failure)</li>
                                            </ol>
                                        </li>
                                        <li><strong>Response Fields:</strong> severityString (Critical/High/Medium/Low), introducedDate (date-time), closedDate (date-time), isSuppressed, primaryLocationRemoved. Same fields as KPI 3, with additional grouping by severity.</li>
                                        <li><strong>SLA Threshold Mapping:</strong> Define organizational SLA policies per severity: Critical (7-15 days typical), High (15-30 days), Medium (30-60 days), Low (60-180 days). Compare calculated MTTR to SLA thresholds for compliance validation. Example shows excellent compliance: Critical 5.8d < 7d SLA, High 9.7d < 15d SLA, Medium 17.2d < 30d SLA, Low 25.3d < 60d SLA. Add SLA threshold lines to visualization for pass/fail color coding (green/yellow/red).</li>
                                        <li><strong>Volume Considerations:</strong> Low-severity issues most numerous (8,832 in example)â€”large sample size, stable MTTR calculation. Critical issues least numerous (2,418 in example)â€”smaller sample, more volatility from outliers. For severity groups with <30 closed issues, prefer median over mean for robustness. Report sample sizes alongside MTTR: "5.8d (based on 2,418 critical fixes)" to provide statistical context.</li>
                                        <li><strong>YoY Trend Calculation:</strong> For each severity group: Query closedDate range from prior year (13-25 months ago), calculate MTTR, compare to current 12-month MTTR. Formula: %Change = ((current - prior) / prior) Ã— 100. Negative % = improvement (e.g., "â–¼12%" means 12% faster). Display trend indicators (â–¼/â–²) with each severity bar. All improving trends (example shows â–¼12%, â–¼15%, â–¼22%, â–¼8%) = continuous improvement culture.</li>
                                        <li><strong>Priority Gradient Validation:</strong> **CRITICAL:** Validate MTTR increases with decreasing severity (Critical fastest â†’ Low slowest). Inverted gradient flags: (1) Severity gaming (teams downgrading Criticalâ†’High to hit faster MTTR targets), (2) Poor prioritization (low-severity issues getting disproportionate attention), (3) Low-sev issues genuinely easier to fix (acceptable if gradient gap is minor, e.g., Low=26d vs Medium=17d). Monitor for sudden inversions month-over-monthâ€”indicates process degradation.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center:</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li>GET /api/v1/projectVersions?includeInactive=false&fields=id,name â†’ enumerate active project versions</li>
                                        <li>GET /api/v1/projectVersions/{versionId}/issues?q=removed:true+removedDate:[{12monthsAgo} TO *]&fields=foundDate,removedDate,friority â†’ get remediated issues with severity</li>
                                        <li>Group by <code>friority</code> field (Critical, High, Medium, Low)</li>
                                        <li>Calculate MTTR per severity: Median(durations) for each group</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic (Pseudocode):</strong></p>
                                    <pre style="background: #1e1e1e; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 13px;">
<span style="color: #6a9955;">// Step 1: Initialize duration collections by severity</span>
durationsBySeverity = {
    Critical: [],
    High: [],
    Medium: [],
    Low: []
}

<span style="color: #6a9955;">// Step 2: Calculate date range</span>
dateFilter = <span style="color: #dcdcaa;">getDateFilter</span>(12)  <span style="color: #6a9955;">// "2024-11-07"</span>

<span style="color: #6a9955;">// Step 3: Iterate all active project versions</span>
versions = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">'/api/v1/projectVersions?includeInactive=false&fields=id,name'</span>)

<span style="color: #c586c0;">for</span> each version <span style="color: #c586c0;">in</span> versions.data:
    <span style="color: #6a9955;">// Step 4: Get remediated issues with severity field</span>
    issues = <span style="color: #dcdcaa;">apiCall</span>(<span style="color: #ce9178;">`/api/v1/projectVersions/${version.id}/issues?q=removed:true+removedDate:[${dateFilter} TO *]&fields=foundDate,removedDate,friority&start=0&limit=-1`</span>)
    
    <span style="color: #c586c0;">for</span> each issue <span style="color: #c586c0;">in</span> issues.data:
        <span style="color: #c586c0;">if</span> (issue.foundDate && issue.removedDate && issue.friority):
            <span style="color: #6a9955;">// Step 5: Calculate duration</span>
            durationMs = Date.<span style="color: #dcdcaa;">parse</span>(issue.removedDate) - Date.<span style="color: #dcdcaa;">parse</span>(issue.foundDate)
            durationDays = durationMs / (1000 * 60 * 60 * 24)
            
            <span style="color: #6a9955;">// Step 6: Group by severity (filter outliers)</span>
            <span style="color: #c586c0;">if</span> (durationDays >= 0.04 && durationDays <= 365):
                severity = issue.friority  <span style="color: #6a9955;">// "Critical", "High", "Medium", "Low"</span>
                durationsBySeverity[severity].<span style="color: #dcdcaa;">push</span>(durationDays)

<span style="color: #6a9955;">// Step 7: Calculate MTTR per severity (use median for robustness)</span>
mttrBySeverity = {}
<span style="color: #c586c0;">for</span> each severity <span style="color: #c586c0;">in</span> [<span style="color: #ce9178;">"Critical"</span>, <span style="color: #ce9178;">"High"</span>, <span style="color: #ce9178;">"Medium"</span>, <span style="color: #ce9178;">"Low"</span>]:
    <span style="color: #c586c0;">if</span> (durationsBySeverity[severity].length > 0):
        sortedDurations = durationsBySeverity[severity].<span style="color: #dcdcaa;">sort</span>((a, b) => a - b)
        medianIndex = Math.<span style="color: #dcdcaa;">floor</span>(sortedDurations.length / 2)
        mttrBySeverity[severity] = {
            median: sortedDurations[medianIndex],
            count: sortedDurations.length,
            mean: sortedDurations.<span style="color: #dcdcaa;">reduce</span>((sum, d) => sum + d, 0) / sortedDurations.length
        }

<span style="color: #6a9955;">// Step 8: Validate priority gradient (CRITICAL CHECK)</span>
<span style="color: #c586c0;">if</span> (mttrBySeverity.Critical.median > mttrBySeverity.High.median ||
    mttrBySeverity.High.median > mttrBySeverity.Medium.median ||
    mttrBySeverity.Medium.median > mttrBySeverity.Low.median):
    <span style="color: #dcdcaa;">console.warn</span>(<span style="color: #ce9178;">"PRIORITY DISCIPLINE FAILURE: MTTR gradient inverted!"</span>)

<span style="color: #6a9955;">// Display: "Critical: 5.8d, High: 9.7d, Medium: 17.2d, Low: 25.3d"</span>
</pre>
                                    
                                    <p><strong>Key Limitations:</strong> Same limitations as overall MTTR (KPI 3) but amplified by severity segmentationâ€”<strong>small sample sizes for Critical issues</strong> create high volatility (e.g., 50 critical fixes vs 8,000 low fixes). Use median over mean for all severity groups, especially Critical/High with smaller samples. SSC <code>friority</code> field (Fortify Priority) combines severity + exploitability; ensure it aligns with your SLA severity definitions (may differ from raw CVSS severity). Performance scales linearly with issue volumeâ€”must retrieve and process all remediated issues to group by severity client-side; no native MTTR aggregation by severity in SSC API. <strong>CRITICAL:</strong> Validate priority gradient (Critical < High < Medium < Low MTTR)â€”inverted gradient indicates severity gaming, poor prioritization, or process failure. Monitor for sudden gradient inversions month-over-month. Recommend batch job implementation with daily refresh, SLA threshold comparison for compliance reporting, and automated alerts for gradient violations or SLA breaches.</p>
                                </li>
                                <li><strong>SLA Alignment:</strong> Common SLA policies: Critical = 7-15 days, High = 15-30 days, Medium = 30-60 days, Low = 60-180 days. Example data shows excellent SLA performance: Critical 5.8d (under 7-day SLA), High 9.7d (under 15-day), Medium 17.2d (under 30-day), Low 25.3d (under 60-day). Add SLA threshold lines to visualization for compliance tracking.</li>
                                <li><strong>Priority Discipline Validation:</strong> <strong>CRITICAL:</strong> MTTR gradient must increase with decreasing severity (Critical < High < Medium < Low). Inverted gradient indicates: (1) Severity gaming (downgrading criticals to improve MTTR), (2) Poor prioritization (low-sev issues getting more attention), or (3) Low-sev issues actually easier to fix (acceptable if gap is small). Monitor for sudden inversions.</li>
                                <li><strong>YoY Trend Tracking:</strong> Calculate MTTRby severity for same trailing 12-month period last year. Compute % change. Example shows all improving: Critical â–¼12%, High â–¼15%, Medium â–¼22%, Low â–¼8%. Continuous improvement across all tiers = program maturity. Declining trends (increasing MTTR) require investigationâ€”process degradation or tool noise increase.</li>
                                <li><strong>Platform Implementation:</strong> Both platforms support severity-based grouping. Calculation logic identical to overall MTTR. Volume considerations: Low-severity issues are most numerousâ€”ensure sufficient sample size for statistical validity. Critical issues least numerousâ€”small sample sizes can create volatility (10 criticals with 1 outlier = skewed MTTR).</li>
                                <li><strong>Visualization Best Practices:</strong> Horizontal bar chart (as shown) effectively displays gradient. Add: (1) SLA threshold markers, (2) YoY trend indicators (â–¼/â–² with %), (3) actual counts (e.g., "based on 2,418 critical fixes"), (4) median alongside mean. Color-code: Green if under SLA, Yellow if near SLA, Red if over SLA.</li>
                                <li><strong>Gaming Prevention:</strong> Monitor severity distribution over timeâ€”sudden decrease in Critical issues with increase in High issues = potential gaming. Cross-reference with external severity sources (CVSSv3 scores, vendor ratings). Implement severity review process for Critical/High issues. Audit random sample of severity classifications quarterly.</li>
                                <li><strong>Recommendation:</strong> <strong>Essential executive dashboard metric.</strong> Pair with overall MTTR for complete picture. Add drill-down showing "Top 10 Longest Open Critical Issues" to identify stuck items. Implement automated SLA violation alerts. For cyber insurance/audit: generate compliance report showing % of issues fixed within SLA by severity. Target: Critical <7d, High <15d, Medium <30d, Low <60d, all improving YoY.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">5. Remediated Issues (12 Month trend) - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Count remediated issues per month for last 12 months. Group by severity. Grouped bar chart format (not stacked). Operational capacity metricâ€”less important than MTTR for executives. Pair with "New Issues by Month" to show net backlog change.</li>
                                <li><strong>Fortify on Demand (Detailed Implementation):</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?filters=isSuppressed:false+primaryLocationRemoved:false+closedDate:&gt;{12monthsAgo}&offset=0&limit=50</li>
                                        <li><strong>Monthly Grouping Workflow:</strong>
                                            <ol>
                                                <li>Query all closed vulnerabilities for trailing 12 months</li>
                                                <li>For each vulnerability: Extract month from closedDate (parse ISO 8601, format as YYYY-MM)</li>
                                                <li>Group by [month][severityString]: Create nested structure with 12 months Ã— 4 severity tiers</li>
                                                <li>Count vulnerabilities in each [month][severity] cell</li>
                                                <li>Result: Monthly remediation volume matrix for grouped bar chart visualization</li>
                                            </ol>
                                        </li>
                                        <li><strong>Response Fields:</strong> closedDate (date-time ISO 8601), severityString (Critical/High/Medium/Low), isSuppressed, primaryLocationRemoved. Same fields as MTTR calculation (KPI 3/4), different aggregation.</li>
                                        <li><strong>Net Backlog Calculation:</strong> Pair with Risk Dashboard "New Issues Trend" (KPI 6) for backlog health: Net Change = Remediated - Discovered. Example: If Feb discovered 500 and remediated 600 â†’ net improvement of 100 issues. Positive net = backlog shrinking, negative = growing. Track cumulative backlog over 12 months.</li>
                                        <li><strong>Performance:</strong> Same dataset as MTTR queriesâ€”reuse cached results. Pre-aggregate monthly counts to avoid recalculating daily. Cache refreshes once per day. Query optimization: Use closedDate range filter to minimize API calls.</li>
                                        <li><strong>Seasonality Patterns:</strong> Typical dips: December (holidays), July/August (summer vacations), around major releases (all-hands deployment efforts). Spikes: Post-security training, post-executive pressure (board meetings), quarterly close-out pushes. Annotate known patterns in visualization to avoid false alarms.</li>
                                    </ul>
                                </li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions?includeInactive=false</code> â€” Retrieve all active project versions</li>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=removedDate:[{12monthsAgo} TO *]&fields=removedDate,friority</code> â€” Get remediated issues with dates</li>
                                        <li>Group client-side by month: Parse <code>removedDate</code> field (ISO 8601), extract YYYY-MM, aggregate counts by [month][severity]</li>
                                        <li>Render grouped bar chart with 12 months Ã— 4 severity categories</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>// Step 1: Calculate 12-month date range
startDate = new Date()
startDate.setMonth(startDate.getMonth() - 12)
startDateStr = startDate.toISOString().substring(0, 10)  // "2024-11-07"

// Step 2: Initialize monthly data structure
monthlyData = {}  // {"2024-11": {Critical: 0, High: 0, Medium: 0, Low: 0}, ...}
for (i = 0; i < 12; i++) {
  month = new Date()
  month.setMonth(month.getMonth() - i)
  monthKey = month.toISOString().substring(0, 7)  // "2024-11"
  monthlyData[monthKey] = {Critical: 0, High: 0, Medium: 0, Low: 0}
}

// Step 3: Query all active versions
versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

// Step 4: Aggregate remediated issues by month and severity
for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=removedDate:[{startDateStr} TO *]&fields=removedDate,friority&start=0&limit=500"
  
  for (issue in issues.data) {
    if (issue.removedDate) {
      // Extract month from removedDate "2024-11-15T10:30:00.000+0000"
      monthKey = issue.removedDate.substring(0, 7)  // "2024-11"
      severity = issue.friority  // "Critical", "High", "Medium", "Low"
      
      if (monthlyData[monthKey]) {
        monthlyData[monthKey][severity]++
      }
    }
  }
}

// Step 5: Render grouped bar chart from monthlyData
for (month in monthlyData) {
  // Create 4 bars for each month (Critical, High, Medium, Low)
  renderGroupedBars(month, monthlyData[month])
}</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> No native monthly grouping in SSCâ€”requires client-side parsing of <code>removedDate</code> field and manual aggregation across all versions. <code>removedDate</code> format is ISO 8601 (<code>2024-11-15T10:30:00.000+0000</code>), extract first 7 characters for YYYY-MM month key. Must paginate through all remediated issues (use <code>start</code> and <code>limit</code> parameters, max 500 per call). For portfolios with 1000+ versions and 100K+ remediated issues, expect 10-15 minute calculation time. <strong>Recommendation:</strong> Implement as nightly batch job, cache monthly counts in custom attributes or external database, display from cache rather than real-time calculation.</p>
                                </li>
                                <li><strong>Recommendation:</strong> Subordinate to MTTR metrics. Add annotation for seasonal patterns (holiday dips). Pair with discovery rate to calculate net backlog Î” (discovered - remediated = backlog growth/shrinkage). Consider adding "Remediation Velocity" metric (issues/engineer/month) for capacity planning. Chart implementation already complete (grouped bars with axes).</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">6. Reviewed Issues by Severity - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate review rate: (Reviewed Issues / Total Discovered Issues) Ã— 100 for trailing 12 months. Group by severity. "Reviewed" = triaged, analyzed, dispositioned by security team or AI (if Aviator enabled). Includes: Confirmed, False Positive marked, Risk Accepted. Excludes: New/Unreviewed queue items.</li>
                                <li><strong>Fortify on Demand:</strong> FoD tracks review status via issue audit log and <code>reviewedBy</code> field. Filter issues where <code>reviewedBy</code> is not null OR <code>analysisType</code> is set (indicates triage decision). With Aviator: AI-triaged issues count as "reviewed" if confidence score meets threshold. Calculate per-severity review rates.</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC has no standardized "reviewed" field. Must use custom workflow states or <code>audited</code> field.</p>
                                    
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions?includeInactive=false</code> â€” Get all active versions</li>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=foundDate:[{12monthsAgo} TO *]&groupingtype=friority</code> â€” Total issues discovered (denominator)</li>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=foundDate:[{12monthsAgo} TO *]+audited:true&groupingtype=friority</code> â€” Reviewed issues (numerator)</li>
                                        <li>Calculate review rate: (audited / total) Ã— 100 per severity</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>// Calculate 12-month date range
startDate = new Date()
startDate.setMonth(startDate.getMonth() - 12)
startDateStr = startDate.toISOString().substring(0, 10)

// Initialize counters by severity
totalBySeverity = {Critical: 0, High: 0, Medium: 0, Low: 0}
reviewedBySeverity = {Critical: 0, High: 0, Medium: 0, Low: 0}

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

for (version in versions.data) {
  // Get total issues discovered in last 12 months
  totalIssues = GET "/api/v1/projectVersions/{version.id}/issues?q=foundDate:[{startDateStr} TO *]&groupingtype=friority"
  
  for (group in totalIssues.groupBySet) {
    severity = group.id  // "Critical", "High", "Medium", "Low"
    totalBySeverity[severity] += group.totalCount
  }
  
  // Get reviewed issues (audited=true OR has analysis attribute)
  reviewedIssues = GET "/api/v1/projectVersions/{version.id}/issues?q=foundDate:[{startDateStr} TO *]+audited:true&groupingtype=friority"
  
  for (group in reviewedIssues.groupBySet) {
    severity = group.id
    reviewedBySeverity[severity] += group.totalCount
  }
}

// Calculate review percentages
for (severity in totalBySeverity) {
  reviewRate = (reviewedBySeverity[severity] / totalBySeverity[severity]) * 100
  console.log(severity + ": " + reviewRate + "% reviewed")
}</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <code>audited</code> field availability depends on SSC workflow configurationâ€”may not be populated in all deployments. Alternative: Check for issues with <code>analysis</code> attribute set (indicates security team reviewed), but field name varies by custom configuration. SSC workflows are highly customizableâ€”"reviewed" state may be called "Triaged", "Analyzed", or something deployment-specific. <strong>Recommendation:</strong> Work with SSC admin to identify correct field/workflow state for "reviewed" status. If no audited tracking exists, consider implementing custom attribute or workflow state to track review completion.</p>
                                </li>
                                <li><strong>Human vs AI Review:</strong> With Aviator adoption (67%), distinguish: (1) AI-reviewed (auto-triaged by ML), (2) Human-reviewed (security analyst triage), (3) Hybrid (AI triage + human validation). Report separately if possible to show automation impact. "Reviewed" should include both AI and human unless specifically tracking human-only throughput.</li>
                                <li><strong>Recommendation:</strong> Pair with "Mean Time to Review" for complete triage picture. Track review queue depth separately (unreviewed backlog count). Monitor review quality via false positive precision (% of FP markings that are accurate). For Aviator deployments: track AI review confidence scores and human override rates. Target: >90% Critical/High, >70% Medium, >50% Low reviewed.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">7. Reviewed Issues by Scan Type - Value: 6/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Same as KPI #6 but grouped by scan type instead of severity. Less actionable than severity-based view. Consider removing from executive dashboard and keeping in operational views only.</li>
                                <li><strong>Fortify on Demand:</strong> Group reviewed/total issues by assessmentTypeId. Calculate review % per tool type (SAST, DAST, SCA, Mobile, Other).</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>Key Endpoints:</strong> Combine KPI 6 logic (audited field) with KPI 2 logic (engineType mapping). <code>GET /api/v1/projectVersions/{id}/issues?q=audited:true</code> grouped by client-side engineType extraction â†’ SAST/DAST/SCA/Other categories. Calculate (audited / total) Ã— 100 per scan type.</p>
                                    <p><strong>âš ï¸ Limitation:</strong> Requires same engineType string parsing as Remediated by Scan Type (KPI 2). <strong>Recommendation:</strong> Reuse engineType mapping logic from KPI 2, apply audited filter from KPI 6. Consider removing from dashboardâ€”redundant metric with limited actionability.</p>
                                </li>
                                <li><strong>Recommendation:</strong> <strong>Consider removing this KPI</strong> from production dashboardsâ€”redundant with other metrics. If keeping: use for tool noise diagnosis (low review rate + low fix rate = noisy tool needing tuning). More useful in operational tool management views than executive dashboards.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">8. Mean Time to Review - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Calculate average: Î£(reviewedDate - introducedDate) / Count(reviewed issues) for trailing 12 months. Include all severities (optionally break down by severity). Clock: Start = introducedDate/foundDate, Stop = first review/triage completion timestamp. Critical for developer experienceâ€”fast feedback loop.</li>
                                <li><strong>Fortify on Demand (Detailed Implementation):</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?filters=introducedDate:&gt;{12monthsAgo}&offset=0&limit=50 returns vulnerabilities with audit trail data.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>introducedDate (date-time) â€” Issue discovery timestamp</li>
                                                <li>firstReviewedDate (date-time, nullable) â€” Timestamp when first triaged/reviewed (may be in issue history/audit log)</li>
                                                <li>severityString (Critical/High/Medium/Low) â€” For severity-based MTT-Review breakdown</li>
                                                <li>Issue audit trail â€” May need to query GET /api/v3/releases/{releaseId}/vulnerabilities/{vulnId}/audit-history for review events</li>
                                            </ul>
                                        </li>
                                        <li><strong>MTT-Review Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query vulnerabilities introduced in trailing 12 months</li>
                                                <li>For each vulnerability: Retrieve first review/triage timestamp from audit trail (look for "Reviewed", "Analyzed", "Triaged" events)</li>
                                                <li>Calculate duration: (firstReviewedDate - introducedDate) / 86400000 â†’ days</li>
                                                <li>Calculate Mean: Î£(durations) / count(reviewed) â†’ average days</li>
                                                <li>Calculate Median (P50): Sort durations, take middle â†’ more robust metric</li>
                                                <li>Optional: Break down by severity (Critical should be <1d, High <3d, Medium <7d, Low <14d)</li>
                                            </ol>
                                        </li>
                                        <li><strong>Aviator AI Triage Separation:</strong> **CRITICAL**: With Aviator adoption (67% in example), separate AI-triaged from human-triaged: Query audit trail for triage source (AI vs human analyst). Calculate separate metrics: AI MTT-Review (typically <1 day, often instant), Human MTT-Review (typically 3-7 days), Blended average (2.8d in example = 67% AI Ã— 0.5d + 33% human Ã— 5d). Track AI triage precision (% correct) to ensure speed doesn't sacrifice quality. 24% YoY improvement likely driven by Aviator adoptionâ€”calculate ROI: time saved, capacity freed, faster developer feedback.</li>
                                        <li><strong>Performance:</strong> May require querying audit trail per vulnerability (expensive for large datasets). Cache MTT-Review calculation daily. Pre-aggregate by severity for dashboard performance. For Aviator deployments, flag AI-triaged issues in database for faster filtering.</li>
                                        <li><strong>Review Timestamp Extraction:</strong> "Reviewed" may mean different things: (1) AI auto-triage (instant), (2) Security analyst initial look (quick scan), (3) Deep analysis with notes (thorough). FoD audit trail contains review events with timestamps. Look for first substantive review action (not just "viewed" but "analyzed", "assigned", "commented"). Aviator triage events clearly marked in audit log.</li>
                                        <li><strong>Industry Benchmarks:</strong> World-class: <3 days median. Good: 3-7 days. Average: 7-14 days. Poor: >14 days. Example 2.8 days = world-class performance. Aviator-enabled programs often achieve <1 day median for AI-triaged issues (instant triage + human validation time). Human-only triage typically 5-10 days in mature programs.</li>
                                        <li><strong>Developer Experience Impact:</strong> MTT-Review directly affects developer cycle time: Fast triage (2.8d) + Fast fix (14.2d MTTR) = 17d total cycle time. Slow triage (14d) + Fast fix (14d) = 28dâ€”developer blocked waiting. For CI/CD integration, target <1 day MTT-Review so developers get feedback within sprint. Aviator enables near-real-time feedback.</li>
                                    </ul>
                                </li>
                                <li><strong>Fortify on Demand:</strong> MTT-Review = <code>firstReviewedDate</code> - <code>introducedDate</code>. FoD audit trail tracks when issue first reviewed (human or Aviator). With Aviator: AI triage timestamp vs human review timestampâ€”track separately to measure automation impact. Filter to reviewed issues only.</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC has no native "reviewedDate" field. Must use workflow state transition timestamps or custom attributes.</p>
                                    
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=foundDate:[{12monthsAgo} TO *]+audited:true</code> â€” Get reviewed issues</li>
                                        <li><code>GET /api/v1/issues/{id}/auditHistory</code> â€” Retrieve audit trail for each reviewed issue</li>
                                        <li>Parse audit history for first "review" or "audited" event timestamp</li>
                                        <li>Calculate MTT-Review: (reviewTimestamp - foundDate) / 86400000 â†’ days, compute median</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>reviewDurations = []

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

for (version in versions.data) {
  // Get reviewed issues from last 12 months
  reviewedIssues = GET "/api/v1/projectVersions/{version.id}/issues?q=foundDate:[{12monthsAgo} TO *]+audited:true&fields=id,foundDate&start=0&limit=500"
  
  for (issue in reviewedIssues.data) {
    // Get audit trail to find review timestamp
    auditHistory = GET "/api/v1/issues/{issue.id}/auditHistory"
    
    // Find first "audited" or "reviewed" event
    reviewEvent = auditHistory.data.find(event => 
      event.action.match(/audit|review|triage/i)
    )
    
    if (reviewEvent && reviewEvent.occurredDate) {
      foundDateMs = Date.parse(issue.foundDate)
      reviewDateMs = Date.parse(reviewEvent.occurredDate)
      
      durationDays = (reviewDateMs - foundDateMs) / (1000 * 60 * 60 * 24)
      
      if (durationDays >= 0 && durationDays <= 90) {  // Filter outliers
        reviewDurations.push(durationDays)
      }
    }
  }
}

// Calculate median (more robust than mean)
reviewDurations.sort((a, b) => a - b)
median = reviewDurations[Math.floor(reviewDurations.length / 2)]
console.log("Median Time to Review: " + median.toFixed(1) + " days")</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> Requires querying audit history per issueâ€”expensive for large datasets (expect 15-20 min for 20K+ reviewed issues). Audit event names ("audited", "reviewed", "triaged") vary by SSC workflow configurationâ€”may need deployment-specific event matching regex. If no audit trail exists, consider using custom timestamp attributes populated by workflow automation. <strong>Recommendation:</strong> Implement as nightly batch job, cache MTT-Review metric, display from cache. For Aviator integration: SSC audit trail should contain AI triage eventsâ€”filter by event source.</p>
                                </li>
                                <li><strong>Aviator Impact Analysis:</strong> <strong>CRITICAL:</strong> Separate MTT-Review for AI-triaged vs human-triaged issues. Example: AI triage = 0.5 days (instant), Human triage = 5 days, Blended (67% AI) = 2.8 days. Track AI precision (% correct triage) to ensure speed doesn't sacrifice quality. 24% YoY improvement correlates with Aviator adoptionâ€”calculate exact ROI.</li>
                                <li><strong>Severity Breakdown (Advanced):</strong> Calculate MTT-Review by severity: Critical should be <1 day, High <3 days, Medium <7 days, Low <14 days. Priority discipline in triage same as remediation. Alert if Critical MTT-Review >2 days (SLA violation).</li>
                                <li><strong>Industry Benchmarks:</strong> World-class: <3 days. Good: 3-7 days. Average: 7-14 days. Poor: >14 days. Example 2.8 days = world-class. Aviator-enabled programs often achieve <1 day for AI-triaged issues.</li>
                                <li><strong>Recommendation:</strong> Pair with MTTR to show full cycle time (2.8d review + 14.2d fix = 17d total). Track separately for AI vs human to demonstrate automation ROI. Add SLA alerts for Critical review delays. Target: <3 days overall, <1 day for Critical, improving YoY. For Aviator users: aim for 70%+ AI triage coverage.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--status-error);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">9. Reopen Rate - Value: 9/10 â­</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Reopen Rate = (Count of reopened issues / Total closed issues) Ã— 100 for trailing 12 months. "Reopened" = issue closed/fixed, then same instance reappears in subsequent scan. Time window: typically 90 days after closure. Critical quality metricâ€”prevents gaming remediation counts.</li>
                                <li><strong>Fortify on Demand (Detailed Implementation):</strong>
                                    <ul>
                                        <li><strong>Endpoint:</strong> GET /api/v3/releases/{releaseId}/vulnerabilities?filters=closedDate:&gt;{12monthsAgo}&offset=0&limit=50 returns closed vulnerabilities with lifecycle tracking.</li>
                                        <li><strong>Key Response Fields:</strong>
                                            <ul>
                                                <li>closedDate (date-time) â€” When vulnerability was marked fixed/resolved</li>
                                                <li>reopenedDate (date-time, nullable) â€” If issue reappeared after closure, timestamp of redetection</li>
                                                <li>id (string GUID) â€” Unique vulnerability identifier for instance tracking</li>
                                                <li>Issue audit trail â€” May need GET /api/v3/releases/{releaseId}/vulnerabilities/{vulnId}/audit-history for complete lifecycle</li>
                                            </ul>
                                        </li>
                                        <li><strong>Reopen Rate Calculation Workflow:</strong>
                                            <ol>
                                                <li>Query all closed vulnerabilities from trailing 12 months</li>
                                                <li>For each closed vulnerability: Check if reopenedDate exists within 90 days of closedDate</li>
                                                <li>Count reopened issues: Filter where reopenedDate != null AND (reopenedDate - closedDate) â‰¤ 90 days</li>
                                                <li>Count total closed issues (denominator)</li>
                                                <li>Calculate reopen rate: (reopenedCount / totalClosed) Ã— 100</li>
                                                <li>Optional: Break down by severity (Critical reopens are most concerning)</li>
                                            </ol>
                                        </li>
                                        <li><strong>Same Instance vs Recurrence:</strong> **CRITICAL**: "Reopen" = exact same vulnerability instance (same file path + line number + issue type) redetected after fix. "Recurrence" = new instance of same vulnerability category (different location). FoD deduplication uses vulnerability correlation keys (file + location + type). Reopen tracked via vulnerability ID persistence across scansâ€”same ID reappearing = reopen. Recurrence appears as new ID. Track separately: Reopen = fix quality problem (incomplete remediation), Recurrence = developer training problem (pattern reintroduced elsewhere).</li>
                                        <li><strong>90-Day Window Rationale:</strong> Issues reappearing within 90 days typically indicate incomplete fix or immediate regression. <30 days = likely incomplete fix (symptom patched, not root cause). 30-90 days = code regression (fix reverted by later commit) or test gap. >90 days = may be new technical debt rather than true reopen. Document window clearly in metric definition.</li>
                                        <li><strong>Root Cause Classification:</strong> FoD audit trail may capture reopen reasons: (1) Incomplete fix (developer addressed symptom, not root cause), (2) Code regression (fix merged then reverted), (3) Variant reintroduction (fixed one instance, missed others), (4) Test coverage gap (fix works in dev, fails in prod). Track reopen reason codes for targeted remediation training.</li>
                                        <li><strong>Industry Benchmarks:</strong> World-class: <5% reopen rate. Good: 5-10%. Average: 10-15%. Poor: >15%. Example 8% = good performance. High reopen rates indicate: Rushing fixes without root cause analysis, Insufficient testing before marking fixed, Developer skill gaps in remediation techniques, Inadequate code review process. Target <8% overall, <5% for Critical/High severity.</li>
                                        <li><strong>Gaming Prevention:</strong> This metric validates "remediated issues" counts are real, durable fixes (not superficial patches). High reopen rates expose teams marking issues "fixed" without proper verification or testing. Pair with MTTR: Fast MTTR + high reopen rate = rushing. Slow MTTR + low reopen rate = thorough fixes. Optimal: Fast MTTR + low reopens (automation + quality).</li>
                                        <li><strong>Performance:</strong> Tracking reopens requires scanning audit trail historyâ€”expensive for large datasets. Cache reopen rate calculation daily. Pre-compute reopen counts monthly. For real-time tracking, flag reopened issues in database when detected (reopen event trigger).</li>
                                    </ul>
                                </li>
                                <li><strong>Fortify on Demand:</strong> FoD tracks reopens via issue lifecycle: issue transitions from Resolved/Fixed â†’ Open again when detected in new scan. Filter for <code>reopenedDate != null</code> or parse issue history for Closeâ†’Open transitions. Calculate rate: reopened count Ã· total closed issues. FoD audit trail provides reopen timestamps and reasons.</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>âš ï¸ HIGH UNCERTAINTY:</strong> SSC does not explicitly track "reopens" with dedicated field. Must detect by analyzing issue lifecycle: <code>removedDate</code> â†’ <code>foundDate</code> sequence.</p>
                                    
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=removedDate:[{12monthsAgo} TO *]</code> â€” Get issues marked removed in last 12 months</li>
                                        <li>For each removed issue: Check if <code>foundDate > removedDate</code> (indicates reappearance after fix)</li>
                                        <li>Apply 90-day window filter: <code>(foundDate - removedDate) <= 90 days</code></li>
                                        <li>Calculate reopen rate: (reopened count / total removed) Ã— 100</li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>// Calculate 12-month date range
startDate = new Date()
startDate.setMonth(startDate.getMonth() - 12)
startDateStr = startDate.toISOString().substring(0, 10)

totalClosed = 0
reopenedCount = 0

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

for (version in versions.data) {
  // Get issues removed in last 12 months with full date history
  removedIssues = GET "/api/v1/projectVersions/{version.id}/issues?q=removedDate:[{startDateStr} TO *]&fields=id,removedDate,foundDate&start=0&limit=500"
  
  for (issue in removedIssues.data) {
    totalClosed++
    
    // Check if issue reappeared after removal
    if (issue.removedDate && issue.foundDate) {
      removedDateMs = Date.parse(issue.removedDate)
      foundDateMs = Date.parse(issue.foundDate)
      
      // If foundDate > removedDate, issue reappeared (reopen)
      if (foundDateMs > removedDateMs) {
        daysSinceRemoval = (foundDateMs - removedDateMs) / (1000 * 60 * 60 * 24)
        
        // Only count if within 90-day window
        if (daysSinceRemoval <= 90) {
          reopenedCount++
        }
      }
    }
  }
}

reopenRate = (reopenedCount / totalClosed) * 100
console.log("Reopen Rate: " + reopenRate + "% (" + reopenedCount + " of " + totalClosed + ")")</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>SSC issue lifecycle tracking is complex</strong>â€”an issue can have multiple <code>foundDate</code> and <code>removedDate</code> values if it appears/disappears across scans. The API returns only the most recent dates, not full history. For accurate reopen tracking, may need to query issue audit trail: <code>GET /api/v1/issues/{id}/auditHistory</code> to detect all removalâ†’reappearance cycles. Issue deduplication across scans may not be perfectâ€”same vulnerability at different line numbers might create new issue ID rather than reopening existing one. <strong>Recommendation:</strong> This metric is difficult to implement accurately in SSC without custom tracking. Consider implementing external tracking system that monitors issue state changes across scan uploads, or use FoD which has built-in reopen detection.</p>
                                </li>
                                <li><strong>Same Instance vs New Instance:</strong> <strong>CRITICAL:</strong> Distinguish "reopen" (same file/line redetected) from "recurrence" (different instance, same category). Use issue deduplication keys (file path + line number + vulnerability type). Recurrence is developer training issue, reopen is fix quality issueâ€”very different root causes.</li>
                                <li><strong>Root Cause Analysis:</strong> Common reopen causes: (1) Incomplete fix (symptom addressed, not root cause), (2) Code regression (fix reverted by later commit), (3) Incomplete test coverage (fix worked in dev, failed in prod), (4) Variant reintroduction (same pattern in different code path). Track reopen reason codes for targeted improvement.</li>
                                <li><strong>Industry Benchmarks:</strong> World-class: <5%. Good: 5-10%. Average: 10-15%. Poor: >15%. Example 8% = good performance. High reopen rates indicate rushing, insufficient testing, or remediation skill gaps.</li>
                                <li><strong>Time Window Definition:</strong> Recommend 90-day window: issue closed, then reappears within 90 days = reopen. <30 days = likely incomplete fix. 30-90 days = regression. >90 days = new technical debt (may not count as reopen). Document window clearly.</li>
                                <li><strong>Recommendation:</strong> Pair with "Most Reopened Categories" (next KPI) to identify specific problem areas. Track reopen rate by severity (Critical reopens are unacceptable). Add YoY trend. Monitor per-team reopen rates to identify training needs. Target: <8% overall, <5% for Critical/High. Gaming prevention: this metric validates that "remediated" counts are real fixes, not superficial.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--accent);">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">10. Most Reopened Categories - Value: 8/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Top 5 vulnerability categories by reopen count for trailing 12 months. Category = CWE or OWASP category. Reopen = same instance closed then redetected. Ranked by absolute count (not rate). Critical for root cause analysis and training focus.</li>
                                <li><strong>Fortify on Demand:</strong> Group reopened issues by <code>categoryName</code> or <code>primaryTag</code>. Count reopens per category. Sort descending, take top 5. FoD provides standardized category taxonomy (OWASP, CWE). Example shown: SQL Injection (142), XSS (118), Command Injection (97), Path Manipulation (76), Privacy Violation (54).</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>Key Endpoints:</strong> Apply KPI 9 (Reopen Rate) logic to detect reopened issues, then group by <code>issueName</code> field (vulnerability category). <code>GET /api/v1/projectVersions/{id}/issues?q=removedDate:[{12monthsAgo} TO *]&fields=id,issueName,foundDate,removedDate</code> â†’ filter where foundDate > removedDate â†’ group by issueName â†’ count per category â†’ sort descending â†’ top 5.</p>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>reopensByCategory = {}  // {"SQL Injection": 0, "Cross-Site Scripting": 0, ...}

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=removedDate:[{12monthsAgo} TO *]&fields=issueName,foundDate,removedDate&start=0&limit=500"
  
  for (issue in issues.data) {
    // Detect reopen: foundDate > removedDate within 90-day window
    if (issue.foundDate && issue.removedDate) {
      foundMs = Date.parse(issue.foundDate)
      removedMs = Date.parse(issue.removedDate)
      
      if (foundMs > removedMs && (foundMs - removedMs) <= 90 * 24 * 60 * 60 * 1000) {
        category = issue.issueName  // e.g., "SQL Injection", "Cross-Site Scripting"
        
        if (!reopensByCategory[category]) {
          reopensByCategory[category] = 0
        }
        reopensByCategory[category]++
      }
    }
  }
}

// Sort categories by reopen count, take top 5
sortedCategories = Object.keys(reopensByCategory).sort((a, b) => 
  reopensByCategory[b] - reopensByCategory[a]
)
top5 = sortedCategories.slice(0, 5)</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> SSC <code>issueName</code> field contains tool-specific category names (e.g., "Fortify: SQL Injection" vs. standard "SQL Injection")â€”may need string normalization for cross-tool consistency. Reopen detection same limitations as KPI 9 (no explicit reopen tracking, must infer from date sequences). Performance: Must process all removed issues to detect reopens, then aggregate by categoryâ€”expect 10-15 min for large datasets. <strong>Recommendation:</strong> Cache category reopen counts daily, normalize category names to OWASP/CWE standards for consistency.</p>
                                </li>
                                <li><strong>Absolute Count vs Reopen Rate:</strong> Chart shows absolute counts (142 reopens), not rates. This has volume biasâ€”categories with more issues naturally have more reopens. <strong>Recommendation:</strong> Add reopen rate % alongside count. Example: SQL Injection: 142 reopens from 1,200 fixes = 11.8% reopen rate. XSS: 118 reopens from 800 fixes = 14.8% reopen rate (actually worse despite lower count). Rate reveals true problem areas.</li>
                                <li><strong>Root Cause Patterns by Category:</strong> Common patterns: (1) <strong>SQL Injection</strong> reopens: developers use string concat instead of parameterized queries, incomplete fix misses some query paths. (2) <strong>XSS</strong> reopens: output encoding applied inconsistently, context-specific encoding misunderstood. (3) <strong>Command Injection</strong> reopens: input validation incomplete, developers sanitize some parameters but miss others. (4) <strong>Path Manipulation</strong> reopens: path traversal defenses bypassed by encoding tricks. (5) <strong>Privacy Violation</strong> reopens: logging/error handling reintroduces sensitive data exposure.</li>
                                <li><strong>Actionable Interventions:</strong> For each high-reopen category: (1) Develop category-specific remediation playbook with examples, (2) Create secure coding standards, (3) Add automated testing (SAST rules, unit tests), (4) Conduct targeted training workshops, (5) Implement architectural patterns (ORM for SQLi, templating engines for XSS, allow-lists for Command Injection). Track reopen rate reduction YoY per category.</li>
                                <li><strong>Recommendation:</strong> Add reopen rate % (not just count) to reveal true problem severity. Expand to top 10 categories for comprehensive view. Track YoY trend per category (are interventions working?). Pair with training completion rates to measure training effectiveness. For each category: create remediation playbook, secure coding standard, automated test coverage. Target: reduce top category reopen rate by 30% YoY through focused interventions.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid #1aa364;">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">11. SAST Aviator Adoption Rate - Value: 7/10</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> Adoption Rate = (Applications with Aviator enabled / Total applications) Ã— 100. "Enabled" = Aviator auto-triage turned on for SAST scans in application. Current snapshot (not trailing 12 months). Example: 83 of 124 apps = 67% adoption. Leading indicator of automation ROI.</li>
                                <li><strong>Fortify on Demand:</strong> FoD Aviator enabled at Release level. Query <code>/api/v3/releases</code> with <code>aviatorEnabled=true</code> filter. Count enabled releases vs total releases. FoD provides Aviator configuration per release in release settings API. May also filter by subscription entitlement (Aviator requires specific license tier).</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>âš ï¸ MODERATE UNCERTAINTY:</strong> SSC Aviator integration varies by version (SSC 23.1+ required). No native "aviatorEnabled" fieldâ€”must check scan configuration or custom attributes.</p>
                                    
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions?includeInactive=false&fields=id,name</code> â€” Get all active project versions</li>
                                        <li><code>GET /api/v1/projectVersions/{id}/attributes</code> â€” Check for Aviator-related custom attributes (e.g., "Aviator Enabled")</li>
                                        <li>Alternative: <code>GET /api/v1/projectVersions/{id}/artifacts</code> â†’ check artifact metadata for "cloudscan" or "aviator" flags</li>
                                        <li>Count versions with Aviator indicators, calculate (aviatorVersions / totalVersions) Ã— 100</li>
                                    </ol>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>SSC lacks native Aviator enablement tracking</strong>â€”no standardized field like FoD's <code>aviatorEnabled</code>. Must rely on custom attributes manually populated by teams or scan metadata flags. Aviator integration through Fortify Scan Central may set <code>cloudscan.enabled</code> or similar property in artifact metadata. <strong>Recommendation:</strong> Create custom boolean attribute "Aviator Enabled" for each project version, update via API/workflow when Aviator configured. Alternative: Query recent scan artifacts for AI triage events in audit history as proxy for enablement. For accurate tracking, implement custom tracking solution or primarily use FoD for Aviator adoption metrics.</p>
                                </li>
                                <li><strong>Enabled vs Active Usage:</strong> "Enabled" â‰  "actively triaging issues". Track separately: (1) <strong>Enabled:</strong> Aviator turned on (67% in example), (2) <strong>Active:</strong> Aviator actually triaging issues in recent scans (may be lower if scans infrequent), (3) <strong>Effective:</strong> Aviator triage decisions accepted by humans (precision metric). Recommend tracking all three.</li>
                                <li><strong>Adoption Barriers:</strong> Common reasons for non-adoption: (1) Trust/control concerns (teams want human review), (2) Licensing constraints (Aviator not included in tier), (3) Integration complexity (SSC setup harder than FoD), (4) Unawareness (teams don't know Aviator exists), (5) Legacy apps (older apps not migrated to Aviator-compatible versions). Survey non-adopters to identify barriers.</li>
                                <li><strong>Adoption Trend Tracking:</strong> <strong>CRITICAL:</strong> Track adoption velocity. Example: Q1: 45%, Q2: 56%, Q3: 61%, Q4: 67% = healthy growth (+22% YoY). Stagnant adoption suggests intervention needed. Add monthly adoption rate chart showing growth trajectory. Project when 100% adoption achievable.</li>
                                <li><strong>Recommendation:</strong> Pair with "Aviator ROI" (next KPI) to show value proposition. Track adoption barriers and address systematically. Celebrate early adopters and share success stories. Add accuracy metrics (Aviator precision/recall) to build trust. Set adoption target: 80%+ within 12 months, 90%+ within 24 months. For non-adopters: create onboarding playbook, assign adoption champions, offer hands-on training. Adoption velocity matters more than snapshot percentage.</li>
                            </ul>
                        </div>
                    </details>

                    <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid #1aa364;">
                        <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">12. SAST Aviator ROI - Value: 10/10 â­ðŸ†</summary>
                        <div style="padding-left: 16px; margin-top: 12px;">
                            <p><strong>Design &amp; Implementation Notes:</strong></p>
                            <ul>
                                <li><strong>Data Inclusion:</strong> ROI calculation: (Issues auto-triaged Ã— Time saved per issue Ã— Hourly cost) = Annual savings. Example: 24,360 issues Ã— 0.75 hours Ã— $80/hr = $1,461,600 potential savings. Actual savings: $487,200 (assumes 67% adoption, 50% precision, redeployment factor). Multi-dimensional: dollars, hours, issues, time per issue. <strong>Perfect executive metricâ€”converts technical automation to business value.</strong></li>
                                <li><strong>Calculation Formula (Detailed):</strong>
                                    <ol>
                                        <li><strong>Issues auto-triaged:</strong> Count of issues triaged by Aviator (not human) in trailing 12 months. Example: 24,360 issues.</li>
                                        <li><strong>Time saved per issue:</strong> Average human triage time (30-60 min) - AI triage time (~0 min, instant). Example: 45 min saved per issue.</li>
                                        <li><strong>Total hours saved:</strong> Issues Ã— (Time saved / 60). Example: 24,360 Ã— (45/60) = 18,270 hours.</li>
                                        <li><strong>Hourly cost:</strong> Fully-loaded AppSec analyst cost (salary + benefits + overhead). Typical: $60-100/hr. Example: $80/hr.</li>
                                        <li><strong>Gross savings:</strong> Hours Ã— Hourly cost. Example: 18,270 Ã— $80 = $1,461,600.</li>
                                        <li><strong>Precision adjustment:</strong> If Aviator precision is 85%, multiply by 0.85 (15% require human rework). Example: $1,461,600 Ã— 0.85 = $1,242,360.</li>
                                        <li><strong>Redeployment factor:</strong> Assume 80% of freed time redeployed productively (20% waste). Example: $1,242,360 Ã— 0.80 = $993,888.</li>
                                        <li><strong>Net annual savings:</strong> After adjustments. Example: ~$487,200 (assumes additional conservative factors or partial-year deployment).</li>
                                    </ol>
                                </li>
                                <li><strong>Fortify on Demand:</strong> FoD Aviator audit log tracks AI-triaged issues. Query issues where <code>analysisType=Automated</code> or <code>reviewedBy=Aviator</code>. Count for trailing 12 months. FoD provides Aviator triage confidence scoresâ€”use to calculate precision (% correct). Time saved = industry benchmark (45 min) or measure pre/post Aviator adoption MTT-Review.</li>
                                <li><strong>Software Security Center (Concise Implementation):</strong>
                                    <p><strong>Key Endpoints:</strong></p>
                                    <ol>
                                        <li><code>GET /api/v1/projectVersions/{id}/issues?q=foundDate:[{12monthsAgo} TO *]</code> â€” Get all issues from last 12 months</li>
                                        <li><code>GET /api/v1/issues/{id}/auditHistory</code> â€” Check for Aviator/AI triage events in audit trail</li>
                                        <li>Filter issues where audit history contains "Aviator", "AI", or "Automated" triage events</li>
                                        <li>Count AI-triaged issues, apply ROI formula: <code>Count Ã— 0.75 hours Ã— $80/hr</code></li>
                                    </ol>
                                    
                                    <p><strong>Implementation Logic:</strong></p>
                                    <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 6px; overflow-x: auto;"><code>aiTriagedCount = 0
TIME_SAVED_PER_ISSUE = 0.75  // hours (45 minutes)
HOURLY_COST = 80  // fully-loaded analyst cost

versions = GET "/api/v1/projectVersions?includeInactive=false&fields=id"

for (version in versions.data) {
  issues = GET "/api/v1/projectVersions/{version.id}/issues?q=foundDate:[{12monthsAgo} TO *]&fields=id&start=0&limit=500"
  
  for (issue in issues.data) {
    // Check audit history for Aviator triage
    auditHistory = GET "/api/v1/issues/{issue.id}/auditHistory"
    
    aviatorEvent = auditHistory.data.find(event => 
      event.userName.match(/aviator|ai|automated/i) ||
      event.comment.match(/aviator|ai.*triage|automated/i)
    )
    
    if (aviatorEvent) {
      aiTriagedCount++
    }
  }
}

// Calculate ROI
totalHoursSaved = aiTriagedCount * TIME_SAVED_PER_ISSUE
grossSavings = totalHoursSaved * HOURLY_COST

// Apply precision adjustment (assume 85% precision if not measured)
PRECISION_FACTOR = 0.85
netSavings = grossSavings * PRECISION_FACTOR

console.log("Aviator ROI: $" + netSavings.toFixed(0) + " annual savings")
console.log("(" + aiTriagedCount + " issues Ã— " + TIME_SAVED_PER_ISSUE + " hrs Ã— $" + HOURLY_COST + "/hr Ã— " + PRECISION_FACTOR + " precision)")</code></pre>
                                    
                                    <p><strong>âš ï¸ Key Limitations:</strong> <strong>Requires querying audit history for every issue</strong>â€”extremely expensive for large portfolios (expect 20-30 min for 30K+ issues). SSC audit trail may not clearly identify Aviator vs human triageâ€”event attribution varies by deployment. No native precision tracking in SSCâ€”must manually validate sample of AI-triaged issues quarterly. <strong>Recommendation:</strong> Implement as weekly batch job, cache AI-triaged issue count. Create custom attribute flagging AI-triaged issues for faster filtering. For precision measurement: randomly sample 100 AI-triaged issues monthly, have expert validate, track agreement rate. Primarily use FoD for Aviator ROI metrics if availableâ€”SSC tracking significantly more complex.</p>
                                </li>
                                <li><strong>Precision Measurement (CRITICAL):</strong> Aviator precision = (Correct AI triage decisions / Total AI triage decisions) Ã— 100. Measure via human validation sampling: randomly sample 100 AI-triaged issues, have expert review, calculate agreement rate. Industry typical: 80-90% precision. Low precision erodes ROIâ€”if 50% wrong, humans must rework everything (no savings). <strong>Must validate precision quarterly to ensure ROI claims are credible.</strong></li>
                                <li><strong>Hourly Cost Methodology:</strong> Fully-loaded cost includes: base salary + benefits (30% overhead) + facilities/IT (10%) + management overhead (10%). Example: $100K salary â†’ $150K fully-loaded â†’ $75/hr (2,000 work hours/year). Range: Junior analyst $50/hr, Senior analyst $100/hr, use blended rate. <strong>Document assumptions for CFO scrutiny.</strong></li>
                                <li><strong>Alternative ROI Metrics:</strong> Beyond cost savings: (1) <strong>Capacity creation:</strong> 18,270 hours = 9.1 FTE-years freed for higher-value work (threat modeling, architecture review, pen testing). (2) <strong>Cycle time improvement:</strong> MTT-Review reduced 24% YoY (2.8 days now vs 3.7 days pre-Aviator). (3) <strong>Developer experience:</strong> Faster feedback loops improve satisfaction. (4) <strong>Scalability:</strong> Handle 2x issue volume without headcount growth. Report multiple dimensions for comprehensive ROI story.</li>
                                <li><strong>Multi-Year Projection:</strong> Assuming 67% adoption â†’ 90% over 2 years, and issue volume grows 15%/year: Year 1: $487K, Year 2: $650K (+33% from higher adoption + volume growth), Year 3: $750K. Cumulative 3-year savings: $1.89M. Compare to Aviator license cost (e.g., $50K/year) for net ROI. Payback period: typically <3 months. <strong>Multi-year view strengthens business case for tool renewals.</strong></li>
                                <li><strong>Recommendation:</strong> <strong>Lead with this metric in executive presentations</strong>â€”converts AppSec from cost center to value creator. Pair with precision metrics to build credibility. Update quarterly as adoption grows. Add multi-year projection for budget planning. Compare to Aviator license cost to show net ROI ($487K savings vs ~$50K cost = 874% ROI). Use to justify expanding automation to DAST, SCA (Aviator roadmap). Target: maintain >500% ROI, improve precision to 90%+, grow adoption to 90%+ (increases savings to $700K+). <strong>This single metric justifies entire ASPM dashboard and AppSec program investment.</strong></li>
                            </ul>
                        </div>
                    </details>

                    
                </div>
            </div>
        </section>

        <!-- FoD API Enhancement Recommendations for Remediation Dashboard (Collapsible) -->
        <section style="margin-top: 24px;">
            <div class="grid row">
                <div class="card col-12">
                    <details>
                        <summary style="cursor: pointer; font-size: 18px; font-weight: 600; margin-bottom: 16px;">ðŸ’¡ Recommended FoD API Enhancements for Remediation Dashboard Implementation</summary>
                        <div>
                            <p style="margin-bottom: 16px;"><strong>Analysis Context:</strong> Based on implementation requirements for the 12 Remediation Dashboard KPIs, the following API enhancements would dramatically simplify implementation and improve performance. Current implementation requires 300-600+ API calls with complex client-side aggregation and timeToFixDays calculations. Proposed enhancements would reduce this to ~5-8 calls with 95%+ faster load times.</p>
                            
                            <div style="margin: 16px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #28a745; border-radius: 4px;">
                                <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âœ… What FoD Already Provides (Existing Capabilities):</p>
                                <p style="margin: 0 0 8px 0; font-size: 13px;">The FoD vulnerability endpoints already include excellent remediation-related fields:</p>
                                <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                    <li>âœ… <strong>timeToFixDays</strong> - Pre-calculated time from discovery to fix (perfect for MTTR)</li>
                                    <li>âœ… <strong>introducedDate</strong>, <strong>closedDate</strong>, <strong>closedStatus</strong> - Full remediation lifecycle</li>
                                    <li>âœ… <strong>developerStatus</strong>, <strong>auditorStatus</strong> - Review workflow tracking</li>
                                    <li>âœ… <strong>status</strong> (with history support) - Reopened issue detection</li>
                                    <li>âœ… <strong>scantype</strong>, <strong>severity</strong>, <strong>category</strong> - All necessary dimensions</li>
                                    <li>âœ… <strong>fortifyAviator</strong>, <strong>aviatorRemediationGuidanceAvailable</strong> - AI triage tracking</li>
                                    <li>âœ… Filtering support: <code>filters</code> parameter with complex logic (AND/OR, field:value syntax)</li>
                                    <li>âœ… Date range support: <code>scanStartedDate</code>, <code>scanCompletedDate</code>, <code>introducedDate</code> filters</li>
                                </ul>
                                <p style="margin: 8px 0 0 0; font-size: 13px; font-weight: 600; color: var(--text);">These fields enable calculating all 12 KPIs, but require heavy client-side aggregation across 300+ releases with paginated results.</p>
                            </div>
                            
                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--sev-critical); border-radius: 8px;">
                                <h4 style="margin-top: 0; color: var(--sev-critical);">ðŸ”´ Critical Missing Endpoint (Highest ROI)</h4>
                                
                                <details style="margin-bottom: 16px; padding: 12px; background: var(--card-bg); border-left: 3px solid var(--sev-critical);">
                                    <summary style="cursor: pointer; font-weight: 600; margin-bottom: 8px;">1. GET /api/v3/remediation-metrics/summary â­â­â­ðŸ†</summary>
                                    <div style="padding-left: 16px; margin-top: 12px;">
                                        <p><strong>Problem Solved:</strong> Currently requires 300-600+ API calls to aggregate remediation metrics across portfolio. Each KPI (#1-#12) requires iterating through all releases, paginating through vulnerabilities (50/page), calculating timeToFixDays statistics client-side, detecting reopened issues via history, and grouping by severity/scantype. Extremely expensive for large portfolios.</p>
                                        
                                        <div style="margin: 16px 0; padding: 12px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                            <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">âš¡ Why Existing Endpoints Aren't Sufficient:</p>
                                            <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                                <li><strong>GET /api/v3/releases/{id}/vulnerabilities</strong> - Returns individual vulnerabilities, not aggregates
                                                    <br><em>Gap:</em> To get "78% critical issues fixed", must query all 300+ releases, paginate through thousands of vulnerabilities per release, filter by severity+closedStatus, calculate percentages client-side</li>
                                                <li><strong>No portfolio-wide aggregation</strong> - Every metric requires iterating entire portfolio
                                                    <br><em>Impact:</em> 300 releases Ã— 50 vulns/page Ã— 5 pages avg = 75,000 vulnerability records transferred</li>
                                                <li><strong>No MTTR statistics</strong> - timeToFixDays exists per vulnerability but no mean/median/p90 aggregates
                                                    <br><em>Current workaround:</em> Download all fixed vulnerabilities (10K+), calculate statistics in browser (slow, memory-intensive)</li>
                                                <li><strong>No trend/time-series support</strong> - Can't get "remediated issues by month" without manual grouping
                                                    <br><em>Workaround:</em> Filter by closedDate ranges, query 12 times (once per month), aggregate client-side</li>
                                                <li><strong>Reopen detection requires history API</strong> - Must call /vulnerabilities/{vulnId}/history for every closed issue
                                                    <br><em>Cost:</em> 10,000 fixed issues Ã— 1 history call each = 10,000 API calls just for reopen rate</li>
                                            </ul>
                                            <p style="margin: 8px 0 0 0; font-size: 13px; font-weight: 600; color: var(--text);">Result: Dashboard load time 45-90 seconds, 300-600 API calls, 50-100 MB data transfer, frequent rate limiting.</p>
                                        </div>
                                        
                                        <p><strong>Proposed Response Schema:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto;"><code>{
  "dateRange": {
    "startDate": "2024-01-01T00:00:00Z",
    "endDate": "2024-12-31T23:59:59Z"
  },
  "fixRates": {
    "bySeverity": {
      "critical": { "discovered": 3102, "remediated": 2418, "rate": 0.78 },
      "high": { "discovered": 5202, "remediated": 4264, "rate": 0.82 },
      "medium": { "discovered": 8290, "remediated": 5893, "rate": 0.71 },
      "low": { "discovered": 13800, "remediated": 8832, "rate": 0.64 }
    },
    "byScanType": {
      "SAST": { "discovered": 11395, "remediated": 8436, "rate": 0.74 },
      "DAST": { "discovered": 6900, "remediated": 4692, "rate": 0.68 },
      "SCA": { "discovered": 8800, "remediated": 7128, "rate": 0.81 },
      "Other": { "discovered": 1947, "remediated": 1151, "rate": 0.59 }
    },
    "overall": { "discovered": 29394, "remediated": 21407, "rate": 0.73 }
  },
  "meanTimeToRemediate": {
    "overall": {
      "mean": 14.2,
      "median": 11.5,
      "p90": 28.4,
      "sampleSize": 21407
    },
    "bySeverity": {
      "critical": { "mean": 5.8, "median": 4.2, "p90": 12.1 },
      "high": { "mean": 9.7, "median": 7.8, "p90": 19.3 },
      "medium": { "mean": 17.2, "median": 14.6, "p90": 34.2 },
      "low": { "mean": 25.3, "median": 21.8, "p90": 48.7 }
    }
  },
  "reviewMetrics": {
    "bySeverity": {
      "critical": { "total": 3102, "reviewed": 2914, "rate": 0.94 },
      "high": { "total": 5202, "reviewed": 4628, "rate": 0.89 },
      "medium": { "total": 8290, "reviewed": 6308, "rate": 0.76 },
      "low": { "total": 13800, "reviewed": 8004, "rate": 0.58 }
    },
    "byScanType": {
      "SAST": { "total": 11395, "reviewed": 9804, "rate": 0.86 },
      "DAST": { "total": 6900, "reviewed": 5451, "rate": 0.79 },
      "SCA": { "total": 8800, "reviewed": 6247, "rate": 0.71 },
      "Other": { "total": 550, "reviewed": 352, "rate": 0.64 }
    },
    "meanTimeToReview": {
      "mean": 2.8,
      "median": 2.1,
      "p90": 5.7,
      "sampleSize": 21854
    }
  },
  "recurrenceMetrics": {
    "totalClosed": 21407,
    "reopened": 1713,
    "reopenRate": 0.08,
    "topReopenedCategories": [
      { "category": "SQL Injection", "count": 142, "rate": 0.083 },
      { "category": "XSS", "count": 128, "rate": 0.068 },
      { "category": "Path Traversal", "count": 97, "rate": 0.052 },
      { "category": "Weak Crypto", "count": 84, "rate": 0.071 },
      { "category": "Auth Bypass", "count": 76, "rate": 0.046 }
    ]
  },
  "remediationTrend": {
    "monthlyData": [
      {
        "month": "2024-01",
        "bySeverity": {
          "critical": 187,
          "high": 342,
          "medium": 456,
          "low": 712
        }
      },
      // ... 11 more months
    ]
  },
  "aviatorMetrics": {
    "issuesTriaged": 42630,
    "hoursS saved": 10658,
    "estimatedAnnualSavings": 532875,
    "adoptionRate": 0.67
  },
  "appliedFilters": {
    "applicationFilters": null,
    "releaseFilters": null,
    "dateRange": "2024-01-01 to 2024-12-31"
  },
  "lastUpdated": "2024-11-07T10:30:00Z"
}</code></pre>
                                        
                                        <p><strong>Benefits:</strong></p>
                                        <ul>
                                            <li>Reduces 300-600 API calls to 1 call (99.5%+ reduction)</li>
                                            <li>Dashboard load time: &lt;2s vs. 45-90s (96%+ improvement)</li>
                                            <li>Server-side statistics calculation (mean, median, p90) - more accurate than client-side</li>
                                            <li>Reopen detection via server-side history analysis (eliminates 10,000+ history API calls)</li>
                                            <li>Monthly trend data pre-aggregated (no 12x separate queries)</li>
                                            <li>Powers all 12 Remediation Dashboard KPIs with single endpoint</li>
                                            <li>Consistent calculations across all users (no client-side variance)</li>
                                        </ul>
                                        
                                        <p><strong>Query Parameters (Filtering Support):</strong></p>
                                        <ul>
                                            <li><code>startDate</code> (ISO 8601, default: 12 months ago) - Start of date range for remediation analysis</li>
                                            <li><code>endDate</code> (ISO 8601, default: now) - End of date range</li>
                                            <li><code>groupByMonth</code> (boolean, default: true) - Include monthly trend breakdown</li>
                                            <li><code>includeAviatorMetrics</code> (boolean, default: true) - Include Aviator ROI calculations</li>
                                            <li><code>topN</code> (integer, default: 5) - Number of top reopened categories to return</li>
                                        </ul>
                                        
                                        <p><strong>Application-Level Attribute Filters:</strong></p>
                                        <ul>
                                            <li><code>filters</code> (string, FoD filter syntax) - Apply application filters:
                                                <ul>
                                                    <li><code>filters=businessCriticalityType:High</code> - High criticality apps only</li>
                                                    <li><code>filters=businessCriticalityType:High|Medium</code> - Multiple values (OR)</li>
                                                    <li><code>filters=attributes.BusinessUnit:Finance</code> - Custom application attributes</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>Release-Level Attribute Filters:</strong></p>
                                        <ul>
                                            <li><code>releaseFilters</code> (string) - Filter by release attributes:
                                                <ul>
                                                    <li><code>releaseFilters=sdlcStatusType:Production</code> - Production releases only</li>
                                                    <li><code>releaseFilters=sdlcStatusType:Production|QA</code> - Production OR QA</li>
                                                    <li><code>releaseFilters=attributes.ComplianceScope:PCI</code> - Custom release attributes</li>
                                                </ul>
                                            </li>
                                        </ul>
                                        
                                        <p><strong>âš ï¸ CRITICAL REQUIREMENT:</strong> Must support custom attribute filtering on both applications and releases. Customers need to filter remediation metrics by business unit, compliance scope, geography, etc. Default behavior: include all active releases across all SDLC statuses (exclude only retired/suppressed). Do NOT default to production-only.</p>
                                        
                                        <p><strong>Example Filtered Queries:</strong></p>
                                        <pre style="background: #1e1e1e; color: #d4d4d4; padding: 12px; border-radius: 4px; overflow-x: auto; font-size: 12px;"><code>// Default: All active releases, last 12 months, all KPIs
GET /api/v3/remediation-metrics/summary

// Production releases only, YTD
GET /api/v3/remediation-metrics/summary?startDate=2024-01-01&releaseFilters=sdlcStatusType:Production

// High-criticality apps, last 6 months
GET /api/v3/remediation-metrics/summary?startDate=2024-05-01&filters=businessCriticalityType:High

// Finance BU, PCI-compliant, production releases, custom date range
GET /api/v3/remediation-metrics/summary?startDate=2024-01-01&endDate=2024-06-30&filters=attributes.BusinessUnit:Finance+attributes.ComplianceScope:PCI&releaseFilters=sdlcStatusType:Production

// Minimal payload (no monthly trend, no Aviator)
GET /api/v3/remediation-metrics/summary?groupByMonth=false&includeAviatorMetrics=false</code></pre>
                                        
                                        <p><strong>Implementation Notes:</strong></p>
                                        <ul>
                                            <li><strong>Reopen Detection Algorithm:</strong> Issue is "reopened" if closedStatus=true AND history shows status changed from Closed â†’ Open/In Progress after initial closure. Count unique issues reopened (not reopen events).</li>
                                            <li><strong>MTTR Calculation:</strong> Use <code>timeToFixDays</code> field (already calculated by FoD). Calculate mean, median (50th percentile), and p90 (90th percentile). Filter to only closedStatus=true issues within date range.</li>
                                            <li><strong>Review Detection:</strong> Issue is "reviewed" if auditorStatus OR developerStatus is not null/empty. MTT-Review = time from introducedDate to first audit/developer status change.</li>
                                            <li><strong>Aviator Metrics:</strong> Count issues where fortifyAviator=true. Hours saved = count Ã— 0.25 hours (15 min avg). Adoption rate = aviator issues / total reviewed issues.</li>
                                            <li><strong>Performance Optimization:</strong> Cache results for 1 hour (remediation metrics change slowly). Use database aggregation queries (GROUP BY, AVG, PERCENTILE functions). Index on closedDate, introducedDate, severity, scantype for fast filtering.</li>
                                            <li><strong>Backward Compatibility:</strong> Existing vulnerability endpoints remain unchanged. This is additive enrichment.</li>
                                        </ul>
                                    </div>
                                </details>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸ“Š Performance Impact Summary</h4>
                                
                                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-top: 16px;">
                                    <div style="background: rgba(255,107,107,0.1); padding: 12px; border-radius: 4px; border-left: 3px solid var(--sev-critical);">
                                        <h5 style="margin-top: 0; color: var(--sev-critical);">âŒ Current State (Without Enhancements)</h5>
                                        <ul style="margin: 8px 0; font-size: 14px;">
                                            <li>Remediation Dashboard load time: <strong>45-90 seconds</strong></li>
                                            <li>Total API calls: <strong>300-600+</strong></li>
                                            <li>Data transfer: <strong>50-100 MB</strong> (50 vulns/page Ã— 300 releases Ã— 5-10 pages avg)</li>
                                            <li>Client-side processing: <strong>Heavy</strong> (statistics, grouping, reopen detection)</li>
                                            <li>Memory footprint: <strong>Large</strong> (30K+ vulnerability records in browser)</li>
                                            <li>Rate limiting risk: <strong>Very High</strong> (burst of 300-600 calls)</li>
                                            <li>Special case: Reopen rate KPI requires <strong>additional 10,000+ history API calls</strong> (one per closed issue)</li>
                                        </ul>
                                    </div>
                                    
                                    <div style="background: rgba(40,167,69,0.1); padding: 12px; border-radius: 4px; border-left: 3px solid #28a745;">
                                        <h5 style="margin-top: 0; color: #28a745;">âœ… Future State (With GET /api/v3/remediation-metrics/summary)</h5>
                                        <ul style="margin: 8px 0; font-size: 14px;">
                                            <li>Remediation Dashboard load time: <strong>&lt;2 seconds</strong> (96%+ improvement)</li>
                                            <li>Total API calls: <strong>1</strong> (99.7% reduction from 300-600)</li>
                                            <li>Data transfer: <strong>&lt;50 KB</strong> (99.95% reduction)</li>
                                            <li>Client-side processing: <strong>Minimal</strong> (just render pre-aggregated data)</li>
                                            <li>Memory footprint: <strong>Tiny</strong> (&lt;1 MB summary object)</li>
                                            <li>Rate limiting risk: <strong>None</strong> (single call)</li>
                                            <li>Server-side caching: <strong>1 hour</strong> (dashboard serves 100s of users from cache)</li>
                                        </ul>
                                    </div>
                                </div>
                                
                                <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-radius: 4px;">
                                    <h5 style="margin-top: 0; color: #4a9eff;">ðŸŽ¯ Business Impact</h5>
                                    <ul style="margin: 8px 0; font-size: 14px;">
                                        <li><strong>User Experience:</strong> Dashboard loads instantly instead of 45-90 second wait. Directors can actually use it in live meetings.</li>
                                        <li><strong>Server Load:</strong> 99.7% reduction in API calls = massive infrastructure savings. One cached query serves 100s of dashboard users.</li>
                                        <li><strong>Data Accuracy:</strong> Server-side statistics (median, p90) more accurate than JavaScript client-side calculations (no floating-point errors).</li>
                                        <li><strong>Adoption:</strong> Fast, responsive dashboard = higher usage. Slow dashboard = abandoned tool.</li>
                                        <li><strong>Scalability:</strong> Current approach breaks at 500+ releases (timeouts, rate limits). New approach scales to 10,000+ releases.</li>
                                    </ul>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--bg-surface); border-left: 4px solid #ffc107; border-radius: 4px;">
                                <h4 style="margin-top: 0; color: var(--text);">âš ï¸ Why Standard Vulnerability Endpoints Don't Suffice</h4>
                                <p style="margin-bottom: 12px;">FoD's existing endpoints provide excellent per-vulnerability data, but the Remediation Dashboard requires <strong>portfolio-wide statistical aggregations</strong> that are fundamentally incompatible with paginated, per-release queries:</p>
                                <ul style="margin-bottom: 12px;">
                                    <li><strong>Statistical Functions Missing:</strong> No built-in mean/median/p90 for timeToFixDays. Client must download 21,407 individual records, sort in memory, calculate percentiles. Slow & error-prone.</li>
                                    <li><strong>Cross-Release Aggregation Missing:</strong> Fix rate "78% critical issues fixed" requires querying 300+ releases, aggregating discovered vs. remediated counts across all releases, calculating single percentage. No endpoint does this.</li>
                                    <li><strong>Time-Series Grouping Missing:</strong> Monthly remediation trend requires 12 separate queries (one per month) OR downloading full year of data and grouping client-side. Neither is performant.</li>
                                    <li><strong>History Analysis at Scale:</strong> Reopen rate requires calling /vulnerabilities/{id}/history for every closed issue (10,000+ calls). Server should do this once and cache result.</li>
                                    <li><strong>Multi-Dimensional Grouping:</strong> "Fix rate by severity AND scan type" requires nested loops across releases â†’ vulnerabilities â†’ group by both dimensions. Complex client-side logic prone to bugs.</li>
                                </ul>
                                <p style="margin: 0; font-weight: 600; color: var(--text);">Conclusion: Remediation Dashboard requires <strong>aggregate analytics endpoint</strong>, not raw data endpoints. This is a different API pattern (OLAP vs OLTP) that necessitates purpose-built endpoint.</p>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸŽ¯ Implementation Priority</h4>
                                
                                <table style="width: 100%; border-collapse: collapse; font-size: 14px;">
                                    <thead>
                                        <tr style="background: var(--accent); color: white;">
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">Priority</th>
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">Endpoint</th>
                                            <th style="padding: 8px; text-align: center; border: 1px solid var(--border);">Impact</th>
                                            <th style="padding: 8px; text-align: center; border: 1px solid var(--border);">Effort</th>
                                            <th style="padding: 8px; text-align: left; border: 1px solid var(--border);">KPIs Enabled</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td style="padding: 8px; border: 1px solid var(--border); font-weight: 600; color: var(--sev-critical);">P0</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">
                                                <strong>Remediation Metrics Summary</strong>
                                                <div style="font-size: 12px; color: #666; margin-top: 4px;">
                                                    <em>No existing equivalent - requires server-side aggregation across portfolio</em>
                                                </div>
                                            </td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">â­â­â­ Critical</td>
                                            <td style="padding: 8px; border: 1px solid var(--border); text-align: center;">Medium</td>
                                            <td style="padding: 8px; border: 1px solid var(--border);">All 12 KPIs (#1-#12)</td>
                                        </tr>
                                    </tbody>
                                </table>
                                
                                <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-radius: 4px;">
                                    <p style="margin: 0; font-size: 14px;"><strong>Estimated Implementation Effort:</strong> Medium (2-3 sprints)</p>
                                    <ul style="margin: 8px 0 0 0; font-size: 13px;">
                                        <li>Backend: Implement aggregation queries with GROUP BY, statistical functions (median, percentile), history analysis for reopens</li>
                                        <li>Caching: Add 1-hour cache layer (remediation metrics update slowly)</li>
                                        <li>Filtering: Reuse existing filters/releaseFilters architecture from other endpoints</li>
                                        <li>Testing: Validate statistical calculations match UI expectations, test with large datasets (1000+ releases)</li>
                                    </ul>
                                </div>
                                
                                <div style="margin-top: 16px; padding: 12px; background: var(--bg-surface); border-left: 3px solid #28a745; border-radius: 4px;">
                                    <p style="margin: 0 0 8px 0; font-weight: 600; color: var(--text);">ðŸ’¡ Alternative Approach (If New Endpoint Not Feasible):</p>
                                    <p style="margin: 0 0 8px 0; font-size: 13px;">If creating new endpoint is delayed, consider enhancing existing <code>GET /api/v3/releases/{id}/vulnerabilities</code>:</p>
                                    <ul style="margin: 4px 0 0 0; font-size: 13px; padding-left: 20px;">
                                        <li>Add <code>aggregate=true</code> parameter to return summary statistics instead of individual vulnerabilities</li>
                                        <li>Response: <code>{ "fixRateBySeverity": {...}, "mttrStats": {...}, "reviewStats": {...} }</code></li>
                                        <li><strong>Pro:</strong> Reuses existing endpoint, incremental enhancement</li>
                                        <li><strong>Con:</strong> Still requires 300+ calls (one per release), client must aggregate across releases</li>
                                        <li><strong>Impact:</strong> Reduces load time to ~15-20 seconds (vs. 45-90 current, vs. &lt;2 with new endpoint)</li>
                                    </ul>
                                </div>
                            </div>

                            <div style="margin-top: 24px; padding: 16px; background: var(--card-bg); border: 1px solid var(--border); border-radius: 8px;">
                                <h4 style="margin-top: 0;">ðŸ“‹ Final Recommendation</h4>
                                <p style="margin-bottom: 12px;"><strong>Create <code>GET /api/v3/remediation-metrics/summary</code> as top priority for Remediation Dashboard enablement.</strong></p>
                                <p style="margin-bottom: 12px;"><strong>Rationale:</strong></p>
                                <ul style="margin-bottom: 12px;">
                                    <li>âœ… <strong>Single endpoint enables all 12 KPIs</strong> - highest ROI of any proposed endpoint</li>
                                    <li>âœ… <strong>99.7% reduction in API calls</strong> (300-600 â†’ 1) - massive performance and infrastructure savings</li>
                                    <li>âœ… <strong>Existing vulnerability data already captures all necessary fields</strong> (timeToFixDays, closedStatus, auditorStatus, history) - implementation is aggregation logic, not new data collection</li>
                                    <li>âœ… <strong>Remediation metrics are inherently portfolio-wide</strong> - cannot be efficiently calculated via per-release endpoints</li>
                                    <li>âœ… <strong>Addresses highest-value dashboard</strong> - Remediation Dashboard is used daily by directors/managers, vs. Program Dashboard (monthly) and Risk Exposure (weekly)</li>
                                    <li>âœ… <strong>Customer pain point:</strong> Current implementations abandon Remediation Dashboard due to 45-90 second load times</li>
                                </ul>
                                <p style="margin: 0; padding: 12px; background: var(--bg-surface); border-radius: 4px; font-weight: 600;">Expected outcome: Remediation Dashboard becomes most-used ASPM dashboard, load time &lt;2 seconds, scales to 1000+ releases, enables data-driven remediation strategy.</p>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
        </section>

    </div>

    <script>
        (function () {
            /* ===== Theme toggle ===== */
            const THEME_KEY = 'ux-theme';
            const prefersDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
            const storedTheme = localStorage.getItem(THEME_KEY);
            const initialTheme = storedTheme || (prefersDark ? 'dark' : 'light');
            const root = document.documentElement;
            const themeToggle = document.getElementById('themeToggle');

            function applyTheme(theme) {
                root.setAttribute('data-theme', theme);
                localStorage.setItem(THEME_KEY, theme);
                themeToggle.checked = theme === 'dark';
            }
            themeToggle.addEventListener('change', () => applyTheme(themeToggle.checked ? 'dark' : 'light'));
            applyTheme(initialTheme);

            /* ===== Page switch (Program <-> Risk <-> Remediation) ===== */
            const title = document.getElementById('pageTitle');
            const subtitle = document.getElementById('pageSubtitle');
            const btnProgram = document.getElementById('navProgram');
            const btnRisk = document.getElementById('navRisk');
            const btnRemediation = document.getElementById('navRemediation');
            const pgProgram = document.getElementById('page-program');
            const pgRisk = document.getElementById('page-risk');
            const pgRemediation = document.getElementById('page-remediation');

            function show(page, pushHash = true) {
                // Hide all pages and deactivate all buttons
                pgProgram.classList.remove('active');
                pgRisk.classList.remove('active');
                pgRemediation.classList.remove('active');
                btnProgram.classList.remove('active');
                btnRisk.classList.remove('active');
                btnRemediation.classList.remove('active');

                // Show selected page
                if (page === 'program') {
                    pgProgram.classList.add('active');
                    btnProgram.classList.add('active');
                    title.textContent = 'Program Dashboard';
                    subtitle.textContent = 'Track application inventory, scan coverage, and overall security program status';
                    if (pushHash) location.hash = '#program';
                } else if (page === 'risk') {
                    pgRisk.classList.add('active');
                    btnRisk.classList.add('active');
                    title.textContent = 'Risk Exposure Dashboard';
                    subtitle.textContent = 'Monitor vulnerability trends, severity distribution, and security posture across all applications';
                    if (pushHash) location.hash = '#risk';
                } else if (page === 'remediation') {
                    pgRemediation.classList.add('active');
                    btnRemediation.classList.add('active');
                    title.textContent = 'Remediation Dashboard';
                    subtitle.textContent = 'Measure remediation efficiency, review processes, and demonstrate ROI of your AppSec program';
                    if (pushHash) location.hash = '#remediation';
                }
                window.scrollTo({ top: 0, behavior: 'instant' });
            }

            btnProgram.addEventListener('click', () => show('program'));
            btnRisk.addEventListener('click', () => show('risk'));
            btnRemediation.addEventListener('click', () => show('remediation'));

            // Initialize from URL hash if present
            if (location.hash === '#risk') { show('risk', false); }
            else if (location.hash === '#remediation') { show('remediation', false); }
            else { show('program', false); }
        })();

        /* ===== Filter Panel Functions ===== */
        function toggleFilters() {
            const panel = document.getElementById('filterPanel');
            panel.classList.toggle('open');
        }

        function updateFilterCount() {
            const checkboxes = document.querySelectorAll('.filter-panel input[type="checkbox"]');
            const checkedCount = Array.from(checkboxes).filter(cb => cb.checked).length;
            const badge = document.getElementById('filterBadge');
            
            if (checkedCount > 0) {
                badge.textContent = checkedCount;
                badge.style.display = 'inline-block';
            } else {
                badge.style.display = 'none';
            }
            
            // Update active filter tags
            updateFilterTags();
        }

        function updateFilterTags() {
            const tagsContainer = document.getElementById('activeFilterTags');
            tagsContainer.innerHTML = '';

            // Helper to get nice display names
            const filterLabels = {
                risk: 'Risk',
                type: 'Type',
                unit: 'Business Unit',
                sdlc: 'SDLC'
            };

            const valueLabels = {
                high: 'High',
                medium: 'Medium',
                low: 'Low',
                mobile: 'Mobile',
                web: 'Web',
                api: 'API',
                thick: 'Thick Client',
                content: 'Content',
                itom: 'ITOM',
                cyber: 'Cybersecurity',
                adm: 'ADM',
                networks: 'Business Networks',
                portfolio: 'Portfolio',
                dev: 'Development',
                qa: 'QA/Test',
                prod: 'Production'
            };

            // Collect all checked filters
            const categories = ['risk', 'type', 'unit', 'sdlc'];
            
            categories.forEach(category => {
                const checkboxes = document.querySelectorAll(`input[name="${category}"]:checked`);
                checkboxes.forEach(cb => {
                    const tag = document.createElement('div');
                    tag.className = 'filter-tag';
                    tag.innerHTML = `
                        <span>${filterLabels[category]}:${valueLabels[cb.value]}</span>
                        <span class="filter-tag-close" onclick="removeFilter('${cb.id}')">&times;</span>
                    `;
                    tagsContainer.appendChild(tag);
                });
            });
        }

        function removeFilter(checkboxId) {
            const checkbox = document.getElementById(checkboxId);
            if (checkbox) {
                checkbox.checked = false;
                updateFilterCount();
            }
        }

        function clearFilters() {
            const checkboxes = document.querySelectorAll('.filter-panel input[type="checkbox"]');
            checkboxes.forEach(cb => cb.checked = false);
            updateFilterCount();
        }

        function applyFilters() {
            // Collect selected filters
            const filters = {
                risk: [],
                type: [],
                unit: [],
                sdlc: []
            };

            document.querySelectorAll('input[name="risk"]:checked').forEach(cb => {
                filters.risk.push(cb.value);
            });
            document.querySelectorAll('input[name="type"]:checked').forEach(cb => {
                filters.type.push(cb.value);
            });
            document.querySelectorAll('input[name="unit"]:checked').forEach(cb => {
                filters.unit.push(cb.value);
            });
            document.querySelectorAll('input[name="sdlc"]:checked').forEach(cb => {
                filters.sdlc.push(cb.value);
            });

            // Show mock message (in real implementation, this would filter the dashboard data)
            const selectedCount = Object.values(filters).flat().length;
            if (selectedCount > 0) {
                console.log('Filters applied:', filters);
                // Optionally show a toast notification
                alert(`Filters applied! (${selectedCount} filter${selectedCount !== 1 ? 's' : ''} selected)\n\nThis is a mockup - filtering functionality will be implemented in the production version.`);
            }
            
            // Close the filter panel
            document.getElementById('filterPanel').classList.remove('open');
        }
    </script>
</body>

</html>